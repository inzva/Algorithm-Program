{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Algorithm Program","text":"<p>Algoritm Program contains lectures about algorithms and data structures which are prepared by inzva community, aimed at teaching advanced knowledge of algorithms to university students, spreading algorithmic thinking and providing training which will help them in international contests as well as in their professional lives.</p> <p>There is also a video playlist in Turkish about some of the algorithms and data structures in YouTube:</p>"},{"location":"#how-to-use-this-site","title":"How to Use This Site","text":"<ul> <li>Lectures can be found by topics at the navigation bar. The sub topics can be found at those pages. Search bar is also available for finding pages by terms.</li> <li>In each lecture related problems and training sets from algoleague.com are mentioned. Practicing those is highly recommended.</li> </ul>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<p>In order to contribute (adding new lecture, fixing any type of errors) below steps should be followed:</p> <ol> <li>Create an issue and briefly explain the purpose of your contribution.</li> <li>Fork the repository with your personal account and apply your changes.</li> <li>Create a pull request to master branch and add the link of pull request to issue.</li> <li>After reviewing your pull request and discussion, your pull request will be merged. Thank you for your contribution!</li> </ol>"},{"location":"algorithms/","title":"Algorithms","text":"<p>Editor: Kadir Emre Oto</p> <p>Reviewers: Muhammed Burak Bu\u011frul, Tahsin Enes Kuru</p>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#search-algorithms","title":"Search Algorithms","text":"<p>It may be necessary to determine if an array or solution set contains a specific data, and we call this finding proccess searching. In this article, three most common search algorithms will be discussed: linear search, binary search, and ternary search.</p> <p>This visualization may help you understand how the search algorithms work.</p>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#linear-search","title":"Linear Search","text":"<p>Simplest search algorithm is linear search, also know as sequential search. In this technique, all elements in the collection of the data is checked one by one, if any element matches, algorithm returns the index; otherwise, it returns \\(-1\\).</p> <p>Its time complexity is \\(\\mathcal{O}(N)\\).</p> Example for linear search <pre><code>int linearSearch(int *array, int size, int key) {\n    for (int i = 0; i &lt; size; i++)\n        if (array[i] == key)\n            return i;\n    return -1;\n}\n</code></pre>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#binary-search","title":"Binary Search","text":"<p>We know linear search is quite a slow algorithm because it compares each element of the set with search key, and there is a high-speed searching technique for sorted data instead of linear search, which is binary search. After each comparison, the algorithm eliminates half of the data using the sorting property. </p> <p>We can also use binary search on increasing functions in the same way. </p>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#procedure","title":"Procedure","text":"<ol> <li>Compare the key with the middle element of the array,</li> <li>If it is a match, return the index of middle.</li> <li>If the key is bigger than the middle, it means that the key must be in the right side of the middle. We can eliminate the left side.</li> <li>If the key is smaller, it should be on the left side. The right side can be ignored.</li> </ol>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#complexity","title":"Complexity","text":"<p>\\[ \\begin{align*} T(N) &amp;= T\\left(\\tfrac{N}{2}\\right) + \\mathcal{O}(1) \\\\ T(N) &amp;= \\mathcal{O}(\\log N) \\end{align*} \\]</p> Example for binary search <pre><code>int binarySearch(int *array, int size, int key) {\n    int left = 0, right = size, mid;\n\n    while (left &lt; right) {\n        mid = (left + right) / 2;\n\n        if (array[mid] &gt;= key)\n            right = mid;\n        else\n            left = mid + 1;\n    }\n    return array[left] == key ? left : -1;\n}\n</code></pre>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#ternary-search","title":"Ternary Search","text":"<p>Suppose that we have a unimodal function, \\(f(x)\\), on an interval \\([l, r]\\), and we are asked to find the local minimum or the local maximum value of the function according to the behavior of it.</p> <p>There are two types of unimodal functions:</p> <ol> <li> <p>The function, \\(f(x)\\) strictly increases for \\(x \\leq m\\), reaches a global maximum at \\(x = m\\), and then strictly decreases for \\(m \\leq x\\). There are no other local maxima.</p> </li> <li> <p>The function, \\(f(x)\\) strictly decreases for \\(x \\leq m\\), reaches a global minimum at \\(x = m\\), and then strictly increases for \\(m \\leq x\\). There are no other local minima.</p> </li> </ol> <p>In this document, we will implement the first type of unimodal function, and the second one can be solved using the same logic.</p>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#procedure_1","title":"Procedure","text":"<ol> <li>Choose any two points \\(m_1\\), and \\(m_2\\) on the interval \\([l, r]\\), where \\(l &lt; m_1 &lt; m_2 &lt; r\\).</li> <li>If \\(f(m_1) &lt; f(m_2)\\), it means the maxima should be in the interval \\([m_1, r]\\), so we can ignore the interval \\([l, m_1]\\), move \\(l\\) to \\(m_1\\)</li> <li>Otherwise, \\(f(m_1) \\geq f(m_2)\\), the maxima have to be in the interval \\([l, m_2]\\), move \\(r\\) to \\(m_2\\)</li> <li>If \\(r - l &lt; \\epsilon\\), where \\(\\epsilon\\) is a negligible value, stop the algorithm, return \\(l\\). Otherwise turn to the step 1. </li> </ol> <p>\\(m_1\\) and \\(m_2\\) can be selected by \\(m_1 = l + \\frac{r-l}{3}\\) and \\(m_2 = r - \\frac{r-l}{3}\\) to avoid increasing the time complexity.</p>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#complexity_1","title":"Complexity","text":"<p>\\[ \\begin{align*} T(N) &amp;= T\\left(2 \\cdot \\tfrac{N}{3}\\right) + \\mathcal{O}(1) \\\\ T(N) &amp;= \\mathcal{O}(\\log N) \\end{align*} \\]</p> Example for ternary search <pre><code>double f(double x);\n\ndouble ternarySearch(double left, double right, double eps = 1e-7) {\n    while (right - left &gt; eps) {\n        double mid1 = left + (right - left) / 3;\n        double mid2 = right - (right - left) / 3;\n\n        if (f(mid1) &lt; f(mid2))\n            left = mid1;\n        else\n            right = mid2;\n    }\n    return f(left);\n}\n</code></pre>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#sorting-algorithms","title":"Sorting Algorithms","text":"<p>Sorting algorithms are used to put the elements of an array in a certain order according to the comparison operator. Numerical order or lexicographical orders are the most common ones, and there are a large number of sorting algorithms, but we discuss four of them:</p> <ul> <li>Insertion Sort</li> <li>Merge Sort</li> <li>Quick Sort</li> <li>Radix Sort</li> </ul> <p>For a better understanding, you are strongly recommended to go into this visualization site after reading the topics.</p>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#insertion-sort","title":"Insertion Sort","text":"<p>Think that you are playing a card game and want to sort them before the game. Your sorting strategy is simple: you have already sorted some part and every time you pick up the next card from unsorted part, you insert it into the correct place in sorted part. After you apply this process to all cards, the whole deck would be sorted.</p> <p>This is the basic idea for sorting an array. We assume that the first element of the array is the sorted part, and other elements are in the unsorted part. Now, we choose the leftmost element of the unsorted part, and put it into the sorted part. In this way the left part of the array always remains sorted after every iteration, and when no element is left in the unsorted part, the array will be sorted. </p> <pre><code>void insertionSort(int *ar, int size) {\n    for (int i = 1; i &lt; size; i++)\n        for (int j = i - 1; 0 &lt;= j and ar[j] &gt; ar[j + 1]; j--)\n            swap(ar[j], ar[j + 1]);\n}\n</code></pre>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#merge-sort","title":"Merge Sort","text":"<p>Merge Sort is one of the fastest sorting algorithms that uses Divide and Conquer paradigm. The algorithm divides the array into two halves, solves each part recursively using same sorting function and combines them in linear time by selecting the smallest value of the arrays every time.</p>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#procedure_2","title":"Procedure","text":"<ol> <li>If the size of the array is 1, it is sorted already, stop the algorithm (base case),</li> <li>Find the middle point of the array, and split it in two,</li> <li>Do the algorithm for these parts separately from the first step,</li> <li>After the two halves got sorted, merge them in linear time and the array will be sorted. </li> </ol>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#complexity_2","title":"Complexity","text":"<p>\\[ \\begin{align*} T(N) &amp;= T\\left(\\tfrac{N}{2}\\right) + \\mathcal{O}(N) \\\\ T(N) &amp;= \\mathcal{O}(N \\cdot \\log N) \\end{align*} \\]</p> <pre><code>void mergeSort(int *ar, int size) {\n    if (size &lt;= 1) // base case\n        return;\n\n    mergeSort(ar, size / 2); // divide the array into two almost equal parts\n    mergeSort(ar + size / 2, size - size / 2);\n\n    int index = 0, left = 0, right = size / 2; // merge them\n    int *temp = new int[size];\n\n    while (left &lt; size / 2 or right &lt; size) {\n        if (right == size or (left &lt; size / 2 and ar[left] &lt; ar[right]))\n            temp[index++] = ar[left++];\n        else\n            temp[index++] = ar[right++];\n    }\n    for (int i = 0; i &lt; size; i++)\n        ar[i] = temp[i];\n    delete[] temp;\n}\n</code></pre>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#quick-sort","title":"Quick Sort","text":"<p>Quick Sort is also a Divide and Conquer algorithm. The algorithm chooses an element from the array as a pivot and partitions the array around it. Partitioning is arranging the array that satisfies those: the pivot should be put to its correct place, all smaller values should be placed before the pivot, and all greater values should be placed after the pivot. The partitioning can be done in linear time, and after the partitioning, we can use the same sorting function to solve the left part of the pivot and the right part of the pivot recursively.</p> <p>If the sellected pivot cannot divide the array uniformly after the partitioning, the time complexity can reach \\(\\mathcal{O}(n ^ 2)\\) like insertion sort. To avoid this, the pivot can generally be picked randomly.</p>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#procedure_3","title":"Procedure","text":"<ol> <li>If the size of the array is \\(1\\), it is sorted already, stop the algorithm (base case),</li> <li>Choose a pivot randomly,</li> <li>For all values in the array, collect smaller values in the left of the array and greater values in the right of array,</li> <li>Move the pivot to the correct place,</li> <li>Repeat the same algorithm for the left partition and the right partition.</li> </ol>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#complexity_3","title":"Complexity","text":"<p>\\[ \\begin{align*} T(N) &amp;= T\\left(\\tfrac{N}{10}\\right) + T\\left(9 \\cdot \\tfrac{N}{10}\\right) + \\mathcal{O}(N) \\\\ T(N) &amp;= \\mathcal{O}(N \\cdot \\log N) \\end{align*} \\]</p> <pre><code>void quickSort(int *ar, int size) {\n    if (size &lt;= 1) // base case\n        return;\n\n    int position = 1; // find the correct place of pivot\n    swap(ar[0], ar[rand() % size]);\n\n    for (int i = 1; i &lt; size; i++)\n        if (ar[0] &gt; ar[i])\n            swap(ar[i], ar[position++]);\n    swap(ar[0], ar[position - 1]);\n\n    quickSort(ar, position - 1);\n    quickSort(ar + position, size - position);\n}\n</code></pre>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#radix-sort","title":"Radix Sort","text":"<p>Quick Sort and Merge Sort are comparison-based sorting algorithms and cannot run better than \\(\\mathcal{O}(N \\log N)\\). However, Radix Sort works in linear time (\\(\\mathcal{O}(N + K)\\), where \\(K\\) is \\(\\log(\\max(ar))\\)).</p>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#procedure_4","title":"Procedure","text":"<ol> <li>For each digit from the least significant to the most, sort the array using Counting Sort according to corresponding digit. Counting Sort is used for keys between specific range, and it counts the number of elements which have different key values. After counting the number of distict key values, we can determine the position of elements in the array. </li> </ol>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#complexity_4","title":"Complexity","text":"<p>\\[ \\begin{align*} T(N) &amp;= \\mathcal{O}(N) \\end{align*} \\]</p> <pre><code>void radixSort(int *ar, int size, int base = 10) {\n    int *temp = new int[size];\n    int *count = new int[base]();\n\n    // Find the maximum value.\n    int maxx = ar[0];\n    for (int i = 1; i &lt; size; i++) {\n        if (ar[i] &gt; maxx) {\n            maxx = ar[i];\n        }\n    }\n\n    for (int e = 1; maxx / e &gt; 0; e *= base) {\n        memset(count, 0, sizeof(int) * base);\n\n        for (int i = 0; i &lt; size; i++)\n            count[(ar[i] / e) % base]++;\n\n        for (int i = 1; i &lt; base; i++)\n            count[i] += count[i - 1];\n\n        for (int i = size - 1; 0 &lt;= i; i--)\n            temp[--count[(ar[i] / e) % base]] = ar[i];\n\n        for (int i = 0; i &lt; size; i++)\n            ar[i] = temp[i];\n    }\n\n    delete[] temp;\n    delete[] count;\n}\n</code></pre>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#quickselect-algorithm","title":"Quickselect Algorithm","text":"<p>Quickselect is a selection algorithm that finds the \\(k^{th}\\) smallest element in an unordered list. The algorithm is closely related to QuickSort in partitioning stage; however, instead of recurring for both sides, it recurs only for the part that contains the \\(k^{th}\\) smallest element.</p>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#procedure_5","title":"Procedure","text":"<ol> <li>Choose a pivot randomly,</li> <li>For all values in the array, collect smaller values in the left of the array and greater values in the right of the array,</li> <li>Move the pivot to the correct place,</li> <li>If the current position is equal to \\(k\\), return the value at the position.</li> <li>If the current position is more than \\(k\\), repeat the same algorithm for the left partition.</li> <li>Else, update \\(k\\) and repeat the same algorithm for the right partition.</li> </ol>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#complexity_5","title":"Complexity","text":"<ul> <li>In average: \\(\\mathcal{O}(N)\\)</li> <li>Worst-case: \\(\\mathcal{O}(N^2)\\)</li> </ul> <p>Note that this algorithm is fast in practice, but has poor worst-case performance, like quicksort. However, it still performs better on average than other algorithms that find the \\(k^{th}\\) smallest element in \\(\\mathcal{O}(n)\\) in the worst case.</p> <pre><code>// This function finds the k-th smallest element in arr within size si.\nint QuickSelect(int *arr, int si, int k) {\n    // Check if k is valid and if arr has no less elements than k.\n    if (0 &lt; k &amp;&amp; k &lt;= si) {\n        // The quicksort-like partitioning. It is same until we find the index of the pivot.\n        int ind = 0;\n\n        // Get a random pivot to decrease the chance of getting worst-case scenario.\n        swap(arr[si - 1], arr[rand() % si]);\n        for (int j = 0; j &lt; si - 1; j++) {\n            if (arr[j] &lt;= arr[si - 1]) {\n                swap(arr[j], arr[ind]);\n                ind++;\n            }\n        }\n        swap(arr[si - 1], arr[ind]);\n\n        // Now check and recur to appropriate situation.\n        // If the index is equal with k-1 (as our array is 0-indexed) return the value.\n        if (ind == k - 1) {\n            return arr[ind];\n        }\n        // Else check if index is greater than k-1. If it is, recur to the left part.\n        else if (ind &gt; k - 1) {\n            return QuickSelect(arr, ind, k);\n        }\n        // Else, recur to the right part.\n        else {\n            return QuickSelect(arr + ind + 1, si - ind - 1, k - ind - 1);\n        }\n    }\n    // If invalid values is given\n    return INT_MAX;\n}\n</code></pre>","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"algorithms/#divide-and-conquer","title":"Divide and Conquer","text":"<p>Divide and Conquer is a well-known paradigm that breaks up the problem into several parts, solves each part independently, and finally combines the solutions to the subproblems into the overall solution. Because each subproblem is solved recursively, they should be the smaller versions of the original problem; and the problem must have a base case to end the recursion. </p> <p>Some example algorithms that use divide and conquer technique: </p> <ul> <li>Merge Sort</li> <li>Count Inversions</li> <li>Finding the Closest Pair of Points </li> <li>Others</li> </ul> The Flow of Divide and Conquer","tags":["Algorithms","Linear Search","Binary Search","Ternary Search","Sorting Algorithms","Insertion Sort","Merge Sort","Quick Sort","Radix Sort","Quickselect Algorithm","Divide and Conquer"]},{"location":"data-structures/","title":"Data Structures","text":"<p>Editor: Tahsin Enes Kuru</p> <p>Reviewers: Baha Eren Yald\u0131z, Burak Bu\u011frul</p> <p>Contributors: Kerim Kochekov</p>","tags":["Data Structures"]},{"location":"data-structures/#giris","title":"Giri\u015f","text":"<p>Bilgisayar biliminde veri yap\u0131lar\u0131, belirli bir eleman k\u00fcmesi \u00fczerinde verimli bir \u015feklide bilgi edinmemize ayn\u0131 zamanda bu elemanlar \u00fczerinde de\u011fi\u015fiklikler yapabilmemize olanak sa\u011flayan yap\u0131lard\u0131r. \u00c7al\u0131\u015fma prensipleri genellikle elemanlar\u0131n de\u011ferlerini belirli bir kurala g\u00f6re saklamak daha sonra bu yap\u0131lar\u0131 kullanarak elemanlar hakk\u0131nda sorulara (mesela, bir dizinin belirli bir aral\u0131\u011f\u0131ndaki en k\u00fc\u00e7\u00fck say\u0131y\u0131 bulmak gibi) cevap aramakt\u0131r.</p>","tags":["Data Structures"]},{"location":"data-structures/#dinamik-veri-yaplar","title":"Dinamik Veri Yap\u0131lar\u0131","text":"","tags":["Data Structures"]},{"location":"data-structures/#linked-list","title":"Linked List","text":"","tags":["Data Structures"]},{"location":"data-structures/#stack","title":"Stack","text":"","tags":["Data Structures"]},{"location":"data-structures/#queue","title":"Queue","text":"","tags":["Data Structures"]},{"location":"data-structures/#deque","title":"Deque","text":"","tags":["Data Structures"]},{"location":"data-structures/#fenwick-tree","title":"Fenwick Tree","text":"","tags":["Data Structures"]},{"location":"data-structures/#segment-tree","title":"Segment Tree","text":"","tags":["Data Structures"]},{"location":"data-structures/#trie","title":"Trie","text":"","tags":["Data Structures"]},{"location":"data-structures/#statik-veri-yaplar","title":"Statik Veri Yap\u0131lar\u0131","text":"","tags":["Data Structures"]},{"location":"data-structures/#prefix-sum","title":"Prefix Sum","text":"","tags":["Data Structures"]},{"location":"data-structures/#sparse-table","title":"Sparse Table","text":"","tags":["Data Structures"]},{"location":"data-structures/#sqrt-decomposition","title":"SQRT Decomposition","text":"","tags":["Data Structures"]},{"location":"data-structures/#mos-algorithm","title":"Mo's Algorithm","text":"","tags":["Data Structures"]},{"location":"data-structures/#common-problems","title":"Common Problems","text":"","tags":["Data Structures"]},{"location":"data-structures/#lca","title":"LCA","text":"","tags":["Data Structures"]},{"location":"data-structures/#ornek-problemler","title":"\u00d6rnek Problemler","text":"<p>Veri yap\u0131lar\u0131 \u00fczerinde pratik yapabilmeniz i\u00e7in \u00f6nerilen problemler:</p> <ol> <li>Link</li> <li>Link</li> <li>Link</li> <li>Link</li> <li>Link</li> </ol>","tags":["Data Structures"]},{"location":"data-structures/#faydal-baglantlar","title":"Faydal\u0131 Ba\u011flant\u0131lar","text":"<ol> <li>https://en.wikipedia.org/wiki/Data_structure</li> <li>https://cp-algorithms.com/data_structures/sparse-table.html</li> <li>https://cp-algorithms.com/data_structures/segment_tree.html</li> <li>https://cp-algorithms.com/data_structures/fenwick.html</li> <li>https://cp-algorithms.com/data_structures/sqrt_decomposition.html</li> <li>https://cses.fi/book/book.pdf</li> <li>https://visualgo.net/en/segmenttree</li> <li>https://visualgo.net/en/fenwicktree</li> <li>https://www.geeksforgeeks.org/binary-indexed-tree-or-fenwick-tree-2</li> <li>http://www.cs.ukzn.ac.za/~hughm/ds/slides/20-stacks-queues-deques.pdf</li> <li>https://www.geeksforgeeks.org/stack-data-structure</li> <li>https://www.geeksforgeeks.org/queue-data-structure</li> <li>https://www.geeksforgeeks.org/deque-set-1-introduction-applications</li> <li>https://www.geeksforgeeks.org/linked-list-set-1-introduction</li> <li>https://www.geeksforgeeks.org/binary-indexed-tree-range-updates-point-queries</li> <li>https://visualgo.net/en/list</li> <li>https://cp-algorithms.com/data_structures/fenwick.html</li> </ol>","tags":["Data Structures"]},{"location":"data-structures/deque/","title":"Deque","text":"<p>Deque veri yap\u0131s\u0131 stack ve queue veri yap\u0131lar\u0131na g\u00f6re daha kapsaml\u0131d\u0131r. Bu veri yap\u0131s\u0131nda yap\u0131n\u0131n en \u00fcst\u00fcne eleman eklenebilirken ayn\u0131 zamanda en alt\u0131na da eklenebilir. Ayn\u0131 \u015fekilde yap\u0131n\u0131n hem en \u00fcst\u00fcndeki eleman\u0131na hem de en alttaki eleman\u0131na eri\u015fim ve silme i\u015flemleri uygulanabilir. Bu veri yap\u0131s\u0131nda uyguluyabildi\u011fimiz i\u015flemler:</p> <ul> <li>Veri yap\u0131s\u0131n\u0131n en \u00fcst\u00fcne eleman ekleme.</li> <li>Veri yap\u0131s\u0131n\u0131n en alt\u0131na eleman ekleme.</li> <li>Veri yap\u0131s\u0131n\u0131n en \u00fcst\u00fcndeki eleman\u0131na eri\u015fim.</li> <li>Veri yap\u0131s\u0131n\u0131n en alt\u0131ndaki eleman\u0131na eri\u015fim.</li> <li>Veri yap\u0131s\u0131n\u0131n en \u00fcst\u00fcndeki eleman\u0131 silme.</li> <li>Veri yap\u0131s\u0131n\u0131n en alt\u0131ndaki eleman\u0131 silme.</li> </ul> <p>C++ dilindeki STL k\u00fct\u00fcphanesinde bulunan haz\u0131r deque yap\u0131s\u0131n\u0131n kullan\u0131m\u0131 a\u015fa\u011f\u0131daki gibidir:</p> <pre><code>int main() {\n    deque&lt;int&gt; q;\n    q.push_front(5);   // deque'nin en altina 5'i ekler.\n    q.push_back(6);    // deque'nin en ustune 6'yi ekler.\n    int x = q.front(); // deque'nin en altindaki elemanina erisim.\n    int y = q.back();  // deque'nin en ustundeki elemanina erisim.\n    q.pop_front();     // deque'nin en altindaki elemanini silme.\n    q.pop_back();      // deque'nin en ustundeki elemanini silme.\n}\n</code></pre> <p>P.S. deque veri yap\u0131s\u0131 stack ve queue veri yap\u0131lar\u0131na g\u00f6re daha kapsaml\u0131 oldu\u011fundan \u00f6t\u00fcr\u00fc stack ve queue veri yap\u0131lar\u0131na g\u00f6re 2 kat fazla memory kulland\u0131\u011f\u0131n\u0131 a\u00e7\u0131kl\u0131kla s\u00f6yleyebiliriz.</p>","tags":["Data Structures","Deque"]},{"location":"data-structures/fenwick-tree/","title":"Fenwick Tree","text":"<p>Binary Indexed Tree olarak da bilinen Fenwick Tree, Prefix Sum ve Sparse Table yap\u0131lar\u0131na benzer bir yap\u0131da olup dizi \u00fczerinde de\u011fi\u015fiklik yapabilmemize olanak sa\u011flayan bir veri yap\u0131s\u0131d\u0131r. Fenwick Tree'nin di\u011fer veri yap\u0131lar\u0131na g\u00f6re en b\u00fcy\u00fck avantaj\u0131 pratikte daha h\u0131zl\u0131 olmas\u0131 ve haf\u0131za karma\u015f\u0131kl\u0131\u011f\u0131n\u0131n \\(\\mathcal{O}(N)\\) olmas\u0131d\u0131r. Ancak Fenwick Tree'de sadece prefix cevaplar\u0131 (veya suffix cevaplar\u0131) saklayabildi\u011fimizden aral\u0131klarda minimum, maksimum ve EBOB gibi baz\u0131 sorgular\u0131n cevaplar\u0131n\u0131 elde edemeyiz.</p>","tags":["Data Structures","Fenwick Tree","Binary Indexed Tree","BIT"]},{"location":"data-structures/fenwick-tree/#yaps-ve-kurulusu","title":"Yap\u0131s\u0131 ve Kurulu\u015fu","text":"<p>\\(g(x)\\), \\(x\\) say\u0131s\u0131n\u0131n bit g\u00f6steriminde yaln\u0131zca en sa\u011fdaki bitin 1 oldu\u011fu tam say\u0131 olsun. \u00d6rne\u011fin \\(20\\)'nin bit g\u00f6sterimi \\((10100)_2\\) oldu\u011fundan \\(g(20)=4\\)'t\u00fcr. \u00c7\u00fcnk\u00fc ilk kez sa\u011fdan \\(3.\\) bit \\(1\\)'dir ve \\((00100)_2=4\\)'t\u00fcr. Fenwick Tree'nin \\(x\\) indeksli d\u00fc\u011f\u00fcm\u00fcnde, \\(x - g(x) + 1\\) indeksli elemandan \\(x\\) indeksli elemana kadar olan aral\u0131\u011f\u0131n cevab\u0131n\u0131 saklayacak \u015fekilde kurulur.</p> $8$ uzunlu\u011fundaki bir dizi i\u00e7in kurulmu\u015f Fenwick Tree yap\u0131s\u0131","tags":["Data Structures","Fenwick Tree","Binary Indexed Tree","BIT"]},{"location":"data-structures/fenwick-tree/#sorgu-algoritmas","title":"Sorgu Algoritmas\u0131","text":"<p>Herhangi bir \\([1,x]\\) aral\u0131\u011f\u0131 i\u00e7in sorgu algoritmas\u0131 s\u0131ras\u0131 ile \u015fu \u015feklide \u00e7al\u0131\u015f\u0131r:</p> <ol> <li>Arad\u0131\u011f\u0131m\u0131z cevaba \\([x - g(x) + 1,x]\\) aral\u0131\u011f\u0131n\u0131n cevab\u0131n\u0131 ekle.</li> <li>\\(x\\)'in de\u011ferini \\(x - g(x)\\) yap. E\u011fer \\(x\\)'in yeni de\u011feri \\(0\\)'dan b\u00fcy\u00fck ise \\(1.\\) i\u015flemden hesaplamaya devam et.</li> </ol> <p>\\([1,x]\\) aral\u0131\u011f\u0131n\u0131n cevab\u0131n\u0131 hesaplamak i\u00e7in yap\u0131lan i\u015flem say\u0131s\u0131 \\(x\\) say\u0131s\u0131n\u0131n \\(2\\)'lik tabandaki yaz\u0131l\u0131\u015f\u0131ndaki \\(1\\) say\u0131s\u0131na e\u015fittir. \u00c7\u00fcnk\u00fc her d\u00f6ng\u00fcde \\(x\\)'ten \\(2\\)'lik tabandaki yaz\u0131l\u0131\u015f\u0131ndaki en sa\u011fdaki \\(1\\) bitini \u00e7\u0131kart\u0131yoruz. Dolay\u0131s\u0131yla sorgu i\u015flemimiz \\(\\mathcal{O}(\\log N)\\) zaman karma\u015f\u0131kl\u0131\u011f\u0131nda \u00e7al\u0131\u015f\u0131r. \\([l,r]\\) aral\u0131\u011f\u0131n\u0131n cevab\u0131n\u0131 da \\([1,r]\\) aral\u0131\u011f\u0131n\u0131n cevab\u0131ndan \\([1,l - 1]\\) aral\u0131\u011f\u0131n\u0131n cevab\u0131n\u0131 \u00e7\u0131kararak kolay bir \u015fekilde elde edebiliriz.</p> <p>NOT: \\(g(x)\\) de\u011ferini bitwise operat\u00f6rlerini kullanarak a\u015fa\u011f\u0131daki e\u015fitlikle kolay bir \u015fekilde hesaplayabiliriz:  \\[g(x) = x \\ \\&amp; \\ (-x)\\]</p>","tags":["Data Structures","Fenwick Tree","Binary Indexed Tree","BIT"]},{"location":"data-structures/fenwick-tree/#eleman-guncelleme-algoritmas","title":"Eleman G\u00fcncelleme Algoritmas\u0131","text":"<p>Dizideki \\(x\\) indeksli eleman\u0131n\u0131n de\u011ferini g\u00fcncellemek i\u00e7in kullan\u0131lan algoritma \u015fu \u015feklide \u00e7al\u0131\u015f\u0131r:</p> <ul> <li>A\u011fa\u00e7ta \\(x\\) indeksli eleman\u0131 i\u00e7eren t\u00fcm d\u00fc\u011f\u00fcmlerin de\u011ferlerini g\u00fcncelle.</li> </ul> <p>Fenwick Tree'de \\(x\\) indeksli eleman\u0131 i\u00e7eren maksimum \\(\\log(N)\\) tane aral\u0131k oldu\u011fundan g\u00fcncelleme algoritmas\u0131 \\(\\mathcal{O}(\\log N)\\) zaman karma\u015f\u0131kl\u0131\u011f\u0131nda \u00e7al\u0131\u015f\u0131r.</p>","tags":["Data Structures","Fenwick Tree","Binary Indexed Tree","BIT"]},{"location":"data-structures/fenwick-tree/#ornek-kod-parcalar","title":"\u00d6rnek Kod Par\u00e7alar\u0131","text":"<pre><code>const int n;\nint tree[n + 1], a[n + 1];\n\nvoid add(int val, int x) { // x indeksli elemanin degerini val degeri kadar artirir.\n    // x indeksinin etkiledigi butun dugumleri val degeri kadar artirir.\n    while (x &lt;= n) {\n        tree[x] += val;\n        x += x &amp; (-x);\n    }\n}\n\nint sum(int x) { // 1 indeksli elemandan x indeksli elemana\n    int res = 0; // kadar olan sayilarin toplamini verir.\n    while (x &gt;= 1) {\n        res += tree[x];\n        x -= x &amp; (-x);\n    }\n    return res;\n}\n\nint query(int l, int r) { // [l,r] araligindaki elemanlarin toplamini verir.\n    return sum(r) - sum(l - 1);\n}\n\nvoid build() { // a dizisi uzerine fenwick tree yapisini kuruyoruz.\n    for (int i = 1; i &lt;= n; i++)\n        add(a[i], i);\n}\n</code></pre> <p>Fenwick Tree veri yap\u0131s\u0131 ile ilgili \u00f6rnek bir probleme buradan ula\u015fabilirsiniz.</p>","tags":["Data Structures","Fenwick Tree","Binary Indexed Tree","BIT"]},{"location":"data-structures/fenwick-tree/#aralk-guncelleme-ve-eleman-sorgu","title":"Aral\u0131k G\u00fcncelleme ve Eleman Sorgu","text":"<p>Bir \\(a\\) dizisi \u00fczerinde i\u015flemler yapaca\u011f\u0131m\u0131z\u0131 varsayal\u0131m daha sonra \\(a\\) dizisi \\(b\\) dizisinin prefix sum dizisi olacak \u015fekilde bir \\(b\\) dizisi tan\u0131mlayal\u0131m. Ba\u015fka bir deyi\u015fle $a_i = \\displaystyle\\sum_{j=1}^{i} {b_j} $ olmal\u0131d\u0131r. Sonradan olu\u015fturdu\u011fumuz \\(b\\) dizisi \u00fczerine Fenwick Tree yap\u0131s\u0131n\u0131 kural\u0131m. \\([l,r]\\) aral\u0131\u011f\u0131ndaki her elemana \\(x\\) de\u011ferini eklememiz i\u00e7in uygulamam\u0131z gereken i\u015flemler:</p> <ul> <li>\\(b_l\\) de\u011ferini \\(x\\) kadar art\u0131r. B\u00f6ylelikle \\(l\\) indeksli elemandan dizinin sonuna kadar t\u00fcm elemanlar\u0131n de\u011feri \\(x\\) kadar artm\u0131\u015f olur.</li> <li>\\(b_{r + 1}\\) de\u011ferini \\(x\\) kadar azalt. B\u00f6ylelikle \\(r + 1\\) indeksli elemandan dizinin sonuna kadar t\u00fcm elemanlar\u0131n de\u011feri \\(x\\) kadar azalm\u0131\u015f olur. Bu i\u015flemelerin sonucunda sadece \\([l,r]\\) aral\u0131\u011f\u0131ndaki elemanlar\u0131n de\u011feri \\(x\\) kadar artm\u0131\u015f olur.</li> </ul>","tags":["Data Structures","Fenwick Tree","Binary Indexed Tree","BIT"]},{"location":"data-structures/fenwick-tree/#ornek-kod-parcalar_1","title":"\u00d6rnek Kod Par\u00e7alar\u0131","text":"<pre><code>const int n;\nint a[n + 1], b[n + 1];\n\nvoid add(int val, int x) { // x indeksli elemanin degerini val degeri kadar artirir.\n    while (x &lt;= n) {\n        tree[x] += val;\n        x += x &amp; (-x);\n    }\n}\n\nint sum(int x) { // 1 indeksli elemandan x indeksli elemana\n    int res = 0; // kadar olan sayilarin toplamini verir.\n    while (x &gt;= 1) {\n        res += tree[x];\n        x -= x &amp; (-x);\n    }\n    return res;\n}\nvoid build() {\n    for (int i = 1; i &lt;= n; i++)\n        b[i] = a[i] - a[i - 1]; // b dizisini olusturuyoruz.\n\n    for (int i = 1; i &lt;= n; i++)\n        add(b[i], i); // b dizisi uzerine fenwick tree kuruyoruz.\n}\n\nvoid update(int l, int r, int x) {\n    add(x, l);\n    add(-x, r + 1);\n}\n\nvoid query(int x) { return sum(x); }\n</code></pre>","tags":["Data Structures","Fenwick Tree","Binary Indexed Tree","BIT"]},{"location":"data-structures/linked-list/","title":"Linked List","text":"<p>Linked List veri yap\u0131s\u0131nda elemanlar, her eleman kendi de\u011ferini ve bir sonraki eleman\u0131n adresini tutacak \u015fekilde saklan\u0131r. Yap\u0131daki elemanlar ba\u015f elemandan (head) ba\u015flanarak son elemana (tail) gidecek \u015fekilde gezilebilir. Diziye kar\u015f\u0131n avantaj\u0131 haf\u0131zan\u0131n dinamik bir \u015fekilde kullan\u0131lmas\u0131d\u0131r. Bu veri yap\u0131s\u0131nda uygulanabilecek i\u015flemler:</p> <ul> <li>Veri yap\u0131s\u0131n\u0131n sonuna eleman ekleme.</li> <li>Anl\u0131k veri yap\u0131s\u0131n\u0131 ba\u015ftan (head) sona (tail) gezme.</li> </ul> \u00d6rnek bir Linked List yap\u0131s\u0131 <pre><code>// Her bir elemani (burada sayilari, yani int) tutacak struct olusturuyoruz.\nstruct node {\n    int data;\n    node *next;\n};\nnode *head, *tail;\n\nvoid push_back(int x) {\n    // Yeni elemanimizi hafizada olusturuyoruz.\n    node *t = (node *)malloc(sizeof(node));\n    t-&gt;data = x;    // Elemanin verisini atiyoruz.\n    t-&gt;next = NULL; // Sona ekledigimizden sonraki elemanina NULL atiyoruz.\n\n    // Eger veri yapimiza hic eleman eklenmediyse head\n    // ve tail elemanlarini olusturuyoruz.\n    if (head == NULL &amp;&amp; tail == NULL) {\n        head = t;\n        tail = t;\n    }\n    // Eklenmisse yeni tail elemanimizi guncelliyoruz.\n    else {\n        tail-&gt;next = t;\n        tail = t;\n    }\n}\n\nvoid print() {\n    // Dizideki tum elemanlari geziyoruz.\n    node *t = head;\n    while (t != NULL) {\n        printf(\"%d \", t-&gt;data);\n        t = t-&gt;next;\n    }\n}\n</code></pre>","tags":["Data Structures","Linked List"]},{"location":"data-structures/lowest-common-ancestor/","title":"Lowest Common Ancestors","text":"<p>This problem consists of queries, LCA(x, y), and asks for the ancestor of both x and y whose depth is maximum. We will use a similar algorithm to the jump pointer algorithm with implementation.</p>","tags":["Tree","LCA","Lowest Common Ancestors","Binary Lifting"]},{"location":"data-structures/lowest-common-ancestor/#initialization","title":"Initialization","text":"<p>As we did in Jump Pointer Method, we will calculate node's all \\(2^i\\). ancestors if they exist. L[x][y] corresponds to x's \\(2^y\\). ancestors. Hence L[x][0] is basically the parent of x.</p> <p><pre><code>void init() {\n    for(int x=1 ; x&lt;=n ; x++)\n        L[x][0] = parent[x];\n\n    for(int y=1 ; y&lt;=logN ; y++)\n        for(int x=1 ; x&lt;=n ; x++)\n            L[x][y] = L[L[x][y-1]][y-1];\n}\n</code></pre> Note that we have used the fact that x's \\(2^y\\). ancestor is x's \\(2^{y\u22121}\\). ancestor's \\(2^{y\u22121}\\). ancestor.</p>","tags":["Tree","LCA","Lowest Common Ancestors","Binary Lifting"]},{"location":"data-structures/lowest-common-ancestor/#queries-binary-lifting","title":"Queries-Binary Lifting","text":"<p>Given LCA(x, y), we calculate answer by following:</p> <p>Firstly, ensure that both x and y are in same depth. If it is not take the deepest one to the other one's depth. Then control whether x and y is equal. If they are equal, that means the lowest common ancestor is x. After that, from i = log(N), check that if x's \\(2^i\\). ancestor is equal to y's \\(2^i\\). ancestor. If they are not equal that means LCA is somewhere above the \\(2^i\\). ancestors of x and y. Then we continue to search LCA of y and x\u2019s ancestors as LCA(L[x][i], L[y][i]) is the same as LCA(x, y). Please notice that we have ensured that depth di\ufb00erence between LCA and both x and y are no longer larger than \\(2^i\\). If we apply this producure until i = 0, we would left with x and y such that parent of x is LCA. Of course, the parent of y would also be LCA.</p>","tags":["Tree","LCA","Lowest Common Ancestors","Binary Lifting"]},{"location":"data-structures/mo-algorithm/","title":"Mo's Algorithm","text":"<p>This method will be a key for solving offline range queries on an array. By offline, we mean we can find the answers of these queries in any order we want and there are no updates. Let\u2019s introduce a problem and construct an efficient solution for it.</p> <p>You have an array a with \\(N\\) elements such that it\u2019s elements ranges from \\(1\\) to \\(M\\). You have to answer \\(Q\\) queries. Each is in the same type. You will be given a range \\([l, r]\\) for each query, you have to print how many different values are there in the subarray \\([a_l , a_{l+1}..a_{r\u22121}, a_r]\\).</p> <p>First let\u2019s find a naive solution and improve it. Remember the frequency array we mentioned before. We will keep a frequency array that contains only given subarray\u2019s values. Number of values in this frequency array bigger than 0 will be our answer for given query. Then we have to update frequency array for next query. We will use \\(\\mathcal{O}(N)\\) time for each query, so total complexity will be \\(\\mathcal{O}(Q \\times N)\\). Look at the code below for implementation.</p> <pre><code>class Query {\n   public:\n    int l, r, ind;\n    Query(int l, int r, int ind) {\n        this-&gt;l = l, this-&gt;r = r, this-&gt;ind = ind;\n    }\n};\n\nvoid del(int ind, vector&lt;int&gt; &amp;a, vector&lt;int&gt; &amp;F, int &amp;num) {\n    if (F[a[ind]] == 1) num--;\n    F[a[ind]]--;\n}\n\nvoid add(int ind, vector&lt;int&gt; &amp;a, vector&lt;int&gt; &amp;F, int &amp;num) {\n    if (F[a[ind]] == 0) num++;\n    F[a[ind]]++;\n}\n\nvector&lt;int&gt; solve(vector&lt;int&gt; &amp;a, vector&lt;Query&gt; &amp;q) {\n    int Q = q.size(), N = a.size();\n    int M = *max_element(a.begin(), a.end());\n    vector&lt;int&gt; F(M + 1, 0);  // This is frequency array we mentioned before\n    vector&lt;int&gt; ans(Q, 0);\n    int l = 0, r = -1, num = 0;\n    for (int i = 0; i &lt; Q; i++) {\n        int nl = q[i].l, nr = q[i].r;\n        while (l &lt; nl) del(l++, a, F, num);\n        while (l &gt; nl) add(--l, a, F, num);\n        while (r &gt; nr) del(r--, a, F, num);\n        while (r &lt; nr) add(++r, a, F, num);\n        ans[q[i].ind] = num;\n    }\n    return ans;\n}\n</code></pre> <p>Time complexity for each query here is \\(\\mathcal{O}(N)\\). So total complexity is \\(\\mathcal{O}(Q \\times N)\\). Just by changing the order of queries we will reduce this complexity to \\(\\mathcal{O}((Q + N) \\times \\sqrt N)\\).</p>","tags":["Data Structures","Mo's Algorithm"]},{"location":"data-structures/mo-algorithm/#mos-algorithm","title":"Mo's Algorithm","text":"<p>We will change the order of answering the queries such that overall complexity will be reduced drastically. We will use following cmp function to sort our queries and will answer them in this sorted order. Block size here is \\(\\mathcal{O}(\\sqrt N)\\).</p> <pre><code>bool operator&lt;(Query other) const {\n    return make_pair(l / block_size, r) &lt;\n        make_pair(other.l / block_size, other.r);\n}\n</code></pre> <p>Why does that work? Let\u2019s examine what we do here first then find the complexity. We divide \\(l\\)'s of queries into blocks. Block number of a given \\(l\\) is \\(l\\) blocksize (integer division). We sort the queries first by their block numbers then for same block numbers, we sort them by their \\(r\\)'s. Sorting all queries will take \\(\\mathcal{O}(Q \\times log{Q})\\) time. Let\u2019s look at how many times we will call add and del operations to change current \\(r\\). For the same block \\(r\\)'s always increases. So for same block it is \\(\\mathcal{O}(N)\\) since it can only increase. Since there are \\(N\\) blocksize blocks in total, it will be \\(\\mathcal{O}(N \\times N / \\text{block\\_size})\\) operations in total. For same block, add and del operations that changes \\(l\\) will be called at most \\(\\mathcal{O}(\\text{block\\_size})\\) times for each query, since if block number is same then their \\(l\\)'s must differ at most by \\(\\mathcal{O}(\\text{block\\_size})\\). So overall it is \\(\\mathcal{O}(Q \\times \\text{block\\_size})\\). Also when consecutive queries has different block numbers we will perform at most \\(\\mathcal{O}(N)\\) operations, but notice that there are at most \\(\\mathcal{O}(N \\div \\text{block\\_size})\\) such consecutive queries, so it doesn't change the overall time complexity. If we pick \\(block\\_size = \\sqrt N\\) overall complexity will be \\(\\mathcal{O}((Q + N) \\times \\sqrt N)\\). Full code is given below.</p> Example for the Algorithm <pre><code>int block_size;\n\nclass Query {\n   public:\n    int l, r, ind;\n    Query(int l, int r, int ind) {\n        this-&gt;l = l, this-&gt;r = r, this-&gt;ind = ind;\n    }\n    bool operator&lt;(Query other) const {\n        return make_pair(l / block_size, r) &lt;\n               make_pair(other.l / block_size, other.r);\n    }\n};\n\nvoid del(int ind, vector&lt;int&gt; &amp;a, vector&lt;int&gt; &amp;F, int &amp;num) {\n    if (F[a[ind]] == 1) num--;\n    F[a[ind]]--;\n}\n\nvoid add(int ind, vector&lt;int&gt; &amp;a, vector&lt;int&gt; &amp;F, int &amp;num) {\n    if (F[a[ind]] == 0) num++;\n    F[a[ind]]++;\n}\n\nvector&lt;int&gt; solve(vector&lt;int&gt; &amp;a, vector&lt;Query&gt; &amp;q) {\n    int Q = q.size(), N = a.size();\n    int M = *max_element(a.begin(), a.end());\n    block_size = sqrt(N);\n    sort(q.begin(), q.end());\n    vector&lt;int&gt; F(M + 1, 0);  // This is frequency array we mentioned before\n    vector&lt;int&gt; ans(Q, 0);\n    int l = 0, r = -1, num = 0;\n    for (int i = 0; i &lt; Q; i++) {\n        int nl = q[i].l, nr = q[i].r;\n        while (l &lt; nl) del(l++, a, F, num);\n        while (l &gt; nl) add(--l, a, F, num);\n        while (r &gt; nr) del(r--, a, F, num);\n        while (r &lt; nr) add(++r, a, F, num);\n        ans[q[i].ind] = num;\n    }\n    return ans;\n}\n</code></pre>","tags":["Data Structures","Mo's Algorithm"]},{"location":"data-structures/prefix-sum/","title":"Prefix Sum","text":"<p>Prefix Sum dizisi bir dizinin prefixlerinin toplamlar\u0131yla olu\u015fturulan bir veri yap\u0131s\u0131d\u0131r. Prefix sum dizisinin \\(i\\) indeksli eleman\u0131 girdi dizisindeki \\(1\\) indeksli elemandan \\(i\\) indeksli elemana kadar olan elemanlar\u0131n toplam\u0131na e\u015fit olacak \u015fekilde kurulur. Ba\u015fka bir deyi\u015fle:</p> <p>\\[sum_i = \\sum_{j=1}^{i} {a_j}\\]</p> <p>\u00d6rnek bir \\(A\\) dizisi i\u00e7in prefix sum dizisi \u015fu \u015fekilde kurulmal\u0131d\u0131r:</p> A Dizisi \\(4\\) \\(6\\) \\(3\\) \\(12\\) \\(1\\) Prefix Sum Dizisi \\(4\\) \\(10\\) \\(13\\) \\(25\\) \\(26\\) \\(4\\) \\(4+6\\) \\(4+6+3\\) \\(4+6+3+12\\) \\(4+6+3+12+1\\) <p>Prefix sum dizisini kullanarak herhangi bir \\([l,r]\\) aral\u0131\u011f\u0131ndaki elemanlar\u0131n toplam\u0131n\u0131 \u015fu \u015fekilde kolayl\u0131kla elde edebiliriz:</p> <p>\\[sum_r = \\sum_{j=1}^{r} {a_j}\\]</p> <p>\\[sum_{l - 1} = \\sum_{j=1}^{l - 1} {a_j}\\]</p> <p>\\[sum_r - sum_{l-1} = \\sum_{j=l}^{r} {a_j}\\]</p>","tags":["Data Structures","Prefix Sum"]},{"location":"data-structures/prefix-sum/#ornek-kod-parcalar","title":"\u00d6rnek Kod Par\u00e7alar\u0131","text":"<p>Prefix Sum dizisini kurarken \\(sum_i = sum_{i - 1} + a_i\\) e\u015fitli\u011fi kolayca g\u00f6r\u00fclebilir ve bu e\u015fitli\u011fi kullanarak \\(sum[]\\) dizisini girdi dizisindeki elemanlar\u0131 s\u0131rayla gezerek kurabiliriz:</p> <pre><code>const int n;\nint sum[n + 1], a[n + 1];\n// a dizisi girdi dizimiz, sum dizisi de prefix sum dizimiz olsun.\n\nvoid build() {\n    for (int i = 1; i &lt;= n; i++)\n        sum[i] = sum[i - 1] + a[i];\n    return;\n}\n\nint query(int l, int r) {\n    return sum[r] - sum[l - 1];\n}\n</code></pre>","tags":["Data Structures","Prefix Sum"]},{"location":"data-structures/prefix-sum/#zaman-karmasklg","title":"Zaman Karma\u015f\u0131kl\u0131\u011f\u0131","text":"<p>Prefix sum dizisini kurma i\u015flemimizin zaman ve haf\u0131za karma\u015f\u0131kl\u0131\u011f\u0131 \\(\\mathcal{O}(N)\\). Her sorguya da \\(\\mathcal{O}(1)\\) karma\u015f\u0131kl\u0131kta cevap verebiliyoruz.</p> <p>Prefix sum veri yap\u0131s\u0131 ile ilgili \u00f6rnek bir probleme buradan ula\u015fabilirsiniz.</p>","tags":["Data Structures","Prefix Sum"]},{"location":"data-structures/queue/","title":"Queue","text":"<p>Queue veri yap\u0131s\u0131nda elemanlar yap\u0131ya ilk giren ilk \u00e7\u0131kar (FIFO) kural\u0131na uygun olacak \u015fekilde saklan\u0131r. Bu veri yap\u0131s\u0131nda uygulayabildigimiz i\u015flemler:</p> <ul> <li>Veri yap\u0131s\u0131n\u0131n en \u00fcst\u00fcne eleman ekleme.</li> <li>Veri yap\u0131s\u0131n\u0131n en alt\u0131ndaki eleman\u0131na eri\u015fim.</li> <li>Veri yap\u0131s\u0131n\u0131n en alt\u0131ndaki eleman\u0131 silme.</li> <li>Veri yap\u0131s\u0131n\u0131n bo\u015f olup olmad\u0131\u011f\u0131n\u0131n kontr\u00f6l\u00fc.</li> </ul> <p>C++ dilindeki STL k\u00fct\u00fcphanesinde bulunan haz\u0131r queue yap\u0131s\u0131n\u0131n kullan\u0131m\u0131 a\u015fa\u011f\u0131daki gibidir:</p> <pre><code>int main() {\n    queue&lt;int&gt; q;\n    cout &lt;&lt; q.empty() &lt;&lt; endl; // Ilk bashta Queue bosh oldugu icin burada True donecektir.\n    q.push(5);                 // Queue'in en ustune 5'i ekler. Queue'in yeni hali: {5}\n    q.push(7);                 // Queue'in en ustune 7'yi ekler. Queue'in yeni hali: {7, 5}\n    q.push(6);                 // Queue'in en ustune 6'yi ekler. Queue'in yeni hali : {6, 7, 5}\n    q.pop();                   // Queue'in en altindaki elemani siler. Queue'in yeni hali : {6, 7}\n    q.push(1);                 // Queue'in en ustune 1'i ekler. Queue'in yeni hali : {1, 6, 7}\n    cout &lt;&lt; Q.front() &lt;&lt; endl; // Queue'in en ustundeki elemana erisir. Ekrana 7 yazdirir.\n}\n</code></pre>","tags":["Data Structures","Queue"]},{"location":"data-structures/segment-tree/","title":"Segment Tree","text":"<p>Segment Tree is a data structure that enables us to answer queries like minimum, maximum, sum etc. for any \\([l,r]\\) interval in \\(\\mathcal{O}(\\log N)\\) time complexity and update these intervals.</p> <p>Segment Tree is more useful than Fenwick Tree and Sparse Table structures because it allows updates on elements and provides the possibility to answer queries like minimum, maximum etc. for any \\([l,r]\\) interval. Also, the memory complexity of Segment Tree is \\(\\mathcal{O}(N)\\) while the memory complexity of the Sparse Table structure is \\(\\mathcal{O}(N \\log N)\\).</p>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#structure-and-construction","title":"Structure and Construction","text":"<p>Segment Tree has a \"Complete Binary Tree\" structure. The leaf nodes of the Segment Tree store the elements of the array, and each internal node's value is calculated with a function that takes its children's values as inputs. Thus, the answers of certain intervals are stored in each node, and the answer of the whole array is stored in the root node. For example, for a Segment Tree structure built for the sum query, the value of each node is equal to the sum of its children's values.</p> segment tree structure to query sum on array $a = [41,67,6,30,85,43,39]$ <pre><code>void build(int ind, int l, int r) {\n    // tree[ind] stores the answer of the interval [l,r]\n    if (l == r) {         // leaf node reached\n        tree[ind] = a[l]; // store the value of the leaf node\n    } else {\n        int mid = (l + r) / 2;\n        build(ind * 2, l, mid);\n        build(ind * 2 + 1, mid + 1, r);\n        // the answer of the interval [l,mid] and [mid + 1,r] is the sum of their answers\n        tree[ind] = tree[ind * 2] + tree[ind * 2 + 1];\n    }\n}\n</code></pre>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#query-and-update-algorithms","title":"Query and Update Algorithms","text":"","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#query-algorithm","title":"Query Algorithm","text":"<p>For any \\([l,r]\\) interval, the query algorithm works as follows: - Divide the \\([l,r]\\) interval into the widest intervals that are stored in the tree. - Merge the answers of these intervals to calculate the desired answer.</p> <p>There are at most \\(2\\) intervals that are needed to calculate the answer at each depth of the tree. Therefore, the query algorithm works in \\(\\mathcal{O}(\\log N)\\) time complexity.</p> on array $a = [41,67,6,30,85,43,39]$ query at $[2,6]$ interval <p>On array \\(a = [41,67,6,30,85,43,39]\\), the answer of the \\([2,6]\\) interval is obtained by merging the answers of the \\([2,3]\\) and \\([4,6]\\) intervals. The answer for the sum query is calculated as \\(36+167=203\\).</p> <pre><code>// [lw,rw] is the interval we are looking for the answer\n// [l,r] is the interval that the current node stores the answer\nint query(int ind, int l, int r, int lw, int rw) {\n    if (l &gt; rw or r &lt; lw) //current interval does not contain the interval we are looking for\n        return 0;\n    if (l &gt;= lw and r &lt;= rw) //current interval is completely inside the interval we are looking for\n        return tree[ind];\n\n    int mid = (l + r) / 2;\n\n    // recursively calculate the answers of all intervals containing the x index\n    return query(ind * 2, l, mid, lw, rw) + query(ind * 2 + 1, mid + 1, r, lw, rw);\n}\n</code></pre>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#update-algorithm","title":"Update Algorithm","text":"<p>Update the value of every node that contains \\(x\\) indexed element.</p> <p>It is sufficient to update the values of at most \\(\\log(N)\\) nodes from the leaf node containing the \\(x\\) indexed element to the root node. Therefore, the time complexity of updating the value of any element is \\(\\mathcal{O}(\\log N)\\).</p> the nodes that should be updated when updating the $5^{th}$ index of the array $a = [41,67,6,30,85,43,39]$ are as follows: <pre><code>void update(int ind, int l, int r, int x, int val) {\n    if (l &gt; x || r &lt; x) // x index is not in the current interval\n        return;\n    if (l == x and r == x) {\n        tree[ind] = val; // update the value of the leaf node\n        return;\n    }\n\n    int mid = (l + r) / 2;\n\n    // recursively update the values of all nodes containing the x index\n    update(ind * 2, l, mid, x, val);\n    update(ind * 2 + 1, mid + 1, r, x, val);\n    tree[ind] = tree[ind * 2] + tree[ind * 2 + 1];\n}\n</code></pre> <p>A sample problem related to the Segment Tree data structure can be found here.</p>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#segment-tree-with-lazy-propagation","title":"Segment Tree with Lazy Propagation","text":"<p>Previously, update function was called to update only a single value in array. Please note that a single value update in array may cause changes in multiple nodes in Segment Tree as there may be many segment tree nodes that have this changed single element in it\u2019s range.</p>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#lazy-propogation-algorithm","title":"Lazy Propogation Algorithm","text":"<p>We need a structure that can perform following operations on an array \\([1,N]\\).</p> <ul> <li>Add inc to all elements in the given range \\([l, r]\\).</li> <li>Return the sum of all elements in the given range \\([l, r]\\).</li> </ul> <p>Notice that if update was for single element, we could use the segment tree we have learned before. Trivial structure comes to mind is to use an array and do the operations by traversing and increasing the elements one by one. Both operations would take \\(\\mathcal{O}(L)\\) time complexity in this structure where \\(L\\) is the number of elements in the given range.</p> <p>Let\u2019s use segment tree\u2019s we have learned. Second operation is easy, We can do it in \\(\\mathcal{O}(\\log N)\\). What about the first operation. Since we can do only single element update in the regular segment tree, we have to update all elements in the given range one by one. Thus we have to perform update operation \\(L\\) times. This works in \\(\\mathcal{O}(L \\times \\log N)\\) for each range update. This looks bad, even worse than just using an array in a lot of cases.</p> <p>So we need a better structure. People developed a trick called lazy propagation to perform range updates on a structure that can perform single update (This trick can be used in segment trees, treaps, k-d trees ...).</p> <p>Trick is to be lazy i.e, do work only when needed. Do the updates only when you have to. Using Lazy Propagation we can do range updates in \\(\\mathcal{O}(\\log N)\\) on standart segment tree. This is definitely fast enough.</p>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#updates-using-lazy-propogation","title":"Updates Using Lazy Propogation","text":"<p>Let\u2019s be lazy as told, when we need to update an interval, we will update a node and mark its children that it needs to be updated and update them when needed. For this we need an array \\(lazy[]\\) of the same size as that of segment tree. Initially all the elements of the \\(lazy[]\\) array will be \\(0\\) representing that there is no pending update. If there is non-zero element \\(lazy[k]\\) then this element needs to update node k in the segment tree before making any query operation, then \\(lazy[2\\cdot k]\\) and \\(lazy[2 \\cdot k + 1]\\) must be also updated correspondingly.</p> <p>To update an interval we will keep 3 things in mind.</p> <ul> <li>If current segment tree node has any pending update, then first add that pending update to current node and push the update to it\u2019s children.</li> <li>If the interval represented by current node lies completely in the interval to update, then update the current node and update the \\(lazy[]\\) array for children nodes.</li> <li>If the interval represented by current node overlaps with the interval to update, then update the nodes as the earlier update function.</li> </ul> <pre><code>void update(int node, int start, int end, int l, int r, int val) {\n    // If there's a pending update on the current node, apply it\n    if (lazy[node] != 0) {\n        tree[node] += (end - start + 1) * lazy[node]; // Apply the pending update\n        // If not a leaf node, propagate the lazy update to the children\n        if (start != end) {\n            lazy[2 * node] += lazy[node];\n            lazy[2 * node + 1] += lazy[node];\n        }\n        lazy[node] = 0; // Clear the pending update\n    }\n\n    // If the current interval [start, end] does not intersect with [l, r], return\n    if (start &gt; r || end &lt; l) {\n        return;\n    }\n\n    // If the current interval [start, end] is completely within [l, r], apply the update\n    if (l &lt;= start &amp;&amp; end &lt;= r) {\n        tree[node] += (end - start + 1) * val; // Update the segment\n        // If not a leaf node, propagate the update to the children\n        if (start != end) {\n            lazy[2 * node] += val;\n            lazy[2 * node + 1] += val;\n        }\n        return;\n    }\n\n    // Otherwise, split the interval and update both halves\n    int mid = (start + end) / 2;\n    update(2 * node, start, mid, l, r, val);\n    update(2 * node + 1, mid + 1, end, l, r, val);\n\n    // After updating the children, recalculate the current node's value\n    tree[node] = tree[2 * node] + tree[2 * node + 1];\n}\n</code></pre> <p>This is the update function for given problem. Notice that when we arrive a node, all the updates that we postponed that would effect this node will be performed since we are pushing them downwards as we go to this node. Thus this node will keep the exact values when the range updates are done without lazy. So it\u2019s seems like it is working. How about queries?</p>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#queries-using-lazy-propogation","title":"Queries Using Lazy Propogation","text":"<p>Since we have changed the update function to postpone the update operation, we will have to change the query function as well. The only change we need to make is to check if there is any pending update operation on that node. If there is a pending update, first update the node and then proceed the same way as the earlier query function. As mentioned in the previous subsection, all the postponed updates that would affect this node will be performed before we reach it. Therefore, the sum value we look for will be correct.</p> <p><pre><code>int query(int node, int start, int end, int l, int r) {\n    // If the current interval [start, end] does not intersect with [l, r], return 0\n    if (start &gt; r || end &lt; l) {\n        return 0;\n    }\n\n    // If there's a pending update on the current node, apply it\n    if (lazy[node] != 0) {\n        tree[node] += (end - start + 1) * lazy[node]; // Apply the pending update\n        // If not a leaf node, propagate the lazy update to the children\n        if (start != end) {\n            lazy[2 * node] += lazy[node];\n            lazy[2 * node + 1] += lazy[node];\n        }\n        lazy[node] = 0; // Clear the pending update\n    }\n\n    // If the current interval [start, end] is completely within [l, r], return the value\n    if (l &lt;= start &amp;&amp; end &lt;= r) {\n        return tree[node];\n    }\n\n    // Otherwise, split the interval and query both halves\n    int mid = (start + end) / 2;\n    int p1 = query(2 * node, start, mid, l, r);       // Query the left child\n    int p2 = query(2 * node + 1, mid + 1, end, l, r); // Query the right child\n\n    // Combine the results from the left and right child nodes\n    return (p1 + p2);\n}\n</code></pre> Notice that the only difference with the regular query function is pushing the lazy values downwards as we traverse. This is a widely used trick applicable to various problems, though not all range problems. You may notice that we leveraged properties of addition here. The associative property of addition allows merging multiple updates in the lazy array without considering their order. This assumption is crucial for lazy propagation. Other necessary properties are left as an exercise to the reader.</p>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#binary-search-on-segment-tree","title":"Binary Search on Segment Tree","text":"<p>Assume we have an array A that contains elements between 1 and \\(M\\). We have to perform 2 kinds of operations.</p> <ul> <li>Change the value of the element in given index i by x.</li> <li>Return the value of the kth element on the array when sorted.</li> </ul>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#how-to-solve-it-naively","title":"How to Solve It Naively","text":"<p>Let\u2019s construct a frequency array, \\(F[i]\\) will keep how many times number i occurs in our original array. So we want to find smallest \\(i\\) such that \\(\\sum_{j=1}^{i} F[i] \\geq k\\). Then the number \\(i\\) will be our answer for the query. And for updates we just have to change \\(F\\) array accordingly.</p> A naive update example <p>This is the naive algorithm. Update is O(1) and query is O(M).</p> <pre><code>void update(int i, int x) {\n    F[A[i]]--;\n    F[A[i] = x]++;\n}\n\nint query(int k) {\n    int sum = 0, ans = 0;\n    // Iterate through the frequency array F to find the smallest value\n    // for which the cumulative frequency is at least k\n    for (int i = 1; i &lt;= M; i++) {\n        sum += F[i]; // Add the frequency of F[i] to the cumulative sum\n        if (sum &gt;= k) {\n            return i;\n        }\n    }\n}\n</code></pre>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#how-to-solve-it-with-segment-tree","title":"How to Solve It With Segment Tree","text":"<p>This is of course, slow. Let\u2019s use segment tree\u2019s to improve it. First we will construct a segment tree on \\(F\\) array. Segment tree will perform single element updates and range sum queries. We will use binary search to find corresponding \\(i\\) for \\(k^{th}\\) element queries.</p> Segment Tree After First Update <pre><code>void update(int i, int x) {\n    update(1, 1, M, A[i], --F[A[i]]); // Decrement frequency of old value\n    A[i] = x;                        // Update A[i] to new value\n    update(1, 1, M, A[i], ++F[A[i]]); // Increment frequency of new value\n}\n\nint query(int k) {\n    int l = 1, r = m; // Initialize binary search range\n    while (l &lt; r) {\n        int mid = (l + r) / 2; \n        if (query(1, 1, M, 1, mid) &lt; k)\n            l = mid + 1; // Move lower bound up\n        else\n            r = mid; // Move upper bound down\n    }\n    return l; // Return index where cumulative frequency is at least k\n}\n</code></pre> <p>If you look at the code above you can notice that each update takes \\(\\mathcal{O}(\\log M)\\) time and each query takes \\(\\mathcal{O}(\\log^{2} M)\\) time, but we can do better.</p>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/segment-tree/#how-to-speed-up","title":"How To Speed Up?","text":"<p>If you look at the segment tree solution on preceding subsection you can see that queries are performed in \\(\\mathcal{O}(\\log^{2} M)\\) time. We can make is faster, actually we can reduce the time complexity to \\(\\mathcal{O}(\\log M)\\) which is same with the time complexity for updates. We will do the binary search when we are traversing the segment tree. We first will start from the root and look at its left child\u2019s sum value, if this value is greater than k, this means our answer is somewhere in the left child\u2019s subtree. Otherwise it is somewhere in the right child\u2019s subtree. We will follow a path using this rule until we reach a leaf, then this will be our answer. Since we just traversed \\(\\mathcal{O}(\\log M)\\) nodes (one node at each level), time complexity will be \\(\\mathcal{O}(\\log M)\\). Look at the code below for better understanding.</p> Solution of First Query <pre><code>void update(int i, int x) {\n    update(1, 1, M, A[i], --F[A[i]]); // Decrement frequency of old value\n    A[i] = x;                         // Update A[i] to new value\n    update(1, 1, M, A[i], ++F[A[i]]); // Increment frequency of new value\n}\n\nint query(int node, int start, int end, int k) {\n    if (start == end) return start; // Leaf node, return the index\n    int mid = (start + end) / 2; \n    if (tree[2 * node] &gt;= k)\n        return query(2 * node, start, mid, k); // Search in left child\n    return query(2 * node + 1, mid + 1, end, k - tree[2 * node]); // Search in right child\n}\n\nint query(int k) {\n    return query(1, 1, M, k); // Public interface for querying\n}\n</code></pre>","tags":["Data Structures","Segment Tree"]},{"location":"data-structures/sparse-table/","title":"Sparse Table","text":"<p>Sparse table aral\u0131klardaki elemanlar\u0131n toplam\u0131, minimumu, maksimumu ve EBOB'lar\u0131 gibi sorgulara \\(\\mathcal{O}(\\log N)\\) zaman karma\u015f\u0131kl\u0131\u011f\u0131nda cevap alabilmemizi sa\u011flayan bir veri yap\u0131s\u0131d\u0131r. Baz\u0131 tip sorgular (aral\u0131ktaki minimum, maksimum say\u0131y\u0131 bulma gibi) ise \\(\\mathcal{O}(1)\\) zaman karma\u015f\u0131kl\u0131\u011f\u0131nda yapmaya uygundur.</p> <p>Bu veri yap\u0131s\u0131 durumu de\u011fi\u015fmeyen, sabit bir veri \u00fczerinde \u00f6n i\u015flemler yaparak kurulur. Dinamik veriler i\u00e7in kullan\u0131\u015fl\u0131 de\u011fildir. Veri \u00fczerinde herhangi bir de\u011fi\u015fiklik durumda Sparse table tekrardan kurulmal\u0131d\u0131r. Bu da maliyetli bir durumdur.</p>","tags":["Data Structures","Sparse Table"]},{"location":"data-structures/sparse-table/#yaps-ve-kurulusu","title":"Yap\u0131s\u0131 ve Kurulu\u015fu","text":"<p>Sparse table iki bouyutlu bir dizi \u015feklinde, \\(\\mathcal{O}(N\\log N)\\) haf\u0131za karma\u015f\u0131kl\u0131\u011f\u0131na sahip bir veri yap\u0131s\u0131d\u0131r. Dizinin her eleman\u0131ndan \\(2\\)'nin kuvvetleri uzakl\u0131ktaki elemanlara kadar olan cevaplar Sparse table'da saklan\u0131r. \\(ST_{x,i}\\), \\(x\\) indeksli elemandan \\(x + 2^i - 1\\) indeksli elemana kadar olan aral\u0131\u011f\u0131n cevab\u0131n\u0131 saklayacak \u015fekilde sparse table kurulur.</p> <pre><code>// Toplam sorgusu icin kurulmus Sparse Table Yapisi\nconst int n;\nconst int LOG = log2(n);\nint a[n + 1], ST[2 * n][LOG + 1];\n\nvoid build() {\n    for (int i = 1; i &lt;= n; i++) {\n        // [i,i] araliginin cevabi dizinin i indeksli elemanina esittir.\n        ST[i][0] = a[i];\n    }\n\n    for (int i = 1; i &lt;= LOG; i++)\n        for (int j = 1; j &lt;= n; j++) {\n            // [i,i+2^(j)-1] araliginin cevabi\n            // [i,i+2^(j - 1) - 1] araligi ile [i+2^(j - 1),i+2^j-1] araliginin\n            // cevaplarinin birlesmesiyle elde edilir\n            ST[i][j] = ST[i][j - 1] + ST[i + (1 &lt;&lt; (j - 1))][j - 1];\n        }\n\n    return;\n}\n</code></pre>","tags":["Data Structures","Sparse Table"]},{"location":"data-structures/sparse-table/#sorgu-algoritmas","title":"Sorgu Algoritmas\u0131","text":"<p>Herhangi bir \\([l,r]\\) aral\u0131\u011f\u0131 i\u00e7in sorgu algoritmas\u0131 s\u0131ras\u0131yla \u015fu \u015fekilde \u00e7al\u0131\u015f\u0131r:</p> <ul> <li>\\([l,r]\\) aral\u0131\u011f\u0131n\u0131 cevaplar\u0131n\u0131 \u00f6nceden hesaplad\u0131\u011f\u0131m\u0131z aral\u0131klara par\u00e7ala.<ul> <li>Sadece \\(2\\)'nin kuvveti uzunlu\u011funda par\u00e7alar\u0131n cevaplar\u0131n\u0131 saklad\u0131\u011f\u0131m\u0131z i\u00e7in aral\u0131\u011f\u0131m\u0131z\u0131 \\(2\\)'nin kuvveti uzunlu\u011funda aral\u0131klara ay\u0131rmal\u0131y\u0131z. \\([l,r]\\) aral\u0131\u011f\u0131n\u0131n uzunlu\u011funun ikilik tabanda yazd\u0131\u011f\u0131m\u0131zda hangi aral\u0131klara par\u00e7alamam\u0131z gerekti\u011fini bulmu\u015f oluruz.</li> </ul> </li> <li>Bu aral\u0131klardan gelen cevaplar\u0131 birle\u015ftirerek \\([l,r]\\) aral\u0131\u011f\u0131n\u0131n cevab\u0131n\u0131 hesapla.</li> </ul> <p>Herhangi bir aral\u0131\u011f\u0131n uzunlu\u011funun ikilik tabandaki yaz\u0131l\u0131\u015f\u0131ndaki \\(1\\) rakamlar\u0131n\u0131n say\u0131s\u0131 en fazla \\(\\log(N)\\) olabilece\u011finden par\u00e7alayaca\u011f\u0131m\u0131z aral\u0131k say\u0131s\u0131 da en fazla \\(\\log(N)\\) olur. Dolay\u0131s\u0131yla sorgu i\u015flemimiz \\(\\mathcal{O}(\\log N)\\) zaman karma\u015f\u0131kl\u0131\u011f\u0131nda \u00e7al\u0131\u015f\u0131r.</p> <p>\u00d6rne\u011fin: \\([4,17]\\) aral\u0131\u011f\u0131n\u0131n cevab\u0131n\u0131 hesaplamak i\u00e7in algoritmam\u0131z \\([4,17]\\) aral\u0131\u011f\u0131n\u0131 \\([4,11]\\), \\([12,15]\\) ve \\([16,17]\\) aral\u0131klar\u0131na ay\u0131r\u0131r ve bu \\(3\\) aral\u0131ktan gelen cevaplar\u0131 birle\u015ftirerek istenilen cevab\u0131 hesaplar.</p> <pre><code>// toplam sorgusu\nint query(int l, int r) {\n    int res = 0;\n\n    for (int i = LOG; i &gt;= 0; i--) {\n        // her seferinde uzunlugu r - l + 1 gecmeyecek\n        // en buyuk araligin cevabi ekleyip l'i o araligin sonuna cekiyoruz.\n        if (l + (1 &lt;&lt; i) &lt;= r) {\n            res += ST[l][i];\n            l += (1 &lt;&lt; i);\n        }\n    }\n\n    return res;\n}\n</code></pre>","tags":["Data Structures","Sparse Table"]},{"location":"data-structures/sparse-table/#minimum-ve-maksimum-sorgu","title":"Minimum ve Maksimum Sorgu","text":"<p>Sparse Table veri yap\u0131s\u0131n\u0131n di\u011fer veri yap\u0131lar\u0131ndan farkl\u0131 olarak \\(\\mathcal{O}(1)\\) zaman karma\u015f\u0131kl\u0131\u011f\u0131nda aral\u0131klarda minimum veya maksimum sorgusu yapabilmesi en avantajl\u0131 \u00f6zelli\u011fidir.</p> <p>Herhangi bir aral\u0131\u011f\u0131n cevab\u0131n\u0131 hesaplarken bu aral\u0131ktaki herhangi bir eleman\u0131 birden fazla kez de\u011ferlendirmemiz cevab\u0131 etkilemez. Bu durum aral\u0131\u011f\u0131m\u0131z\u0131 \\(2\\)'nin kuvveti uzunlu\u011funda maksimum \\(2\\) adet aral\u0131\u011fa b\u00f6lebilmemize ve bu aral\u0131klar\u0131n cevaplar\u0131n\u0131 \\(\\mathcal{O}(1)\\) zaman karma\u015f\u0131kl\u0131\u011f\u0131nda birle\u015ftirebilmemize olanak sa\u011flar.</p> <pre><code>int RMQ(int l, int r) {\n    // log[] dizisinde her sayinin onceden hesapadigimiz log2 degerleri saklidir.\n    int j = log[r - l + 1];\n    return min(ST[l][j], ST[r - (1 &lt;&lt; j) + 1][j]);\n}\n</code></pre> <p>Sparse Table veri yap\u0131s\u0131 ile ilgili \u00f6rnek bir probleme buradan ula\u015fabilirsiniz.</p>","tags":["Data Structures","Sparse Table"]},{"location":"data-structures/sqrt-decomposition/","title":"SQRT Decomposition","text":"<p>Square Root Decomposition algoritmas\u0131 dizi \u00fczerinde \\(\\mathcal{O}(\\sqrt{N})\\) zaman karma\u015f\u0131kl\u0131\u011f\u0131nda sorgu yapabilmemize ve \\(\\mathcal{O}(1)\\) zaman karma\u015f\u0131kl\u0131\u011f\u0131nda ise de\u011fi\u015fiklik yapabilmemize olanak sa\u011flayan bir veri yaps\u0131d\u0131r.</p>","tags":["Data Structures","SQRT Decomposition","Square Root Decomposition"]},{"location":"data-structures/sqrt-decomposition/#yaps-ve-kurulusu","title":"Yap\u0131s\u0131 ve Kurulu\u015fu","text":"<p>Dizinin elemanlar\u0131 her biri yakla\u015f\u0131k \\(\\mathcal{O}(\\sqrt{N})\\) uzunlu\u011funda bloklar halinde par\u00e7alan\u0131r. Her bir blokun cevab\u0131 ayr\u0131 ayr\u0131 hesaplan\u0131r ve bir dizide saklan\u0131r.</p> Bloklar\u0131n Cevaplar\u0131 \\(21\\) \\(13\\) \\(50\\) \\(32\\) Dizideki Elemanlar \\(3\\) \\(6\\) \\(2\\) \\(10\\) \\(3\\) \\(1\\) \\(4\\) \\(5\\) \\(2\\) \\(7\\) \\(37\\) \\(4\\) \\(11\\) \\(6\\) \\(8\\) \\(7\\) Elemanlar\u0131n \u0130ndeksleri \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(6\\) \\(7\\) \\(8\\) \\(9\\) \\(10\\) \\(11\\) \\(12\\) \\(13\\) \\(14\\) \\(15\\) \\(16\\) \u00d6rnek bir dizi \u00fczerinde toplam sorgusu i\u00e7in kurulmu\u015f SQRT Decompostion veri yap\u0131s\u0131. <pre><code>void build() {\n    for (int i = 1; i &lt;= n; i++) {\n        if (i % sq == 1) { // sq = sqrt(n)\n            t++;           // yeni blok baslangici.\n            st[t] = i;     // t.blok i indisli elemanda baslar.\n        }\n        fn[t] = i;      // t.blokun bitisini i indisli eleman olarak guncelliyoruz.\n        wh[i] = t;      // i indeksli eleman t.blogun icinde.\n        sum[t] += a[i]; // t. blokun cevabina i indeksli elemani ekliyoruz.\n    }\n}\n</code></pre>","tags":["Data Structures","SQRT Decomposition","Square Root Decomposition"]},{"location":"data-structures/sqrt-decomposition/#sorgu-algoritmas","title":"Sorgu Algoritmas\u0131","text":"<p>Herhangi bir \\([l,r]\\) aral\u0131\u011f\u0131 i\u00e7in sorgu algoritmas\u0131 s\u0131ras\u0131 ile \u015fu \u015fekilde \u00e7al\u0131\u015f\u0131r:</p> <ol> <li>Cevab\u0131n\u0131 arad\u0131\u011f\u0131m\u0131z aral\u0131\u011f\u0131n tamamen kaplad\u0131\u011f\u0131 bloklar\u0131n cevab\u0131n\u0131 cevab\u0131m\u0131za ekliyoruz.</li> <li>Tamamen kaplamad\u0131\u011f\u0131 bloklardaki aral\u0131\u011f\u0131m\u0131z\u0131n i\u00e7inde olan elemanlar\u0131 tek tek gezerek cevab\u0131m\u0131za ekliyoruz.</li> </ol> <p>Cevab\u0131n\u0131 arad\u0131\u011f\u0131m\u0131z aral\u0131\u011f\u0131n kapsad\u0131\u011f\u0131 blok say\u0131s\u0131 en fazla \\(\\sqrt{N}\\) olabilece\u011finden \\(1.\\) i\u015flem en fazla \\(\\sqrt{N}\\) kez \u00e7al\u0131\u015f\u0131r. Tamamen kaplamad\u0131\u011f\u0131 ancak baz\u0131 elemanlar\u0131 i\u00e7eren en fazla \\(2\\) adet blok olabilir. (Biri en solda di\u011feri en sa\u011fda olacak \u015fekilde.) Bu \\(2\\) blok i\u00e7in de gezmemiz gereken eleman say\u0131s\u0131 maksimum \\(2\\sqrt{N}\\) oldu\u011fundan bir sorgu i\u015fleminde en fazla \\(3\\sqrt{N}\\) i\u015flem yap\u0131l\u0131r, dolay\u0131s\u0131yla sorgu i\u015flemimiz \\(\\mathcal{O}(\\sqrt{N})\\) zaman karma\u015f\u0131kl\u0131\u011f\u0131nda cal\u0131\u015f\u0131r.</p> Bloklar\u0131n Cevaplar\u0131 \\(21\\) \\(13\\) \\(50\\) \\(32\\) Dizideki Elemanlar \\(3\\) \\(6\\) \\(2\\) \\(10\\) \\(3\\) \\(1\\) \\(4\\) \\(5\\) \\(2\\) \\(7\\) \\(37\\) \\(4\\) \\(11\\) \\(6\\) \\(8\\) \\(7\\) Elemanlar\u0131n \u0130ndeksleri \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(6\\) \\(7\\) \\(8\\) \\(9\\) \\(10\\) \\(11\\) \\(12\\) \\(13\\) \\(14\\) \\(15\\) \\(16\\) \u00d6rnek dizideki \\([3,13]\\) aral\u0131\u011f\u0131n\u0131n cevab\u0131n\u0131 \\(2.\\) ve \\(3.\\) bloklar\u0131n cevaplar\u0131 ile \\(3,4\\) ve \\(11\\) indeksli elemanlar\u0131n toplanmas\u0131yla elde edilir. <pre><code>// [l,r] araligindaki elemanlarin toplamini hesaplayan fonksiyon.\nint query(int l, int r) {\n    int res = 0;\n\n    if (wh[l] == wh[r]) { // l ve r ayni blogun icindeyse\n        for (int i = l; i &lt;= r; i++)\n            res += a[i];\n    } else {\n        for (int i = wh[l] + 1; i &lt;= wh[r] - 1; i++)\n            res += sum[i]; // tamamen kapladigimiz bloklarin cevaplarini ekliyoruz.\n\n        // tamamen kaplamadigimiz bloklardaki araligimiz icindeki\n        // elemanlarin cevaplarini ekliyoruz.\n\n        for (int i = st[wh[l]]; i &lt;= fn[wh[l]]; i++)\n            if (i &gt;= l &amp;&amp; i &lt;= r)\n                res += a[i];\n\n        for (int i = st[wh[r]]; i &lt;= fn[wh[r]]; i++)\n            if (i &gt;= l &amp;&amp; i &lt;= r)\n                res += a[i];\n    }\n\n    return res;\n}\n</code></pre>","tags":["Data Structures","SQRT Decomposition","Square Root Decomposition"]},{"location":"data-structures/sqrt-decomposition/#eleman-guncelleme-algoritmas","title":"Eleman G\u00fcncelleme Algoritmas\u0131","text":"<p>Herhangi bir eleman\u0131n de\u011ferini g\u00fcncellerken o eleman\u0131 i\u00e7eren blokun de\u011ferini g\u00fcncellememiz yeterli olacakt\u0131r. Dolay\u0131s\u0131yla g\u00fcncelleme i\u015flemimimiz \\(\\mathcal{O}(1)\\) zaman karma\u015f\u0131kl\u0131\u011f\u0131nda \u00e7al\u0131\u015f\u0131r.</p> <pre><code>void update(int x, int val) {\n    // x indeksli elemanin yeni degerini val degerine esitliyoruz.\n    sum[wh[x]] -= a[x];\n    a[x] = val;\n    sum[wh[x]] += a[x];\n}\n</code></pre> <p>SQRT Decomposition veri yap\u0131s\u0131 ile ilgili \u00f6rnek bir probleme buradan ula\u015fabilirsiniz.</p>","tags":["Data Structures","SQRT Decomposition","Square Root Decomposition"]},{"location":"data-structures/stack/","title":"Stack","text":"<p>Stack veri yap\u0131s\u0131nda elemanlar yap\u0131ya son giren ilk \u00e7\u0131kar (LIFO) kural\u0131na uygun olacak \u015fekilde saklan\u0131r. Bu veri yap\u0131s\u0131nda uygulayabildi\u011fimiz i\u015flemler:</p> <ul> <li>Veri yap\u0131s\u0131n\u0131n en \u00fcst\u00fcne eleman ekleme.</li> <li>Veri yap\u0131s\u0131n\u0131n en \u00fcst\u00fcndeki elemana eri\u015fim.</li> <li>Veri yap\u0131s\u0131n\u0131n en \u00fcst\u00fcndeki eleman\u0131 silme.</li> <li>Veri yap\u0131s\u0131n\u0131n bo\u015f olup olmad\u0131\u011f\u0131n\u0131n kontr\u00f6l\u00fc.</li> </ul> <p>C++ dilindeki STL k\u00fct\u00fcphanesinde bulunan haz\u0131r stack yap\u0131s\u0131n\u0131n kullan\u0131m\u0131 a\u015fa\u011f\u0131daki gibidir:</p> <pre><code>int main() {\n    stack&lt;int&gt; st;\n    cout &lt;&lt; st.empty() &lt;&lt; endl; // Ilk bashta Stack bosh oldugu icin burada True donecektir.\n    st.push(5);                 // Stack'in en ustune 5'i ekler. Stack'in yeni hali: {5}\n    st.push(7);                 // Stack'in en ustune 7'yi ekler. Stack'in yeni hali: {7, 5}\n    st.push(6);                 // Stack'in en ustune 6'yi ekler. Stack'in yeni hali : {6, 7, 5}\n    st.pop();                   // Stack'in en ustundeki elemani siler. Stack'in yeni hali : {7, 5}\n    st.push(1);                 // Stack'in en ustune 1'i ekler. Stack'in yeni hali : {1, 7, 5}\n    cout &lt;&lt; st.top() &lt;&lt; endl;   // Stack'in en ustundeki elemana erisir. Ekrana 1 yazirir.\n    cout &lt;&lt; st.empty() &lt;&lt; endl; // Burada Stack bosh olmadigindan oturu False donecektir.\n}\n</code></pre>","tags":["Data Structures","Stack"]},{"location":"data-structures/trie/","title":"Trie","text":"<p>Trie is an efficient information reTrieval data structure. Using Trie, search complexities can be brought to optimal limit (key length). If we store keys in binary search tree, a well balanced BST will need time proportional to \\(M \\times log N\\), where \\(M\\) is maximum string length and \\(N\\) is number of keys in tree. Using Trie, we can search the key in \\(\\mathcal{O}(M)\\) time. However the penalty is on Trie storage requirements (Please refer Applications of Trie for more details)</p> Trie Structure. https://www.geeksforgeeks.org/wp-content/uploads/Trie.png <p>Every node of Trie consists of multiple branches. Each branch represents a possible character of keys. We need to mark the last node of every key as end of word node. A Trie node field isEndOfWord is used to distinguish the node as end of word node. A simple structure to represent nodes of English alphabet can be as following,</p> <pre><code>// Trie node\nclass TrieNode {\n   public:\n    TrieNode *children[ALPHABET_SIZE];\n    bool isEndOfWord;\n    TrieNode() {\n        isEndOfWord = false;\n        for (int i = 0; i &lt; ALPHABET SIZE; i++)\n            children[i] = NULL;\n    }\n};\n</code></pre>","tags":["Data Structures","Trie"]},{"location":"data-structures/trie/#insertion","title":"Insertion","text":"<p>Inserting a key into Trie is simple approach. Every character of input key is inserted as an individual Trie node. Note that the children is an array of pointers (or references) to next level Trie nodes. The key character acts as an index into the array children. If the input key is new or an extension of existing key, we need to construct non-existing nodes of the key, and mark end of word for last node. If the input key is prefix of existing key in Trie, we simply mark the last node of key as end of word. The key length determines Trie depth.</p> <pre><code>void insert(struct TrieNode *root, string key) {\n    struct TrieNode *pCrawl = root;\n    for (int i = 0; i &lt; key.length(); i++) {\n        int index = key[i] - 'a';\n        if (!pCrawl-&gt;children[index])\n            pCrawl-&gt;children[index] = new TrieNode;\n        pCrawl = pCrawl-&gt;children[index];\n    }\n    pCrawl-&gt;isEndOfWord = true;\n}\n</code></pre>","tags":["Data Structures","Trie"]},{"location":"data-structures/trie/#search","title":"Search","text":"<p>Searching for a key is similar to insert operation, however we only compare the characters and move down. The search can terminate due to end of string or lack of key in Trie. In the former case, if the isEndofWord field of last node is true, then the key exists in Trie. In the second case, the search terminates without examining all the characters of key, since the key is not present in Trie.</p> <pre><code>bool search(struct TrieNode *root, string key) {\n    TrieNode *pCrawl = root;\n    for (int i = 0; i &lt; key.length(); i++) {\n        int index = key[i] - 'a';\n        if (!pCrawl-&gt;children[index])\n            return false;\n        pCrawl = pCrawl-&gt;children[index];\n    }\n    return (pCrawl != NULL &amp;&amp; pCrawl-&gt;isEndOfWord);\n}\n</code></pre> <p>Insert and search costs \\(\\mathcal{O}(\\text{key\\_length})\\). However the memory requirements of Trie high. It is \\(\\mathcal{O}(\\text{ALPHABET SIZE} \\times \\text{key\\_length} \\times N)\\) where \\(N\\) is number of keys in Trie. There are efficient representation of trie nodes (e.g. compressed trie, ternary search tree, etc.) to minimize memory requirements of trie.</p>","tags":["Data Structures","Trie"]},{"location":"dynamic-programming/","title":"Dynamic Programming","text":"<p>Editor: Halil \u00c7etiner</p> <p>Reviewers: Onur Y\u0131ld\u0131z</p>","tags":["Dynamic Programming"]},{"location":"dynamic-programming/#introduction","title":"Introduction","text":"<p>Next section is about the Greedy Algorithms and Dynamic Programming. It will be quite a generous introduction to the concepts and will be followed by some common problems.</p>","tags":["Dynamic Programming"]},{"location":"dynamic-programming/#greedy-algorithms","title":"Greedy Algorithms","text":"","tags":["Dynamic Programming"]},{"location":"dynamic-programming/#dynamic-programming","title":"Dynamic Programming","text":"","tags":["Dynamic Programming"]},{"location":"dynamic-programming/#common-dp-problems","title":"Common DP Problems","text":"","tags":["Dynamic Programming"]},{"location":"dynamic-programming/#bitmask-dp","title":"Bitmask DP","text":"","tags":["Dynamic Programming"]},{"location":"dynamic-programming/#dp-on-rooted-trees","title":"DP on Rooted Trees","text":"","tags":["Dynamic Programming"]},{"location":"dynamic-programming/#dp-on-directed-acyclic-graphs","title":"DP on Directed Acyclic Graphs","text":"","tags":["Dynamic Programming"]},{"location":"dynamic-programming/#digit-dp","title":"Digit DP","text":"","tags":["Dynamic Programming"]},{"location":"dynamic-programming/#walk-counting-using-matrix-exponentiation","title":"Walk Counting using Matrix Exponentiation","text":"","tags":["Dynamic Programming"]},{"location":"dynamic-programming/#tree-child-sibling-notation","title":"Tree Child-Sibling Notation","text":"","tags":["Dynamic Programming"]},{"location":"dynamic-programming/#references","title":"References","text":"<ol> <li>\"Competitive Programmer\u2019s Handbook\" by Antti Laaksonen - Draft July 3, 2018</li> <li>Wikipedia - Dynamic Programming</li> <li>Topcoder - Competitive Programming Community / Dynamic Programming from Novice to Advanced</li> <li>Hacker Earth - Dynamic Programming</li> <li>Geeks for Geeks - Dynamic Programming</li> </ol>","tags":["Dynamic Programming"]},{"location":"dynamic-programming/bitmask-dp/","title":"Bitmask DP","text":"","tags":["Dynamic Programming","Bitmask DP"]},{"location":"dynamic-programming/bitmask-dp/#what-is-bitmask","title":"What is Bitmask?","text":"<p>Let\u2019s say that we have a set of objects. How can we represent a subset of this set? One way is using a map and mapping each object with a Boolean value indicating whether the object is picked. Another way is, if the objects can be indexed by integers, we can use a Boolean array. However, this can be slow due to the operations of the map and array structures. If the size of the set is not too large (less than 64), a bitmask is much more useful and convenient.</p> <p>An integer is a sequence of bits. Thus, we can use integers to represent a small set of Boolean values. We can perform all the set operations using bit operations. These bit operations are faster than map and array operations, and the time difference may be significant in some problems.</p> <p>In a bitmask, the \\( i \\)-th bit from the right represents the \\( i \\)-th object. For example, let \\( A = \\{1, 2, 3, 4, 5\\} \\), we can represent \\( B = \\{1, 2, 4\\} \\) with the 11 (01011) bitmask.</p>","tags":["Dynamic Programming","Bitmask DP"]},{"location":"dynamic-programming/bitmask-dp/#bitmask-operations","title":"Bitmask Operations","text":"<ul> <li> <p>Add the \\( i \\)-th object to the subset:   Set the \\( i \\)-th bit to 1: \\( \\text{mask } = \\text{mask } | \\text{ } (1 &lt;&lt; i) \\)</p> </li> <li> <p>Remove the \\( i \\)-th object from the subset:   Set the \\( i \\)-th bit to 0: \\( \\text{mask } = \\text{mask } \\&amp; \\sim (1 &lt;&lt; i) \\)</p> </li> <li> <p>Check whether the \\( i \\)-th object is in the subset:   Check if the \\( i \\)-th bit is set: \\( \\text{mask } \\&amp; \\text{ } (1 &lt;&lt; i) \\).   If the expression is equal to 1, the \\( i \\)-th object is in the subset. If the expression is equal to 0, the \\( i \\)-th object is not in the subset.</p> </li> <li> <p>Toggle the existence of the \\( i \\)-th object:   XOR the \\( i \\)-th bit with 1, turning 1 into 0 and 0 into 1: \\( \\text{mask} = \\text{mask}\\)  ^ \\( (1 &lt;&lt; i) \\)</p> </li> <li> <p>Count the number of objects in the subset:   Use a built-in function to count the number of 1\u2019s in an integer variable: <code>__builtin_popcount(mask)</code> for integers or <code>__builtin_popcountll(mask)</code> for long longs.</p> </li> </ul>","tags":["Dynamic Programming","Bitmask DP"]},{"location":"dynamic-programming/bitmask-dp/#iterating-over-subsets","title":"Iterating Over Subsets","text":"<ul> <li> <p>Iterate through all subsets of a set with size \\( n \\): \\( \\text{for (int x = 0; x &lt; (1 &lt;&lt; n); ++x)} \\)</p> </li> <li> <p>Iterate through all subsets of a subset with the mask \\( y \\): \\( \\text{for (int x = y; x &gt; 0; x = (y \\&amp; (x \u2212 1)))} \\)</p> </li> </ul>","tags":["Dynamic Programming","Bitmask DP"]},{"location":"dynamic-programming/bitmask-dp/#task-assignment-problem","title":"Task Assignment Problem","text":"<p>There are \\( N \\) people and \\( N \\) tasks, and each task is going to be allocated to a single person. We are also given a matrix <code>cost</code> of size \\( N \\times N \\), where <code>cost[i][j]</code> denotes how much a person is going to charge for a task. We need to assign each task to a person such that the total cost is minimized. Note that each task is allocated to only one person, and each person is allocated only one task.</p>","tags":["Dynamic Programming","Bitmask DP"]},{"location":"dynamic-programming/bitmask-dp/#naive-approach","title":"Naive Approach:","text":"<p>Try \\( N! \\) possible assignments. Time complexity: \\( O(N!) \\).</p>","tags":["Dynamic Programming","Bitmask DP"]},{"location":"dynamic-programming/bitmask-dp/#dp-approach","title":"DP Approach:","text":"<p>For every possible subset, find the new subsets that can be generated from it and update the DP array. Here, we use bitmasking to represent subsets and iterate over them. Time complexity: \\( O(2^N \\times N) \\).</p> <p>Note: The Hungarian Algorithm solves this problem in \\( O(N^3) \\) time complexity.</p> <p>Solution code for DP approach:</p> <pre><code>for (int mask = 0; mask &lt; (1 &lt;&lt; n); ++mask)\n{\n    for (int j = 0; j &lt; n; ++j)\n    {\n        if((mask &amp; (1 &lt;&lt; j)) == 0) // jth task not assigned\n        {\n            dp[mask | (1 &lt;&lt; j)] = min(dp[mask | (1 &lt;&lt; j)], dp[mask] + cost[__builtin_popcount(mask)][j])\n        }\n    }\n}\n// after this operation our answer stored in dp[(1 &lt;&lt; N) - 1]\n</code></pre>","tags":["Dynamic Programming","Bitmask DP"]},{"location":"dynamic-programming/bitmask-dp/#references","title":"References","text":"<ul> <li>Bitmask Tutorial on HackerEarth</li> </ul>","tags":["Dynamic Programming","Bitmask DP"]},{"location":"dynamic-programming/common-dp-problems/","title":"Common Dynamic Programming Problems","text":"","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#coin-problem","title":"Coin Problem","text":"<p>As discussed earlier, the Greedy approach doesn\u2019t work all the time for the coin problem. For example, if the coins are {4, 3, 1} and the target sum is \\(6\\), the greedy algorithm produces the solution \\(4+1+1\\), while the optimal solution is \\(3+3\\). This is where Dynamic Programming (DP) helps.</p>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#solution","title":"Solution","text":"","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#approach","title":"Approach:","text":"<ol> <li>If \\( V == 0 \\), then 0 coins are required.</li> <li>If \\( V &gt; 0 \\), compute \\( \\text{minCoins}(coins[0..m-1], V) = \\min \\{ 1 + \\text{minCoins}(V - \\text{coin}[i]) \\} \\) for all \\( i \\) where \\( \\text{coin}[i] \\leq V \\).</li> </ol> <pre><code>def minCoins(coins, target):\n    # base case\n    if (V == 0):\n        return 0\n\n    n = len(coins)\n    # Initialize result\n    res = sys.maxsize\n\n    # Try every coin that has smaller value than V\n    for i in range(0, n):\n        if (coins[i] &lt;= target):\n            sub_res = minCoins(coins, target-coins[i])\n\n    # Check for INT_MAX to avoid overflow and see if\n    # result can minimized\n    if (sub_res != sys.maxsize and sub_res + 1 &lt; res):\n        res = sub_res + 1\n\n    return res\n</code></pre>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#knapsack-problem","title":"Knapsack Problem","text":"<p>We are given the weights and values of \\( n \\) items, and we are to put these items in a knapsack of capacity \\( W \\) to get the maximum total value. In other words, we are given two integer arrays <code>val[0..n-1]</code> and <code>wt[0..n-1]</code>, which represent the values and weights associated with \\( n \\) items. We are also given an integer \\( W \\), which represents the knapsack's capacity. Our goal is to find out the maximum value subset of <code>val[]</code> such that the sum of the weights of this subset is smaller than or equal to \\( W \\). We cannot break an item; we must either pick the complete item or leave it.</p>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#approach_1","title":"Approach:","text":"<p>There are two cases for every item: 1. The item is included in the optimal subset. 2. The item is not included in the optimal subset.</p> <p>The maximum value that can be obtained from \\( n \\) items is the maximum of the following two values: 1. Maximum value obtained by \\( n-1 \\) items and \\( W \\) weight (excluding the \\( n \\)-th item). 2. Value of the \\( n \\)-th item plus the maximum value obtained by \\( n-1 \\) items and \\( W - \\text{weight of the } n \\)-th item (including the \\( n \\)-th item).</p> <p>If the weight of the \\( n \\)-th item is greater than \\( W \\), then the \\( n \\)-th item cannot be included, and case 1 is the only possibility.</p> <p>For example:</p> <ul> <li>Knapsack max weight: \\( W = 8 \\) units</li> <li>Weight of items: \\( \\text{wt} = \\{3, 1, 4, 5\\} \\)</li> <li>Values of items: \\( \\text{val} = \\{10, 40, 30, 50\\} \\)</li> <li>Total items: \\( n = 4 \\)</li> </ul> <p>The sum \\( 8 \\) is possible with two combinations: {3, 5} with a total value of 60, and {1, 3, 4} with a total value of 80. However, a better solution is {1, 5}, which has a total weight of 6 and a total value of 90.</p>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#recursive-solution","title":"Recursive Solution","text":"<pre><code>def knapSack(W , wt , val , n):\n\n    # Base Case\n    if (n == 0 or W == 0):\n        return 0\n\n    # If weight of the nth item is more than Knapsack of capacity\n    # W, then this item cannot be included in the optimal solution\n    if (wt[n-1] &gt; W):\n        return knapSack(W, wt, val, n - 1)\n\n    # return the maximum of two cases:\n    # (1) nth item included\n    # (2) not included\n    else:\n        return max(val[n-1] + knapSack(W - wt[n - 1], wt, val, n - 1), knapSack(W, wt, val, n - 1))\n</code></pre>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#dynamic-programming-solution","title":"Dynamic Programming Solution","text":"<p>It should be noted that the above function computes the same subproblems again and again. Time complexity of this naive recursive solution is exponential \\(2^n\\). Since suproblems are evaluated again, this problem has Overlapping Subproblems property. Like other typical Dynamic Programming(DP) problems, recomputations of same subproblems can be avoided by constructing a temporary array \\(K[][]\\) in bottom up manner. Following is Dynamic Programming based implementation.</p> <pre><code>def knapSack(W, wt, val, n):\n    K = [[0 for x in range(W + 1)] for x in range(n + 1)]\n\n    # Build table K[][] in bottom up manner\n    for (i in range(n + 1)):\n        for (w in range(W + 1)):\n            if (i == 0 or w == 0):\n                K[i][w] = 0\n            elif (wt[i - 1] &lt;= w):\n                K[i][w] = max(val[i - 1] + K[i - 1][w - wt[i - 1]], K[i - 1][w])\n            else:\n                K[i][w] = K[i - 1][w]\n\n    return K[n][W]\n</code></pre>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#longest-common-substring-lcs-problem","title":"Longest Common Substring (LCS) Problem","text":"<p>We are given two strings \\( X \\) and \\( Y \\), and our task is to find the length of the longest common substring.</p>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#sample-case","title":"Sample Case:","text":"<ul> <li>Input: \\( X = \"inzvahackerspace\" \\), \\( Y = \"spoilerspoiler\" \\)</li> <li>Output: 4</li> </ul> <p>The longest common substring is \"ersp\" and is of length 4.</p>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#approach_2","title":"Approach:","text":"<p>Let \\( m \\) and \\( n \\) be the lengths of the first and second strings, respectively. A simple solution is to consider all substrings of the first string one by one and check if they are substrings of the second string. Keep track of the maximum-length substring. There will be \\( O(m^2) \\) substrings, and checking if one is a substring of the other will take \\( O(n) \\) time. Thus, the overall time complexity is \\( O(n \\cdot m^2) \\).</p> <p>Dynamic programming can reduce this to \\( O(m \\cdot n) \\). The idea is to find the length of the longest common suffix for all substrings of both strings and store these lengths in a table. The longest common suffix has the following property:</p> <p>[ LCSuff(X, Y, m, n) = LCSuff(X, Y, m-1, n-1) + 1 \\text{ if } X[m-1] = Y[n-1] ] Otherwise, \\( LCSuff(X, Y, m, n) = 0 \\).</p> <p>The maximum length of the Longest Common Suffix is the Longest Common Substring.</p>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#dp-iterative","title":"DP - Iterative","text":"<pre><code>def LCSubStr(X, Y):\n    m = len(X)\n    n = len(Y)\n\n    # Create a table to store lengths of\n    # longest common suffixes of substrings.\n    # Note that LCSuff[i][j] contains the\n    # length of longest common suffix of\n    # X[0...i\u22121] and Y[0...j\u22121]. The first\n    # row and first column entries have no\n    # logical meaning, they are used only\n    # for simplicity of the program.\n\n    # LCSuff is the table with zero\n    # value initially in each cell\n    LCSuff = [[0 for k in range(n+1)] for l in range(m + 1)]\n\n    # To store the length of\n    # longest common substring\n    result = 0\n\n    # Following steps to build\n    # LCSuff[m+1][n+1] in bottom up fashion\n    for (i in range(m + 1)):\n        for (j in range(n + 1)):\n    if (i == 0 or j == 0):\n                LCSuff[i][j] = 0\n            elif (X[i - 1] == Y[j - 1]):\n                LCSuff[i][j] = LCSuff[i - 1][j - 1] + 1\n                result = max(result, LCSuff[i][j])\n            else:\n                LCSuff[i][j] = 0\n    return result\n</code></pre>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#dp-recursive","title":"DP - Recursive","text":"<pre><code>def lcs(int i, int j, int count):\n    if (i == 0 or j == 0):\n        return count\n\n    if (X[i - 1] == Y[j - 1]):\n        count = lcs(i - 1, j - 1, count + 1)\n\n    count = max(count, max(lcs(i, j - 1, 0), lcs(i - 1, j, 0)))\n    return count\n</code></pre>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#longest-increasing-subsequence-lis-problem","title":"Longest Increasing Subsequence (LIS) Problem","text":"<p>The Longest Increasing Subsequence (LIS) problem is to find the length of the longest subsequence of a given sequence such that all elements of the subsequence are sorted in increasing order.</p> <p>For example, given the array \\([0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13, 3, 11, 7, 15]\\), the longest increasing subsequence has a length of 6, and it is {0, 2, 6, 9, 11, 15}.</p>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/common-dp-problems/#solution_1","title":"Solution","text":"<p>A naive, brute-force approach is to generate every possible subsequence, check for monotonicity, and keep track of the longest one. However, this is prohibitively expensive, as generating each subsequence takes \\( O(2^N) \\) time.</p> <p>Instead, we can use recursion to solve this problem and then optimize it with dynamic programming. We assume that we have a function that gives us the length of the longest increasing subsequence up to a certain index.</p> <p>The base cases are: - The empty list, which returns 0. - A list with one element, which returns 1.</p> <p>For every index \\( i \\), calculate the longest increasing subsequence up to that point. The result can only be extended with the last element if the last element is greater than \\( \\text{arr}[i] \\), as otherwise, the sequence wouldn\u2019t be increasing.</p> <pre><code>def longest_increasing_subsequence(arr):\n    if (not arr):\n        return 0\n    if (len(arr) == 1):\n        return 1\n\n    max_ending_here = 0\n    for (i in range(len(arr))):\n        ending_at_i = longest_increasing_subsequence(arr[:i])\n        if (arr[-1] &gt; arr[i - 1] and ending_at_i + 1 &gt; max_ending_here):\n            max_ending_here = ending_at_i + 1\n    return max_ending_here\n</code></pre> <p>This is really slow due to repeated subcomputations (exponential in time). So, let\u2019s use dynamic programming to store values to recompute them for later.</p> <p>We\u2019ll keep an array A of length N, and A[i] will contain the length of the longest increasing subsequence ending at i. We can then use the same recurrence but look it up in the array instead:</p> <pre><code>def longest_increasing_subsequence(arr):\n    if (not arr):\n        return 0\n    cache = [1] * len(arr)\n    for (i in range(1, len(arr))):\n        for (j in range(i)):\n            if (arr[i] &gt; arr[j]):\n                cache[i] = max(cache[i], cache[j] + 1)\n    return max(cache)\n</code></pre> <p>This now runs in \\( O(N^2) \\) time and \\( O(N) \\) space.</p>","tags":["Dynamic Programming","Common Dynamic Programming Problems"]},{"location":"dynamic-programming/digit-dp/","title":"Digit DP","text":"<p>Problems that require the calculation of how many numbers there are between two values (say, \\( A \\) and \\( B \\)) that satisfy a particular property can be solved using digit dynamic programming (Digit DP).</p>","tags":["Dynamic Programming","Digit DP"]},{"location":"dynamic-programming/digit-dp/#how-to-work-on-digits","title":"How to Work on Digits","text":"<p>While constructing our numbers recursively (from the left), we need a method to check if our number is still smaller than the given boundary number. To achieve this, we keep a variable called \"strict\" while branching, which limits our ability to select digits that are larger than the corresponding digit of the boundary number.</p> <p>Let\u2019s suppose the boundary number is \\( A \\). We start filling the number from the left (most significant digit) and set <code>strict</code> to <code>true</code>, meaning we cannot select any digit larger than the corresponding digit of \\( A \\). As we branch:</p> <ul> <li>Values less than the corresponding digit of \\( A \\) will now be non-strict (<code>strict = false</code>) because we guarantee that the number will be smaller than \\( A \\) after this point.</li> <li>For values equal to the corresponding digit of \\( A \\), the strictness continues to be <code>true</code>.</li> </ul>","tags":["Dynamic Programming","Digit DP"]},{"location":"dynamic-programming/digit-dp/#counting-problem-example","title":"Counting Problem Example","text":"<p>Problem: How many numbers \\( x \\) are there in the range \\( A \\) to \\( B \\), where the digit \\( d \\) occurs exactly \\( k \\) times in \\( x \\)?</p> <p>Constraints: \\( A, B &lt; 10^{60}, k &lt; 60 \\).</p>","tags":["Dynamic Programming","Digit DP"]},{"location":"dynamic-programming/digit-dp/#brute-force-approach","title":"Brute Force Approach:","text":"<p>The brute-force solution would involve iterating over all the numbers in the range \\([A, B]\\) and counting the occurrences of the digit \\( d \\) one by one for each number. This has a time complexity of \\( O(N \\log_{10}(N)) \\), which is too large for such constraints, and we need a more efficient approach.</p>","tags":["Dynamic Programming","Digit DP"]},{"location":"dynamic-programming/digit-dp/#recursive-approach","title":"Recursive Approach:","text":"<p>We can recursively fill the digits of our number starting from the leftmost digit. At each step, we branch into 3 possibilities:</p> <ol> <li>Pick a number that is not \\( d \\) and smaller than the corresponding digit of the boundary number.</li> <li>Pick the digit \\( d \\).</li> <li>Pick a number that is equal to the corresponding digit of the boundary number.</li> </ol> <p>The depth of recursion is equal to the number of digits in the decimal representation of the boundary number, leading to a time complexity of \\( O(3^{\\log_{10} N}) \\). Although this is better than brute force, it is still not efficient enough.</p>","tags":["Dynamic Programming","Digit DP"]},{"location":"dynamic-programming/digit-dp/#recursive-approach-with-memoization","title":"Recursive Approach with Memoization:","text":"<p>We can further optimize this approach using memoization. We represent a DP state by \\((\\text{current index}, \\text{current strictness}, \\text{number of } d's)\\), which denotes the number of possible configurations of the remaining digits after picking the current digit. We use a <code>dp[\\log_{10} N][2][\\log_{10} N]</code> array, where each value is computed at most once. Therefore, the worst-case time complexity is \\( O((\\log_{10} N)^2) \\).</p> <p>Solution Code:</p> <pre><code>#include &lt;bits/stdc++.h&gt;\nusing namespace std;\n#define ll long long\nll A, B, d, k, dg; // dg: digit count\nvector &lt;ll&gt; v; // digit vector\nll dp[25][2][25];\nvoid setup(ll a)\n{\n    memset(dp,0,sizeof dp);\n    v.clear();\n    ll tmp = a;\n    while(tmp)\n    {\n        v.push_back(tmp%10);\n        tmp/=10;\n    }\n    dg = (ll)v.size();\n    reverse(v.begin(), v.end());\n}\n\nll rec(int idx, bool strict, int count)\n{\n    if(dp[idx][strict][count]) return dp[idx][strict][count];\n    if(idx == dg or count &gt; k) return (count == k);\n    ll sum = 0;\n    if(strict)\n    {\n        // all &lt;v[idx] if d is included -1\n        sum += rec(idx + 1, 0, count) * (v[idx] - (d &lt; v[idx]));\n        // v[idx], if d==v[idx] send count+1\n        sum += rec(idx + 1, 1, count + (v[idx] == d) );\n        if(d &lt; v[idx])\n        sum += rec(idx + 1, 0, count + 1); // d\n    }\n    else\n    {\n        sum += rec(idx + 1, 0, count) * (9); // other than d (10 - 1)\n        sum += rec(idx + 1, 0, count + 1); // d\n    }\n    return dp[idx][strict][count] = sum;\n}\n\nint main()\n{\n    cin &gt;&gt; A &gt;&gt; B &gt;&gt; d &gt;&gt; k;\n    setup(B);\n    ll countB = rec(0, 1, 0); //countB is answer of [0..B]\n    setup(A - 1);\n    ll countA = rec(0, 1, 0); //countA is answer of [0..A-1]\n    cout &lt;&lt; fixed &lt;&lt; countB - countA &lt;&lt; endl; //difference gives us [A..B]\n}\n</code></pre>","tags":["Dynamic Programming","Digit DP"]},{"location":"dynamic-programming/digit-dp/#references","title":"References","text":"<ul> <li> <p>Digit DP on Codeforces</p> </li> <li> <p>Digit DP on HackerRank</p> </li> </ul>","tags":["Dynamic Programming","Digit DP"]},{"location":"dynamic-programming/dp-on-dags/","title":"DP on Directed Acyclic Graphs (DAGs)","text":"<p>As we know, the nodes of a directed acyclic graph (DAG) can be sorted topologically, and DP can be implemented efficiently using this topological order. </p> <p>First, we can find the topological order with a topological sort in \\( O(N) \\) time complexity. Then, we can find the \\( dp(V) \\) values in topological order, where \\( V \\) is a node in the DAG and \\( dp(V) \\) is the answer for node \\( V \\). The answer and implementation will differ depending on the specific problem.</p>","tags":["Dynamic Programming","DP on Directed Acyclic Graphs (DAGs)"]},{"location":"dynamic-programming/dp-on-dags/#converting-a-dp-problem-into-a-directed-acyclic-graph","title":"Converting a DP Problem into a Directed Acyclic Graph","text":"<p>Many DP problems can be converted into a DAG. Let\u2019s explore why this is the case.</p> <p>While solving a DP problem, when we process a state, we evaluate it by considering all possible previous states. To do this, all of the previous states must be processed before the current state. From this perspective, some states depend on other states, forming a DAG structure.</p> <p>However, note that some DP problems cannot be converted into a DAG and may require hyper-graphs. (For more details, refer to Advanced Dynamic Programming in Semiring and Hypergraph Frameworks).</p>","tags":["Dynamic Programming","DP on Directed Acyclic Graphs (DAGs)"]},{"location":"dynamic-programming/dp-on-dags/#example-problem","title":"Example Problem:","text":"<p>There are \\( N \\) stones numbered \\( 1, 2, ..., N \\). For each \\( i \\) ( \\( 1 \\leq i \\leq N \\) ), the height of the \\( i \\)-th stone is \\( h_i \\). There is a frog initially on stone 1. The frog can jump to stone \\( i+1 \\) or stone \\( i+2 \\). The cost of a jump from stone \\( i \\) to stone \\( j \\) is \\( | h_i \u2212 h_j | \\). Find the minimum possible cost to reach stone \\( N \\).</p>","tags":["Dynamic Programming","DP on Directed Acyclic Graphs (DAGs)"]},{"location":"dynamic-programming/dp-on-dags/#solution","title":"Solution:","text":"<p>We define \\( dp[i] \\) as the minimum cost to reach the \\( i \\)-th stone. The answer will be \\( dp[N] \\). The recurrence relation is defined as:</p> <p>\\[ dp[i] = \\min(dp[i\u22121] + |h_i \u2212 h_{i\u22121}|, dp[i\u22122] + |h_i \u2212 h_{i\u22122}|) \\]</p> <p>For \\( N = 5 \\), we can see that to calculate \\( dp[5] \\), we need to calculate \\( dp[4] \\) and \\( dp[3] \\). Similarly:</p> <ul> <li>\\( dp[4] \\) depends on \\( dp[3] \\) and \\( dp[2] \\),</li> <li>\\( dp[3] \\) depends on \\( dp[2] \\) and \\( dp[1] \\),</li> <li>\\( dp[2] \\) depends on \\( dp[1] \\).</li> </ul> <p>These dependencies form a DAG, where the nodes represent the stones, and the edges represent the transitions between them based on the jumps.</p> <pre><code>graph LR\n    A(dp_1) --&gt; B(dp_2);\n    A --&gt; C(dp_3);\n    B --&gt; D(dp_4);\n    B --&gt; E(dp_5);\n    C --&gt; D;\n    D --&gt; E;</code></pre>","tags":["Dynamic Programming","DP on Directed Acyclic Graphs (DAGs)"]},{"location":"dynamic-programming/dp-on-dags/#dp-on-directed-acyclic-graph-problem","title":"DP on Directed Acyclic Graph Problem","text":"<p>Given a DAG with \\( N \\) nodes and \\( M \\) weighted edges, find the longest path in the DAG.</p>","tags":["Dynamic Programming","DP on Directed Acyclic Graphs (DAGs)"]},{"location":"dynamic-programming/dp-on-dags/#complexity","title":"Complexity:","text":"<p>The time complexity for this problem is \\( O(N + M) \\), where \\( N \\) is the number of nodes and \\( M \\) is the number of edges.</p> <p>Solution Code:</p> <pre><code>// topological sort is not written here so we will take tp as it is already sorted\n// note that tp is reverse topologically sorted\n// vector &lt;int&gt; tp\n// n , m and vector &lt;pair&lt;int,int&gt;&gt; adj is given.Pair denotes {node,weight}.\n// flag[] denotes whether a node is processed or not.Initially all zero.\n// dp[] is DP array.Initially all zero.\n\nfor (int i = 0; i &lt; (int)tp.size(); ++i)//processing in order\n{\n    int curNode = tp[i];\n\n    for (auto v : adj[curNode]) //iterate through all neighbours\n        if(flag[v.first]) //if a neighbour is already processed\n            dp[curNode] = max(dp[curNode] , dp[v.first] + v.second);\n\n    flag[curNode] = 1;\n}\n//answer is max(dp[1..n])\n</code></pre>","tags":["Dynamic Programming","DP on Directed Acyclic Graphs (DAGs)"]},{"location":"dynamic-programming/dp-on-dags/#references","title":"References","text":"<ul> <li> <p>NOI IOI training week-5</p> </li> <li> <p>DP on Graphs MIT</p> </li> </ul>","tags":["Dynamic Programming","DP on Directed Acyclic Graphs (DAGs)"]},{"location":"dynamic-programming/dp-on-rooted-trees/","title":"DP on Rooted Trees","text":"<p>In dynamic programming (DP) on rooted trees, we define functions for the nodes of the tree, which are calculated recursively based on the children of each node. One common DP state is usually associated with a node \\(i\\), representing the sub-tree rooted at node \\(i\\).</p>","tags":["Dynamic Programming","DP on Rooted Trees"]},{"location":"dynamic-programming/dp-on-rooted-trees/#problem","title":"Problem","text":"<p>Given a tree \\( T \\) of \\( N \\) (1-indexed) nodes, where each node \\( i \\) has \\( C_i \\) coins attached to it, the task is to choose a subset of nodes such that no two adjacent nodes (nodes directly connected by an edge) are chosen, and the sum of coins attached to the chosen subset is maximized.</p>","tags":["Dynamic Programming","DP on Rooted Trees"]},{"location":"dynamic-programming/dp-on-rooted-trees/#approach","title":"Approach:","text":"<p>We define two functions, \\( dp1(V) \\) and \\( dp2(V) \\), as follows:</p> <ul> <li>\\( dp1(V) \\): The optimal solution for the sub-tree of node \\( V \\) when node \\( V \\) is included in the answer.</li> <li>\\( dp2(V) \\): The optimal solution for the sub-tree of node \\( V \\) when node \\( V \\) is not included in the answer.</li> </ul> <p>The final answer is the maximum of these two cases:</p> <p>\\[ \\text{max}(dp1(V), dp2(V)) \\]</p>","tags":["Dynamic Programming","DP on Rooted Trees"]},{"location":"dynamic-programming/dp-on-rooted-trees/#recursive-definitions","title":"Recursive Definitions:","text":"<ul> <li> <p>\\( dp1(V) = C_V + \\sum_{i=1}^{n} dp2(v_i) \\), where \\( n \\) is the number of children of node \\( V \\), and \\( v_i \\) is the \\( i \\)-th child of node \\( V \\).   This represents the scenario where node \\( V \\) is included in the chosen subset, so none of its children can be selected.</p> </li> <li> <p>\\( dp2(V) = \\sum_{i=1}^{n} \\text{max}(dp1(v_i), dp2(v_i)) \\).   This represents the scenario where node \\( V \\) is not included, so the optimal solution for each child \\( v_i \\) can either include or exclude that child.</p> </li> </ul>","tags":["Dynamic Programming","DP on Rooted Trees"]},{"location":"dynamic-programming/dp-on-rooted-trees/#complexity","title":"Complexity:","text":"<p>The time complexity for this approach is \\( O(N) \\), where \\( N \\) is the number of nodes in the tree. This is because the solution involves a depth-first search (DFS) traversal of the tree, and each node is visited only once.</p> <pre><code>//pV is parent of V\nvoid dfs(int V, int pV)\n{\n    //base case:\n    //when dfs reaches a leaf it finds dp1 and dp2 and does not branch again.\n\n    //for storing sums of dp1 and max(dp1, dp2) for all children of V\n    int sum1 = 0, sum2 = 0;\n\n    //traverse over all children\n    for (auto v : adj[V])\n    {\n        if (v == pV)\n            continue;\n        dfs(v, V);\n        sum1 += dp2[v];\n        sum2 += max(dp1[v], dp2[v]);\n    }\n\n    dp1[V] = C[V] + sum1;\n    dp2[V] = sum2;\n}\n//Nodes are 1-indexed, therefore our answer stored in dp1[1] and dp2[1]\n//for the answer we take max(dp1[1],dp2[1]) after calling dfs(1,0).\n</code></pre>","tags":["Dynamic Programming","DP on Rooted Trees"]},{"location":"dynamic-programming/dp-on-rooted-trees/#references","title":"References","text":"<ul> <li>DP on Tree on CodeForces</li> </ul>","tags":["Dynamic Programming","DP on Rooted Trees"]},{"location":"dynamic-programming/dynamic-programming/","title":"Dynamic Programming","text":"<p>Dynamic programming (DP) is a technique used to avoid computing the same sub-solution multiple times in a recursive algorithm. A sub-solution of the problem is constructed from the previously found ones. DP solutions have a polynomial complexity, which ensures a much faster running time than other techniques like backtracking or brute-force.</p>","tags":["Dynamic Programming"]},{"location":"dynamic-programming/dynamic-programming/#memoization-top-down","title":"Memoization - Top Down","text":"<p>Memoization ensures that a method doesn\u2019t run for the same inputs more than once by keeping a record of the results for the given inputs (usually in a hash map). </p> <p>To avoid duplicate work caused by recursion, we can use a cache that maps inputs to outputs. The approach involves:</p> <ul> <li>Checking the cache to see if we can avoid computing the answer for any given input.</li> <li>Saving the results of any calculations to the cache.</li> </ul> <p>Memoization is a common strategy for dynamic programming problems where the solution is composed of solutions to the same problem with smaller inputs, such as the Fibonacci problem.</p> <p>Another strategy for dynamic programming is the bottom-up approach, which is often cleaner and more efficient.</p>","tags":["Dynamic Programming"]},{"location":"dynamic-programming/dynamic-programming/#bottom-up","title":"Bottom-Up","text":"<p>The bottom-up approach avoids recursion, saving the memory cost associated with building up the call stack. It \"starts from the beginning\" and works towards the final solution, whereas a recursive algorithm often \"starts from the end and works backwards.\"</p>","tags":["Dynamic Programming"]},{"location":"dynamic-programming/dynamic-programming/#an-example-fibonacci","title":"An Example - Fibonacci","text":"<p>Let\u2019s start with a well-known example: finding the \\(n\\)-th Fibonacci number. The Fibonacci sequence is defined as:</p> <p>\\[ F_n = F_{n\u22121} + F_{n\u22122}, \\quad \\text{with } F_0 = 0 \\text{ and } F_1 = 1 \\]</p> <p>There are several approaches to solving this problem:</p>","tags":["Dynamic Programming"]},{"location":"dynamic-programming/dynamic-programming/#recursion","title":"Recursion","text":"<p>In a recursive approach, the function calls itself to compute the previous two Fibonacci numbers until reaching the base cases.</p> <pre><code>def fibonacci(n):\n    if (n == 0):\n        return 0\n    if (n == 1):\n        return 1\n\n    return fibonacci(n - 1) + fibonacci(n - 2)\n</code></pre>","tags":["Dynamic Programming"]},{"location":"dynamic-programming/dynamic-programming/#dynamic-programming","title":"Dynamic Programming","text":"<ul> <li>Top-Down - Memoization:   Recursion leads to unnecessary repeated calculations. Memoization solves this by caching the results of previously computed Fibonacci numbers, so they don't have to be recalculated.</li> </ul> <pre><code>cache = {}\n\ndef fibonacci(n):\n    if (n == 0):\n        return 0\n    if (n == 1):\n        return 1\n    if (n in cache):\n        return cache[n]\n\n    cache[n] = fibonacci(n - 1) + fibonacci(n - 2)\n\n    return cache[n]\n</code></pre> Visualization of Recursive Memoization <ul> <li>Bottom-Up:   The bottom-up approach eliminates recursion by computing the Fibonacci numbers in order, starting from the base cases and building up to the desired value.</li> </ul> <pre><code>cache = {}\n\ndef fibonacci(n):\n    cache[0] = 0\n    cache[1] = 1\n\n    for (i in range(2, n + 1)):\n        cache[i] = cache[i - 1] + cache[i - 2]\n\n    return cache[n]\n</code></pre> <p>Additionally, this approach can be optimized further by using constant space and only storing the necessary partial results along the way.</p> <pre><code>def fibonacci(n):\n    fib_minus_2 = 0\n    fib_minus_1 = 1\n\n    for (i in range(2, n + 1)):\n        fib = fib_minus_1 + fib_minus_2\n        fib_minus_1, fib_minus_2 = fib, fib_minus_1\n\n    return fib\n</code></pre>","tags":["Dynamic Programming"]},{"location":"dynamic-programming/dynamic-programming/#how-to-apply-dynamic-programming","title":"How to Apply Dynamic Programming?","text":"<p>To apply dynamic programming, follow these steps:</p> <ul> <li>Find the recursion in the problem: Identify how the problem can be broken down into smaller subproblems.</li> <li>Top-down approach: Store the result of each subproblem in a table to avoid recomputation.</li> <li>Bottom-up approach: Find the correct order to evaluate the results so that partial results are available when needed.</li> </ul> <p>Dynamic programming generally works for problems that have an inherent left-to-right order, such as strings, trees, or integer sequences. If the naive recursive algorithm does not compute the same subproblem multiple times, dynamic programming won't be useful.</p>","tags":["Dynamic Programming"]},{"location":"dynamic-programming/greedy-algorithms/","title":"Greedy Algorithms","text":"<p>A greedy algorithm is an algorithm that follows the problem solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. A greedy algorithm never takes back its choices, but directly constructs the final solution. For this reason, greedy algorithms are usually very efficient.</p> <p>The difficulty in designing greedy algorithms is to find a greedy strategy that always produces an optimal solution to the problem. The locally optimal choices in a greedy algorithm should also be globally optimal. It is often difficult to argue that a greedy algorithm works.</p>","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/greedy-algorithms/#coin-problem","title":"Coin Problem","text":"<p>We are given a value \\( V \\). If we want to make change for \\( V \\) cents, and we have an infinite supply of each of the coins = { \\( C_1, C_2, \\dots, C_m \\) } valued coins (sorted in descending order), what is the minimum number of coins to make the change?</p>","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/greedy-algorithms/#solution","title":"Solution","text":"","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/greedy-algorithms/#approach","title":"Approach:","text":"<ol> <li>Initialize the result as empty.</li> <li>Find the largest denomination that is smaller than the amount.</li> <li>Add the found denomination to the result. Subtract the value of the found denomination from the amount.</li> <li>If the amount becomes 0, then print the result. Otherwise, repeat steps 2 and 3 for the new value of the amount.</li> </ol> <p><pre><code>def min_coins(coins, amount):\n    n = len(coins)\n    for i in range(n):\n        while amount &gt;= coins[i]:\n            # while loop is needed since one coin can be used multiple times\n            amount -= coins[i]\n            print(coins[i])\n</code></pre> For example, if the coins are the euro coins (in cents) \\({200, 100, 50, 20, 10, 5, 2, 1}\\) and the amount is 548, the optimal solution is to select coins \\(200+200+100+20+20+5+2+1\\), whose sum is \\(548\\).</p> Visualization of the Coin Change Problem <p>In the general case, the coin set can contain any kind of coins, and the greedy algorithm does not necessarily produce an optimal solution.</p> <p>We can prove that a greedy algorithm does not work by showing a counterexample where the algorithm gives a wrong answer. In this problem, we can easily find a counterexample: if the coins are \\({6, 5, 2}\\) and the target sum is \\(10\\), the greedy algorithm produces the solution \\(6+2+2\\), while the optimal solution is \\(5+5\\).</p>","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/greedy-algorithms/#scheduling","title":"Scheduling","text":"<p>Many scheduling problems can be solved using greedy algorithms. A classic problem is as follows:</p> <p>We are given an array of jobs where every job has a deadline and associated profit if the job is finished before the deadline. It is also given that every job takes a single unit of time, thus the minimum possible deadline for any job is 1. How do we maximize total profit if only one job can be scheduled at a time?</p>","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/greedy-algorithms/#solution_1","title":"Solution","text":"<p>A simple solution is to generate all subsets of the given set of jobs and check each subset for feasibility. Keep track of maximum profit among all feasible subsets. The time complexity of this solution is exponential. This is a standard Greedy Algorithm problem.</p>","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/greedy-algorithms/#approach_1","title":"Approach:","text":"<ol> <li>Sort all jobs in decreasing order of profit.</li> <li>Initialize the result sequence as the first job in sorted jobs.</li> <li>For the remaining \\(n-1\\) jobs:</li> <li>If the current job can fit in the current result sequence without missing the deadline, add the current job to the result.</li> <li>Else ignore the current job.</li> </ol> <pre><code># sample job : ['x', 4, 25] \u2212&gt; [job_id, deadline, profit]\n# jobs: array of 'job's\ndef print_job_scheduling(jobs, t):\n    n = len(jobs)\n\n    # Sort all jobs according to decreasing order of profit\n    for i in range(n):\n        for j in range(n - 1 - i):\n            if jobs[j][2] &lt; jobs[j + 1][2]:\n                jobs[j], jobs[j + 1] = jobs[j + 1], jobs[j]\n\n    # To keep track of free time slots\n    result = [False] * t\n    # To store result (Sequence of jobs)\n    job = ['-1'] * t\n\n    # Iterate through all given jobs\n    for i in range(len(jobs)):\n        # Find a free slot for this job\n        # (Note that we start from the last possible slot)\n        for j in range(min(t - 1, jobs[i][1] - 1), -1, -1):\n            # Free slot found\n            if result[j] is False:\n                result[j] = True\n                job[j] = jobs[i][0]\n                break\n    print(job)\n</code></pre>","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/greedy-algorithms/#tasks-and-deadlines","title":"Tasks and Deadlines","text":"<p>Let us now consider a problem where we are given \\(n\\) tasks with durations and deadlines, and our task is to choose an order to perform the tasks. For each task, we earn \\(d - x\\) points, where \\(d\\) is the task\u2019s deadline and \\(x\\) is the moment when we finish the task. What is the largest possible total score we can obtain?</p> <p>For example, suppose the tasks are as follows:</p> Task Duration Deadline A 4 2 B 3 5 C 2 7 D 4 5 <p>An optimal schedule for the tasks is \\( C, B, A, D \\). In this solution, \\( C \\) yields 5 points, \\( B \\) yields 0 points, \\( A \\) yields -7 points, and \\( D \\) yields -8 points, so the total score is -10.</p> <p>Interestingly, the optimal solution to the problem does not depend on the deadlines, but a correct greedy strategy is to simply perform the tasks sorted by their durations in increasing order.</p>","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/greedy-algorithms/#solution_2","title":"Solution","text":"<ol> <li>Sort all tasks according to increasing order of duration.</li> <li>Calculate the total points by iterating through all tasks, summing up the difference between the deadlines and the time at which the task is finished.</li> </ol> <pre><code>def order_tasks(tasks):\n    n = len(tasks)\n\n    # Sort all task according to increasing order of duration\n    for (i in range(n)):\n        for (j in range(n - 1 - i)):\n            if (tasks[j][1] &gt; tasks[j + 1][1]):\n                tasks[j], tasks[j + 1] = tasks[j + 1], tasks[j]\n\n    point = 0\n    current_time = 0\n    # Iterate through all given tasks and calculate point\n    for (i in range(len(tasks))):\n        current_time = current_time + tasks[i][1]\n        point = point + (tasks[i][2] - current_time)\n\n    print(point)\n</code></pre>","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/greedy-algorithms/#minimizing-sums","title":"Minimizing Sums","text":"<p>We are given \\(n\\) numbers and our task is to find a value \\(x\\) that minimizes the sum:</p> <p>\\[ |a_1 \u2212 x|^c + |a_2 \u2212 x|^c + ... + |a_n \u2212 x|^c \\]</p> <p>We focus on the cases \\(c = 1\\) and \\(c = 2\\).</p>","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/greedy-algorithms/#case-c-1","title":"Case \\(c = 1\\)","text":"<p>In this case, we should minimize the sum:</p> <p>\\[ |a_1 \u2212 x| + |a_2 \u2212 x| + ... + |a_n \u2212 x| \\]</p> <p>For example, if the numbers are \\([1, 2, 9, 2, 6]\\), the best solution is to select \\(x = 2\\), which produces the sum:</p> <p>\\[ |1 \u2212 2| + |2 \u2212 2| + |9 \u2212 2| + |2 \u2212 2| + |6 \u2212 2| = 12 \\]</p> <p>In the general case, the best choice for \\(x\\) is the median of the numbers. For instance, the list \\([1, 2, 9, 2, 6]\\) becomes \\([1, 2, 2, 6, 9]\\) after sorting, so the median is 2. The median is an optimal choice because if \\(x\\) is smaller than the median, the sum decreases by increasing \\(x\\), and if \\(x\\) is larger, the sum decreases by lowering \\(x\\). Hence, the optimal solution is \\(x = \\text{median}\\).</p>","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/greedy-algorithms/#case-c-2","title":"Case \\(c = 2\\)","text":"<p>In this case, we minimize the sum:</p> <p>\\[ (a_1 \u2212 x)^2 + (a_2 \u2212 x)^2 + ... + (a_n \u2212 x)^2 \\]</p> <p>For example, if the numbers are \\([1, 2, 9, 2, 6]\\), the best solution is to select \\(x = 4\\), which produces the sum:</p> <p>\\[ (1 \u2212 4)^2 + (2 \u2212 4)^2 + (9 \u2212 4)^2 + (2 \u2212 4)^2 + (6 \u2212 4)^2 = 46 \\]</p> <p>In the general case, the best choice for \\(x\\) is the average of the numbers. For the given example, the average is:</p> <p>\\[ \\frac{(1 + 2 + 9 + 2 + 6)}{5} = 4 \\]</p> <p>This result can be derived by expressing the sum as:</p> <p>\\[ n x^2 \u2212 2x(a_1 + a_2 + ... + a_n) + (a_1^2 + a_2^2 + ... + a_n^2) \\]</p> <p>The last part does not depend on \\(x\\), so we can ignore it. The remaining terms form a function with a parabola opening upwards, and the minimum value occurs at \\(x = \\frac{s}{n}\\), where \\(s\\) is the sum of the numbers, i.e., the average of the numbers.</p>","tags":["Dynamic Programming","Greedy Algorithms"]},{"location":"dynamic-programming/tree-child-sibling-notation/","title":"Tree Child-Sibling Notation","text":"<p>In this method, we change the structure of the tree. In a standard tree, each parent node is connected to all of its children. However, in the child-sibling notation, a node stores a pointer to only one of its children. Additionally, the node also stores a pointer to its immediate right sibling. </p> <p>In this notation, every node has at most 2 children: - Left child (first child), - Right sibling (first sibling).</p> <p>This structure is called the LCRS (Left Child-Right Sibling) notation. It effectively represents a binary tree, as every node has only two pointers (left and right).</p>  a tree notated with child-sibling notation","tags":["Dynamic Programming","Tree Child-Sibling Notation"]},{"location":"dynamic-programming/tree-child-sibling-notation/#why-you-would-use-the-lcrs-notation","title":"Why You Would Use the LCRS Notation","text":"<p>The primary reason for using LCRS notation is to save memory. In the LCRS structure, less memory is used compared to the standard tree notation.</p>","tags":["Dynamic Programming","Tree Child-Sibling Notation"]},{"location":"dynamic-programming/tree-child-sibling-notation/#when-you-might-use-the-lcrs-notation","title":"When You Might Use the LCRS Notation:","text":"<ul> <li>Memory is extremely scarce.</li> <li>Random access to a node\u2019s children is not required.</li> </ul>","tags":["Dynamic Programming","Tree Child-Sibling Notation"]},{"location":"dynamic-programming/tree-child-sibling-notation/#possible-cases-for-using-lcrs","title":"Possible Cases for Using LCRS:","text":"<ol> <li> <p>When storing a large multi-way tree in main memory:    For example, the phylogenetic tree.</p> </li> <li> <p>In specialized data structures where the tree is used in specific ways:    For example, in the heap data structure, the main operations are:</p> </li> <li> <p>Removing the root of the tree and processing each of its children,</p> </li> <li>Joining two trees together by making one tree a child of the other.</li> </ol> <p>These operations can be done efficiently using the LCRS structure, making it convenient for working with heap data structures.</p>","tags":["Dynamic Programming","Tree Child-Sibling Notation"]},{"location":"dynamic-programming/tree-child-sibling-notation/#references","title":"References","text":"<ul> <li> <p>LCRS article on Wikipedia</p> </li> <li> <p>Link to the Figure used</p> </li> <li> <p>LCRS possible uses Stackoverflow</p> </li> </ul>","tags":["Dynamic Programming","Tree Child-Sibling Notation"]},{"location":"dynamic-programming/walk-counting-with-matrix/","title":"Walk Counting using Matrix Exponentiation","text":"<p>Matrix exponentiation can be used to count the number of walks of a given length on a graph.</p> <p>Let \\( l \\) be the desired walk length, and let \\( A \\) and \\( B \\) be nodes in a graph \\( G \\). If \\( D \\) is the adjacency matrix of \\( G \\), then \\( D^l[A][B] \\) represents the number of walks from node \\( A \\) to node \\( B \\) with length \\( l \\), where \\( D^k \\) denotes the \\( k \\)-th power of the matrix \\( D \\).</p>","tags":["Dynamic Programming","Walk Counting using Matrix Exponentiation"]},{"location":"dynamic-programming/walk-counting-with-matrix/#explanation","title":"Explanation:","text":"<ul> <li>Adjacency Matrix \\( D \\):   In the adjacency matrix of a graph, each entry \\( D[i][j] \\) denotes whether there is a direct edge between node \\( i \\) and node \\( j \\). Specifically:</li> <li>\\( D[i][j] = 1 \\) if there is an edge from \\( i \\) to \\( j \\),</li> <li> <p>\\( D[i][j] = 0 \\) otherwise.</p> </li> <li> <p>Matrix Exponentiation:   To find the number of walks of length \\( l \\) between nodes \\( A \\) and \\( B \\), we need to compute \\( D^l \\), which is the \\( l \\)-th power of the adjacency matrix \\( D \\). The entry \\( D^l[A][B] \\) will then give the number of walks of length \\( l \\) from node \\( A \\) to node \\( B \\).</p> </li> </ul> <pre><code>graph LR\n    A(2) --&gt; B(1);\n    B --&gt; C(3);\n    C --&gt; A;\n    C --&gt; D(4);\n    D --&gt; C;</code></pre> D, adjacency matrix of G D^3, 3rd power of the matrix D <p>From the matrix \\( D^3 \\), we can see that there are 4 total walks of length 3.</p> <p>Let \\( S \\) be the set of walks, and let \\( w \\) be a walk where \\( w = \\{n_1, n_2, ..., n_k\\} \\) and \\( n_i \\) is the \\( i \\)-th node of the walk. Then:</p> <p>[ S = {{1, 3, 4, 3}, {3, 4, 3, 2}, {3, 4, 3, 4}, {4, 3, 4, 3}} ] and \\( |S| = 4 \\).</p> <p>Using fast exponentiation on the adjacency matrix, we can efficiently find the number of walks of length \\( k \\) in \\( O(N^3 \\log k) \\) time, where \\( N \\) is the number of nodes in the graph.</p>","tags":["Dynamic Programming","Walk Counting using Matrix Exponentiation"]},{"location":"dynamic-programming/walk-counting-with-matrix/#time-complexity-breakdown","title":"Time Complexity Breakdown:","text":"<ul> <li>Matrix Multiplication: The \\( O(N^3) \\) time complexity comes from multiplying two \\( N \\times N \\) matrices.</li> <li>Fast Exponentiation: Fast exponentiation reduces the number of multiplications to \\( \\log k \\), resulting in the overall time complexity of \\( O(N^3 \\log k) \\).</li> </ul> <p>This method allows for efficiently calculating the number of walks with any length \\( k \\) in large graphs.</p>","tags":["Dynamic Programming","Walk Counting using Matrix Exponentiation"]},{"location":"dynamic-programming/walk-counting-with-matrix/#references","title":"References","text":"<ul> <li>Walk Counting on Sciencedirect</li> </ul>","tags":["Dynamic Programming","Walk Counting using Matrix Exponentiation"]},{"location":"graph/","title":"Graph","text":"<p>Editor: Kayacan Vesek</p> <p>Reviewers: Yasin Kaya</p>"},{"location":"graph/#introduction","title":"Introduction","text":""},{"location":"graph/#definitions","title":"Definitions","text":""},{"location":"graph/#representing-graphs","title":"Representing Graphs","text":""},{"location":"graph/#tree-traversals","title":"Tree Traversals","text":""},{"location":"graph/#binary-search-tree","title":"Binary Search Tree","text":""},{"location":"graph/#heap","title":"Heap","text":""},{"location":"graph/#depth-first-search","title":"Depth First Search","text":""},{"location":"graph/#breadth-first-search","title":"Breadth First Search","text":""},{"location":"graph/#cycle-finding","title":"Cycle Finding","text":""},{"location":"graph/#bipartite-checking","title":"Bipartite Checking","text":""},{"location":"graph/#union-find","title":"Union Find","text":""},{"location":"graph/#shortest-path","title":"Shortest Path","text":""},{"location":"graph/#minimum-spanning-tree","title":"Minimum Spanning Tree","text":""},{"location":"graph/#topological-sort","title":"Topological Sort","text":""},{"location":"graph/#bridges-and-articulation-points","title":"Bridges and Articulation Points","text":""},{"location":"graph/#strong-connectivity-and-biconnectivity","title":"Strong Connectivity and Biconnectivity","text":""},{"location":"graph/#strongly-connected-components","title":"Strongly Connected Components","text":""},{"location":"graph/#max-flow","title":"Max Flow","text":""},{"location":"graph/#references","title":"References","text":"<ol> <li>https://www.hackerearth.com/practice/algorithms/graphs/breadth-first-search/tutorial/</li> <li>https://www.geeksforgeeks.org/depth-first-search-or-dfs-for-a-graph/</li> <li>https://cp-algorithms.com/graph/depth-first-search.html</li> <li>https://www.hackerearth.com/practice/algorithms/graphs/depth-first-search/tutorial/</li> <li>Shortest Path. Wikipedia, the free online encyclopedia. Retrieved January 5, 2019</li> <li>Topological sort. Geeksforgeeks website. Retrieved January 5, 2019</li> <li>Topological Sort. Wikipedia, the free online encyclopedia. Retrieved January 5, 2019</li> <li>https://en.wikipedia.org/wiki/Graph_theory</li> </ol>"},{"location":"graph/binary-search-tree/","title":"Binary Search Tree","text":"<p>A Binary tree is a tree data structure in which each node has at most two children, which are referred to as the left child and the right child.</p> <p>For a binary tree to be a binary search tree, the values of all the nodes in the left sub-tree of the root node should be smaller than the root node's value. Also the values of all the nodes in the right sub-tree of the root node should be larger than the root node's value.</p> a simple binary search tree","tags":["Tree","Binary Search","BST"]},{"location":"graph/binary-search-tree/#insertion-algorithm","title":"Insertion Algorithm","text":"<ol> <li>Compare values of the root node and the element to be inserted.</li> <li>If the value of the root node is larger, and if a left child exists, then repeat step 1 with root = current root's left child. Else, insert element as left child of current root.</li> <li>If the value of the root node is lesser, and if a right child exists, then repeat step 1 with root = current root's right child. Else, insert element as right child of current root.</li> </ol>","tags":["Tree","Binary Search","BST"]},{"location":"graph/binary-search-tree/#deletion-algorithm","title":"Deletion Algorithm","text":"<ul> <li>Deleting a node with no children: simply remove the node from the tree.</li> <li>Deleting a node with one child: remove the node and replace it with its child.</li> <li>Node to be deleted has two children: Find inorder successor of the node. Copy contents of the inorder successor to the node and delete the inorder successor.</li> <li>Note that: inorder successor can be obtained by finding the minimum value in right child of the node.</li> </ul>","tags":["Tree","Binary Search","BST"]},{"location":"graph/binary-search-tree/#sample-code","title":"Sample Code","text":"<pre><code>// C program to demonstrate delete operation in binary search tree \n#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n\nstruct node\n{\n    int key;\n    struct node *left, *right;\n};\n\n// A utility function to create a new BST node\nstruct node *newNode(int item)\n{\n    struct node *temp = (struct node *)malloc(sizeof(struct node));\n    temp-&gt;key = item;\n    temp-&gt;left = temp-&gt;right = NULL;\n    return temp;\n}\n\n// A utility function to do inorder traversal of BST\nvoid inorder(struct node *root)\n{\n    if (root != NULL)\n    {\n        inorder(root-&gt;left);\n        printf(\"%d \", root-&gt;key);\n        inorder(root-&gt;right);\n    }\n}\n\n/* A utility function to insert a new node with given key in BST */\nstruct node* insert(struct node* node, int key)\n{\n    /* If the tree is empty, return a new node */\n    if (node == NULL) return newNode(key);\n\n    /* Otherwise, recur down the tree */\n    if (key &lt; node-&gt;key)\n        node-&gt;left = insert(node-&gt;left, key);\n    else\n        node-&gt;right = insert(node-&gt;right, key);\n\n    /* return the (unchanged) node pointer */\n    return node;\n}\n\n/* Given a non-empty binary search tree, return the node with minimum\n   key value found in that tree. Note that the entire tree does not\n   need to be searched. */\nstruct node * minValueNode(struct node* node)\n{\n    struct node* current = node;\n\n    /* loop down to find the leftmost leaf */\n    while (current-&gt;left != NULL)\n        current = current-&gt;left;\n\n    return current;\n}\n\n/* Given a binary search tree and a key, this function deletes the key\n   and returns the new root */\nstruct node* deleteNode(struct node* root, int key)\n{\n    // base case\n    if (root == NULL) return root;\n\n    // If the key to be deleted is smaller than the root's key,\n    // then it lies in left subtree\n    if (key &lt; root-&gt;key)\n        root-&gt;left = deleteNode(root-&gt;left, key);\n\n    // If the key to be deleted is greater than the root's key,\n    // then it lies in right subtree\n    else if (key &gt; root-&gt;key)\n        root-&gt;right = deleteNode(root-&gt;right, key);\n\n    // if key is same as root's key, then This is the node\n    // to be deleted\n    else\n    {\n        // node with only one child or no child\n        if (root-&gt;left == NULL)\n        {\n            struct node *temp = root-&gt;right;\n            free(root);\n            return temp;\n        }\n        else if (root-&gt;right == NULL)\n        {\n            struct node *temp = root-&gt;left;\n            free(root);\n            return temp;\n        }\n\n        // node with two children: Get the inorder successor (smallest\n        // in the right subtree)\n        struct node* temp = minValueNode(root-&gt;right);\n\n        // Copy the inorder successor's content to this node\n        root-&gt;key = temp-&gt;key;\n\n        // Delete the inorder successor\n        root-&gt;right = deleteNode(root-&gt;right, temp-&gt;key);\n    }\n    return root;\n}\n</code></pre>","tags":["Tree","Binary Search","BST"]},{"location":"graph/binary-search-tree/#time-complexity","title":"Time Complexity","text":"<p>The worst case time complexity of search, insert, and deletion operations is \\(\\mathcal{O}(h)\\) where h is the height of Binary Search Tree. In the worst case, we may have to travel from root to the deepest leaf node. The height of a skewed tree may become \\(N\\) and the time complexity of search and insert operation may become \\(\\mathcal{O}(N)\\). So the time complexity of establishing \\(N\\) node unbalanced tree may become \\(\\mathcal{O}(N^2)\\) (for example the nodes are being inserted in a sorted way). But, with random input the  expected time complexity is \\(\\mathcal{O}(NlogN)\\).</p> <p>However, you can implement other data structures to establish Self-balancing binary search tree (which will be taught later), popular data structures that implementing this type of tree include:</p> <ul> <li>2-3 tree</li> <li>AA tree</li> <li>AVL tree</li> <li>B-tree</li> <li>Red-black tree</li> <li>Scapegoat tree</li> <li>Splay tree</li> <li>Treap</li> <li>Weight-balanced tree</li> </ul>","tags":["Tree","Binary Search","BST"]},{"location":"graph/bipartite-checking/","title":"Bipartite Checking","text":"<p>The question is in the title. Is the given graph bipartite? We can use BFS or DFS on graph. Lets first focus on BFS related algorithm. This procedure is very similar to BFS, we have an extra color array and we assign a color to each vertex when we are traversing the graph. Algorithm proof depends on fact that BFS explores the graph level by level. If the graph contains an odd cycle it means that there must be a edge between two vertices that are in same depth (layer, proof can be found on [1 - Algorithm Design, Kleinberg, Tardos]). Let's say the colors are red and black and we traverse the graph with BFS and assign red to odd layers and black to even layers. Then we check the edges to see if there exists an edge that its vertices are same color. If there is a such edge, the graph is not bipartite, else the graph is bipartite.</p> If two nodes x and y in the same layer are joined by an edge, then the cycle through x, y, and their lowest common ancestor z has odd length, demonstrating that the graph cannot be bipartite. <pre><code>typedef vector&lt;int&gt; adjList;\ntypedef vector&lt;adjList&gt; graph;\ntypedef pair&lt;int,int&gt; ii;\nenum COLOR {RED, GREEN};\nbool bipartite_check(graph &amp;g){\n    int root = 0; // Pick 0 indexed node as root.\n    vector&lt;bool&gt; visited(g.size(),false);\n    vector&lt;int&gt; Color(g.size(),0); \n    queue&lt;ii&gt; Q( { {root,0}} ); // insert root to queue, it is  first layer_0\n    visited[root] = true;\n    Color[root] = RED;\n    while ( !Q.empty() )\n    {\n        /*top.first is node, top.second its depth i.e layer */\n        auto top = Q.front();\n        Q.pop();\n        for (int u : g[top.first]){\n            if ( !visited[u] ){\n                visited[u] = true;\n                //Mark even layers to red, odd layers to green\n                Color[u] = (top.second+1) % 2 == 0 ?  RED : GREEN; \n                Q.push({u, top.second+1 });\n            }\n        }\n    }\n    for(int i=0; i &lt; g.size(); ++i){\n        for( auto v: g[i]){\n            if ( Color[i] == Color[v] ) return false;\n        }\n    }\n    return true;\n}\nint main(){\n    graph g(3);\n    g[0].push_back(1);\n    g[1].push_back(2);\n    g[2].push_back(3);\n    cout &lt;&lt; (bipartite_check(g) == true ? \"YES\" : \"NO\") &lt;&lt; endl;\n    return 0;\n}\n</code></pre> <p>The complexity of algorithm is is $O(V + E) + O(E) $, BFS and loop over edges. But we can say it  \\(O(V+E)\\) since it is Big-O notation.</p>","tags":["Bipartite Checking","Graph"]},{"location":"graph/breadth-first-search/","title":"Breadth First Search","text":"<p>Breadth First Search (BFS) is an algorithm for traversing or searching tree. (For example, you can find the shortest path from one node to another in an unweighted graph.)</p> An example breadth first search traversal","tags":["Graph","Breadth First Search","BFS"]},{"location":"graph/breadth-first-search/#method","title":"Method","text":"<p>BFS is a traversing algorithm where you should start traversing from a selected node (source or starting node) and traverse the graph layerwise thus exploring the neighbour nodes (nodes which are directly connected to source node). You must then move towards the next-level neighbour nodes. [1]</p> <p>\u2022 As the name BFS suggests, you are required to traverse the graph breadthwise as follows: \u2022 First move horizontally and visit all the nodes of the current layer \u2022 Add to the queue neighbour nodes of current layer. \u2022 Move to the next layer, which are in the queue</p> <p>Example question: Given a unweighted graph, a source and a destination, we need to find shortest path from source to destination in the graph in most optimal way?</p> <pre><code>#include &lt;bits/stdc++.h&gt;\nusing namespace std;\n\ncont int MaxN=100005; // Max number of nodes 5\n\nvector &lt;int&gt; adj[MaxN];\nbool mark[MaxN];\n\nvoid bfs(int starting_point,int ending_point) {\n    memset(mark,0,sizeof(mark)); //clear the cache\n    queue &lt;pair &lt;int,int&gt; &gt; q; // the value of node\n    // , and length between this node and the starting node\n\n    q.push_back(make_pair(starting_point,0));\n    mark[starting_point]=1;\n\n    while(q.empty()==false) {\n        pair &lt;int,int&gt; tmp = q.front(); // get the next node\n        q.pop(); // delete from q\n\n        if(ending_point==tmp.first) {\n            printf(\"The length of path between %d - %d : %d\\n\",\n            starting_point,ending_point,tmp.second);\n            return;\n        }\n\n        for (auto j : adj[tmp.first]) {\n            if(mark[j]) continue ; // if it reached before\n            mark[j]=1;\n            q.push_back(make_pair(j,tmp.second+1)); // add next node to queue\n        }\n    }\n}\n\nint main() {\n    cin \u00bb n\n\n    for (int i=0 ; i &lt; m; i++) {\n        cin \u00bb a \u00bb b;\n        adj[a].push_back(b);\n    }\n\n    cin \u00bb start_point \u00bb end_point;\n    bfs(start_point);\n    return 0;\n}\n</code></pre>","tags":["Graph","Breadth First Search","BFS"]},{"location":"graph/breadth-first-search/#complexity","title":"Complexity","text":"<p>The time complexity of BFS is \\(O(V + E)\\), where \\(V\\) is the number of nodes and \\(E\\) is the number of edges.</p>","tags":["Graph","Breadth First Search","BFS"]},{"location":"graph/bridges-and-articulation-points/","title":"Bridges and Articulation Points","text":"","tags":["Bridge","Articulation Point","Cut Vertex","Cut Edge","Graph"]},{"location":"graph/bridges-and-articulation-points/#dfs-order","title":"DFS Order","text":"<p>DFS order is traversing all the nodes of a given graph by fixing the root node in the same way as in the DFS algorithm, but without revisiting a discovered node. An important observation here is that the edges and nodes we use will form a tree structure. This is because, for every node (except the root), we only arrive from another node, and for the root node, we do not arrive from any other node, thus forming a tree structure.</p> <pre><code>void dfs(int node){\n    used[node] = true;\n    for(auto it : g[node])\n        if(!used[it])\n            dfs(it);\n}\n</code></pre>","tags":["Bridge","Articulation Point","Cut Vertex","Cut Edge","Graph"]},{"location":"graph/bridges-and-articulation-points/#types-of-edges","title":"Types of Edges","text":"<p>When traversing a graph using DFS order, several types of edges can be encountered. These edges will be very helpful in understanding some graph algorithms.</p> <p>Types of Edges: - Tree edge: These are the main edges used while traversing the graph. - Forward edge: These edges lead to a node that has been visited before and is located in our own subtree. - Back edge: These edges lead to nodes that have been visited before but where the DFS process is not yet complete. - Cross edge: These edges lead to nodes that have been visited before and where the DFS process is already complete.</p> <p>An important observation about these edges is that in an undirected graph, it is impossible to have a cross edge. This is because it is not possible for an edge emerging from a node where the DFS process is complete to remain unvisited.</p> Green-colored edges are tree edges. Edge (1,8) is a forward edge. Edge (6,4) is a back edge. Edge (5,4) is a cross edge.","tags":["Bridge","Articulation Point","Cut Vertex","Cut Edge","Graph"]},{"location":"graph/bridges-and-articulation-points/#bridge","title":"Bridge","text":"<p>In an undirected and connected graph, if removing an edge causes the graph to become disconnected, this edge is called a bridge.</p>","tags":["Bridge","Articulation Point","Cut Vertex","Cut Edge","Graph"]},{"location":"graph/bridges-and-articulation-points/#finding-bridges","title":"Finding Bridges","text":"<p>Although there are several algorithms to find bridges (such as Chain Decomposition), we will focus on Tarjan's Algorithm, which is among the easiest to implement and the fastest.</p> <p>When traversing a graph using DFS, if there is a back edge coming out of the subtree of the lower endpoint of an edge, then that edge is not a bridge. This is because the back edge prevents the separation of the subtree and its ancestors when the edge is removed.</p> <p>This algorithm is based exactly on this principle, keeping track of the minimum depth reached by the back edges within the subtree of each node.</p> <p>If the minimum depth reached by the back edges in the subtree of the lower endpoint of an edge is greater than or equal to the depth of the upper endpoint, then this edge is a bridge. This is because no back edge in the subtree of the edge's lower endpoint reaches a node above the current edge. Therefore, if we remove this edge, the subtree and its ancestors become disconnected.</p> <p>Using Tarjan's Algorithm, we can find all bridges in a graph with a time complexity of \\(\\mathcal{O}(V + E)\\), where \\(V\\) represents the number of vertices and \\(E\\) represents the number of edges in the graph.</p> <pre><code>int dfs(int node, int parent, int depth) {\n    int minDepth = depth;\n    dep[node] = depth;  // dep dizisi her dugumun derinligini tutmaktadir.\n    used[node] = true;\n    for (auto it : g[node]) {\n        if (it == parent)\n            continue;\n        if (used[it]) {\n            minDepth = min(minDepth, dep[it]);\n            // Eger komsu dugum daha once kullanilmis ise\n            // Bu edge back edge veya forward edgedir.\n            continue;\n        }\n        int val = dfs(it, node, depth + 1);\n        // val degeri alt agacindan yukari cikan minimum derinliktir.\n        if (val &gt;= depth + 1)\n            bridges.push_back({node, it});\n        minDepth = min(minDepth, val);\n    }\n    return minDepth;\n}\n</code></pre>","tags":["Bridge","Articulation Point","Cut Vertex","Cut Edge","Graph"]},{"location":"graph/bridges-and-articulation-points/#articulation-point","title":"Articulation Point","text":"<p>In an undirected graph, if removing a node increases the number of connected components, that node is called an articulation point or cut point.</p> For example, if we remove node 0, the remaining nodes are split into two groups: 5 and 1, 2, 3, 4. Similarly, if we remove node 1, the nodes are split into 5, 0 and 2, 3, 4. Therefore, nodes 0 and 1 are **articulation points**.","tags":["Bridge","Articulation Point","Cut Vertex","Cut Edge","Graph"]},{"location":"graph/bridges-and-articulation-points/#finding-articulation-points","title":"Finding Articulation Points","text":"<p>Tarjan's Algorithm for finding articulation points in an undirected graph:</p> <ul> <li> <p>Traverse the graph using DFS order.</p> </li> <li> <p>For each node, calculate the depth of the minimum depth node that can be reached from the current node and its subtree through back edges. This value is called the low value of the node.</p> </li> <li> <p>If the low value of any child of a non-root node is greater than or equal to the depth of the current node, then the current node is an articulation point. This is because no back edge in the subtree of this node can reach a node above the current node. Therefore, if this node is removed, its subtree will become disconnected from its ancestors.</p> </li> <li> <p>If the current node is the root (the starting node of the DFS order) and there are multiple branches during the DFS traversal, then the root itself is an articulation point. This is because the root has multiple connected subgraphs.</p> </li> </ul> <p>Using Tarjan's Algorithm, we can find all articulation points in a graph with a time complexity of \\(\\mathcal{O}(V + E)\\), where \\(V\\) is the number of vertices and \\(E\\) is the number of edges in the graph.</p> <pre><code>int dfs(int node, int parent, int depth) {\n    int minDepth = depth, children = 0;\n    dep[node] = depth;  // dep array holds depth of each node.\n    used[node] = true;\n    for (auto it : g[node]) {\n        if (it == parent)\n            continue;\n        if (used[it]) {\n            minDepth = min(minDepth, dep[it]);\n            continue;\n        }\n        int val = dfs(it, node, depth + 1);\n        if (val &gt;= depth and parent != -1)\n            isCutPoint[node] = true;\n        minDepth = min(minDepth, val);\n        children++;\n    }\n    // This if represents the root condition that we mentioned above.\n    if (parent == -1 and children &gt;= 2)\n        isCutPoint[node] = true;\n    return minDepth;\n}\n</code></pre>","tags":["Bridge","Articulation Point","Cut Vertex","Cut Edge","Graph"]},{"location":"graph/cycle-finding/","title":"Cycle Finding","text":"<p>Cycle: A sequence of nodes that returns to the starting node while visiting each node at most once and contains at least two nodes. </p> <p>We can use dfs order in order to find the graph has a cycle or not.</p> <p>If we find a back edge while traversing the graph then we can say that graph has a cycle. Because back edge connects the nodes at the top and bottom ends and causes a cycle.</p> <p>The algorithm that we are going to use to find the cycle in the directed graph:</p> <ul> <li>Traverse the graph with dfs order.</li> <li>When you come to a node, color it gray and start visiting its neighbors.</li> <li>If one of the current node's neighbors is gray, then there is a cycle in the graph. Because a gray node is definitely an ancestor of the current node, and an edge to one of its ancestors is definitely a back edge.</li> <li>Once you're done visiting the neighbors, color the node black.</li> </ul> <pre><code>bool dfs(int node){\n    // The color array holds the color of each node.\n    // 0 represents white, 1 represents gray, and 2 represents black.\n    color[node] = 1;\n    for(int i = 0; i &lt; g[node].size(); i++){\n        int child = g[node][i];\n        if(color[child] == 1)\n            return true;\n        if(!color[child])\n            if(dfs(child))\n                return true;\n    }\n    color[node] = 2;\n    return false;\n}\n</code></pre>","tags":["Graph","Cycle"]},{"location":"graph/definitions/","title":"Graph Definitions","text":"","tags":["Graph"]},{"location":"graph/definitions/#definitions-of-common-terms","title":"Definitions of Common Terms","text":"<ul> <li>Node - An individual data element of a graph is called Node. Node is also known as vertex.</li> <li>Edge - An edge is a connecting link between two nodes. It is represented as e = {a,b} Edge is also called Arc.</li> <li>Adjacent - Two vertices are adjacent if they are connected by an edge.</li> <li>Degree - a degree of a node is the number of edges incident to the node.</li> <li>Undirected Graphs - Undirected graphs have edges that do not have a direction. The edges indicate a two-way relationship, in that each edge can be traversed in both directions.</li> <li>Directed Graphs - Directed graphs have edges with direction. The edges indicate a one-way relationship, in that each edge can only be traversed in a single direction.</li> <li>Weighted Edges - If each edge of graphs has an association with a real number, this is called its weight.</li> <li>Self-Loop - It is an edge having the same node for both destination and source point.</li> <li>Multi-Edge - Some Adjacent nodes may have more than one edge between each other.</li> </ul>","tags":["Graph"]},{"location":"graph/definitions/#walks-trails-paths-cycles-and-circuits","title":"Walks, Trails, Paths, Cycles and Circuits","text":"<ul> <li>Walk - A sequence of nodes and edges in a graph.</li> <li>Trail - A walk without visiting the same edge.</li> <li>Circuit - A trail that has the same node at the start and end.</li> <li>Path - A walk without visiting same node.</li> <li>Cycle - A circuit without visiting same node. </li> </ul>","tags":["Graph"]},{"location":"graph/definitions/#special-graphs","title":"Special Graphs","text":"<ul> <li>Complete Graph - A graph having at least one edge between every two nodes.</li> <li>Connected Graph - A graph with paths between every pair of nodes.</li> <li>Tree - an undirected connected graph that has any two nodes that are connected by exactly one path. There are some other definitions that you can notice it is tree:<ul> <li>an undirected graph is connected and has no cycles. an undirected graph is acyclic, and a simple cycle is formed if any edge is added to the graph.</li> <li>an undirected graph is connected, it will become disconnected if any edge is removed.</li> <li>an undirected graph is connected, and has  (number of nodes - 1) edges.</li> </ul> </li> </ul>","tags":["Graph"]},{"location":"graph/definitions/#bipartite-graphs","title":"Bipartite Graphs","text":"<p>A bipartite graph is a graph whose vertices can be divided into two disjoint and independent sets U and V such that every edge connects a vertex in U to one in V. Vertex sets U and V are usually called the parts of the graph. [1]. The figure is shown in below. It is similar to graph coloring with two colors. Coloring graph with two colors is that every vertex have a corresponding color, and for any edge, it's vertices should be different color. In other words, if we can color neighbours two different colors, we can say that graph is bipartite.</p> Example bipartite graph, all edges satisfy the coloring constraint <p>We have some observations here. - A graph 2- colorable if and only if it is bipartite. - A graph does not contain odd-length cycle if and only if it is bipartite. - Every tree is a bipartite graph since trees do not contain any cycles.</p>","tags":["Graph"]},{"location":"graph/definitions/#directed-acyclic-graphs","title":"Directed Acyclic Graphs","text":"<p>A directed acyclic graph(DAG) is a finite directed graph with no directed cycles. Equivalently, a DAG is a directed graph that has a topological ordering (we cover it in this bundle), a sequence of the vertices such that every edge is directed from earlier to later in the sequence [2]. DAGs can be used to encode precedence relations or dependencies in a natural way [3 - Algorithm Design, Kleinberg, Tardos]. There are several applications using topological ordering directly such as finding  critical path or automatic differentiation on computational graphs (this is extremely useful for deep learning frameworks [4]).</p> Example Directed Acyclic Graphs Example computational graph also a DAG, partial derivatives are written to edges respect to topological order","tags":["Graph"]},{"location":"graph/depth-first-search/","title":"Depth First Search","text":"<p>Depth First Search (DFS) is an algorithm for traversing or searching tree. (For example, you can check if graph is connected or not via DFS) [2]</p> Example of DFS traversal","tags":["Graph","Depth First Search","DFS"]},{"location":"graph/depth-first-search/#method","title":"Method","text":"<p>The DFS algorithm is a recursive algorithm that uses the idea of backtracking. It involves exhaustive searches of all the nodes by going ahead, if possible, else by backtracking.</p> <p>Here, the word backtrack means that when you are moving forward and there are no more nodes along the current path, you move backwards on the same path to find nodes to traverse. All the nodes will be visited on the current path till all the unvisited nodes have been traversed after which the next path will be selected. [3]</p> <pre><code>vector&lt;vector&lt;int\u00bb adj; // graph represented as an adjacency list\nint n; // number of vertices\nvector&lt;bool&gt; visited;\nvoid dfs(int v) {\n    visited[v] = true;\n    for (int u : adj[v]) {\n        if (!visited[u]) dfs(u);\n    }\n}\n</code></pre> <p>This recursive nature of DFS can be implemented using stacks. The basic idea is as follows: Pick a starting node and push all its adjacent nodes into a stack. Pop a node from stack to select the next node to visit and push all its adjacent nodes into a stack. Repeat this process until the stack is empty. However, ensure that the nodes that are visited are marked. This will prevent you from visiting the same node more than once. If you do not mark the nodes that are visited and you visit the same node more than once, you may end up in an infinite loop. [3]</p> <pre><code>DFS-iterative(G, s): //Where G is graph and s is source vertex let S be stack\nS.push(s) //Inserting s in stack\nmark s as visited.\nwhile ( S is not empty):\n    //Pop a vertex from stack to visit next v = S.top( )\n    S.pop( )\n    //Push all the neighbours of v in stack that are not visited\n    for all neighbours w of v in Graph G:\n        if w is not visited :\n            S.push(w)\n            mark w as visited\n</code></pre> <p>Example Question: Given an undirected graph, find out whether the graph is strongly connected or not? An undirected graph is strongly connected if there is a path between any two pair of vertices.</p> <pre><code>#include &lt;bits/stdc++.h&gt;\nusing namespace std;\n\ncont int MaxN=100005; // Max number of nodes\n\nvector &lt;int&gt; adj[MaxN];\nbool mark[MaxN];\n\nvoid dfs(int k) {\n    mark[k]=1; // visited\n    for(auto j : adj[k]) // iterate over adjacent nodes\n        if(mark[j]==false) // check if it is visited or not\n            dfs(j); // do these operation for that node\n}\n\nint main() {\n    cin \u00bb n \u00bb m; // number of nodes , number of edges\n    for (int i=0 ; i &lt; m; i++){\n        cin \u00bb a \u00bb b;\n        adj[a].push_back(b);\n        adj[b].push_back(a);\n    }\n\n    dfs(1);\n\n    bool connected=1;\n    for(int i=1 ; i &lt;= n ;i++)\n        if(mark[i]==0) {\n            connected=0;\n            break;\n        }\n\n    if(connected)\n        cout \u00ab \"Graph is connected\" \u00ab endl;\n    else\n        cout \u00ab \"Graph is not connected\" \u00ab endl;\n\n    return 0;\n}\n</code></pre>","tags":["Graph","Depth First Search","DFS"]},{"location":"graph/depth-first-search/#complexity","title":"Complexity","text":"<p>The time complexity of DFS is \\(O(V+E)\\) when implemented using an adjacency list ( with Adjacency Matrices it is \\(O(V^2)\\)), where \\(V\\) is the number of nodes and \\(E\\) is the number of edges. [4]</p>","tags":["Graph","Depth First Search","DFS"]},{"location":"graph/heap/","title":"Heap","text":"an example max-heap with 9 nodes <p>The heap is a complete binary tree with N nodes, the value of all the nodes in the left and right sub-tree of the root node should be smaller than the root node's value.</p> <p>In a heap, the highest (or lowest) priority element is always stored at the root. A heap is not a sorted structure and can be regarded as partially ordered. As visible from the heap-diagram, there is no particular relationship among nodes on any given level, even among the siblings. Because a heap is a complete binary tree, it has a smallest possible height. A heap with \\(N\\) nodes has \\(logN\\) height. A heap is a useful data structure when you need to remove the object with the highest (or lowest) priority.</p>","tags":["Heap","Priority Queue"]},{"location":"graph/heap/#implementation","title":"Implementation","text":"<p>Heaps are usually implemented in an array (fixed size or dynamic array), and do not require pointers between elements. After an element is inserted into or deleted from a heap, the heap property may be violated and the heap must be balanced by internal operations.</p> <p>The first (or last) element will contain the root. The next two elements of the array contain its children. The next four contain the four children of the two child nodes, etc. Thus the children of the node at position n would be at positions \\(2*n\\) and \\(2*n + 1\\) in a one-based array. This allows moving up or down the tree by doing simple index computations. Balancing a heap is done by sift-up or sift-down operations (swapping elements which are out of order). So we can build a heap from an array without requiring extra memory.</p> example a heap as an array","tags":["Heap","Priority Queue"]},{"location":"graph/heap/#insertion","title":"Insertion","text":"<p>Basically add the new element at the end of the heap. Then look it's parent if it is smaller or bigger depends on the whether it is max-heap or min-heap (max-heap called when Parents are always greater), swap with the parent. If it is swapped do the same operation for the parent.</p>","tags":["Heap","Priority Queue"]},{"location":"graph/heap/#deletion","title":"Deletion","text":"<p>If you are going to delete a node (root node or another one does not matter),</p> <ol> <li>Swap the node to be deleted with the last element of heap to maintain a balanced structure.</li> <li>Delete the last element which is the node we want to delete at the start.</li> <li>Now you have a node which is in the wrong place, You have to find the correct place for the swapped last element, to do this starting point you should check its left and right children, if one them is greater than our node you should swap it with the greatest child(or smallest if it is min-heap).</li> <li>Still current node may in the wrong place, so apply Step 3 as long as it is not greater than its children(or smaller if it is min-heap).</li> </ol> an example deletion on a heap structure <pre><code>class BinHeap:\n    def __init__(self):\n        self.heapList = [0]\n        self.currentSize = 0\n\n    def percUp(self,i):\n        while i // 2 &gt; 0:\n            if self.heapList[i] &lt; self.heapList[i // 2]:\n                tmp = self.heapList[i // 2]\n                self.heapList[i // 2] = self.heapList[i]\n                self.heapList[i] = tmp\n            i = i // 2\n\n    def insert(self,k):\n        self.heapList.append(k)\n        self.currentSize = self.currentSize + 1\n        self.percUp(self.currentSize)\n\n    def percDown(self,i):\n        while (i * 2) &lt;= self.currentSize:\n            mc = self.minChild(i)\n            if self.heapList[i] &gt; self.heapList[mc]:\n                tmp = self.heapList[i]\n                self.heapList[i] = self.heapList[mc]\n                self.heapList[mc] = tmp\n            i = mc\n\n    def minChild(self,i):\n        if i * 2 + 1 &gt; self.currentSize:\n            return i * 2\n        else:\n            if self.heapList[i*2] &lt; self.heapList[i*2+1]:\n                return i * 2\n            else:\n                return i * 2 + 1\n\n    def delMin(self):\n        retval = self.heapList[1]\n        self.heapList[1] = self.heapList[self.currentSize]\n        self.currentSize = self.currentSize - 1\n        self.heapList.pop()\n        self.percDown(1)\n        return retval\n\n    def buildHeap(self,alist):\n        i = len(alist) // 2\n        self.currentSize = len(alist)\n        self.heapList = [0] + alist[:]\n        while (i &gt; 0):\n            self.percDown(i)\n            i = i - 1\n\nbh = BinHeap()\nbh.buildHeap([9,5,6,2,3])\n\nprint(bh.delMin())\nprint(bh.delMin())\nprint(bh.delMin())\nprint(bh.delMin())\nprint(bh.delMin())\n</code></pre>","tags":["Heap","Priority Queue"]},{"location":"graph/heap/#complexity","title":"Complexity","text":"<p>Insertion \\(\\mathcal{O}(logN)\\), delete-min \\(\\mathcal{O}(logN)\\) , and finding minimum \\(\\mathcal{O}(1)\\). These operations depend on heap's height and heaps are always complete binary trees, basically the height is \\(logN\\). (N is number of Node)</p>","tags":["Heap","Priority Queue"]},{"location":"graph/heap/#priority-queue","title":"Priority Queue","text":"<p>Priority queues are a type of container adaptors, specifically designed so that its first element is always the greatest of the elements it contains, according to some strict weak ordering criterion.</p> <p>While priority queues are often implemented with heaps, they are conceptually distinct from heaps. A priority queue is an abstract concept like \"a list\" or \"a map\"; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with a heap or a variety of other methods such as an unordered array.</p> <pre><code>#include &lt;iostream&gt;       // std::cout\n#include &lt;queue&gt;          // std::priority_queue\nusing namespace std;\nint main () {\n    priority_queue&lt;int&gt; mypq;\n\n    mypq.push(30);\n    mypq.push(100);\n    mypq.push(25);\n    mypq.push(40);\n\n    cout &lt;&lt; \"Popping out elements...\";\n    while (!mypq.empty()) {\n        cout &lt;&lt; ' ' &lt;&lt; mypq.top();\n        mypq.pop();\n    }\n    return 0;\n}\n</code></pre>","tags":["Heap","Priority Queue"]},{"location":"graph/introduction/","title":"Introduction","text":"<p>A graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense \"related\". The objects correspond to the mathematical abstractions called vertices (also called nodes or points) and each of the related pairs of vertices is called an edge. Typically, a graph is depicted in diagrammatic form as a set of dots for the vertices, joined by lines for the edges. [8]</p> <p>Why graphs? Graphs are usually used to represent different elements that are somehow related to each other.</p> <p>A Graph consists of a finite set of vertices(or nodes) and set of edges which connect a pair of nodes. G = (V,E)</p> <p>V = set of nodes</p> <p>E = set of edges(e) represented as e = a,b</p> <p>Graph are used to show a relation between objects. So, some graphs may have directional edges (e.g. people and their love relationships that are not mutual: Alice may love Alex, while Alex is not in love with her and so on), and some graphs may have weighted edges (e.g. people and their relationship in the instance of a debt)</p> Figure 1: a simple unweigted graph","tags":["Graph"]},{"location":"graph/max-flow/","title":"Max Flow","text":"","tags":["Graph","Max Flow","Maximum Flow","Ford Fulkerson"]},{"location":"graph/max-flow/#flow-network","title":"Flow Network","text":"<p>A flow network is a special type of directed graph that contains a single source and a single target node. In a flow network, each edge has a capacity, which indicates the maximum amount of flow that can pass through that edge.</p> One of the earliest examples of a flow network in history.","tags":["Graph","Max Flow","Maximum Flow","Ford Fulkerson"]},{"location":"graph/max-flow/#maximum-flow","title":"Maximum Flow","text":"<p>Maximum flow is an algorithm that calculates the maximum amount of flow that can reach the target from the source in a flow network while maintaining a continuous flow.</p> <p>There are several algorithms to solve the Maximum Flow problem. The time complexities of some popular ones are:</p> <ul> <li>Ford-Fulkerson algorithm: \\(\\mathcal{O}(E * \\text{flowCount})\\)</li> <li>Edmonds-Karp algorithm: \\(\\mathcal{O}(V * E^2)\\)</li> <li>Dinic's algorithm: \\(\\mathcal{O}(E * V^2)\\)</li> </ul> <p>Where V is the number of vertices and E is the number of edges in the flow network.</p>","tags":["Graph","Max Flow","Maximum Flow","Ford Fulkerson"]},{"location":"graph/max-flow/#ford-fulkerson","title":"Ford Fulkerson","text":"<p>The steps of the Ford-Fulkerson maximum flow algorithm are as follows:</p> <ul> <li>Find a path from the source to the target.</li> <li>The edge with the minimum capacity in the found path determines the flow that can pass through this path.</li> <li>Decrease the capacities of the edges in the path by the flow amount (the minimum capacity found in step 2) and add the reverse edges to the graph with a capacity equal to the flow.</li> <li>Repeat until there are no more paths from the source to the target.</li> </ul> <p>Why does this algorithm work?</p> <p>For example, let's assume we find a flow of size x through an edge from u to v.</p> <p>Suppose the path we found is \\(a \\rightarrow ... \\rightarrow u \\rightarrow v \\rightarrow ... \\rightarrow b\\).</p> <p>We will add a new edge from v to u with a capacity of x to our graph, but this newly added reverse edge does not exist in the original graph.</p> <p>After adding the reverse edges, the new path we find might look like \\(c \\rightarrow ... \\rightarrow v \\rightarrow u \\rightarrow ... \\rightarrow d\\), with a flow of size y.</p> <p>It is clear that \\(y \\leq x\\).</p> <p>We can represent three different valid flows as follows:</p> <ul> <li>A flow of size y following the path \\(a \\rightarrow ... \\rightarrow u \\rightarrow ... \\rightarrow d\\)</li> <li>A flow of size y following the path \\(c \\rightarrow ... \\rightarrow u \\rightarrow ... \\rightarrow b\\)</li> <li>A flow of size x - y following the path \\(a \\rightarrow ... \\rightarrow u \\rightarrow v \\rightarrow ... \\rightarrow d\\)</li> </ul> <p>The overall time complexity of the Ford-Fulkerson algorithm is \\(\\mathcal{O}(E * \\text{flowCount})\\) because, in the worst case, each found path increases the flow by only 1. Since finding each path takes time proportional to the number of edges, the complexity becomes \\(\\mathcal{O}(E * \\text{flowCount})\\).</p> <p>However, if we implement the Ford-Fulkerson algorithm using BFS, the complexity changes. In this case, for every edge, the flows that consider this edge as the bottleneck will continually increase, leading to a time complexity of \\(\\mathcal{O}(V * E^2)\\). This specific implementation is known as the Edmonds-Karp Algorithm.</p> The figure on the left shows how much flow is passing through each edge. The figure on the right represents the current state of the graph. Flow = 7 Flow = 8 Flow = 13 Flow = 15 <pre><code>// c matrix holds the capacities of the edges.\n// g adjacency list allows us to traverse the graph.\nbool bfs() {\n    vector&lt;bool&gt; visited(n, false);\n    queue&lt;int&gt; q;\n    q.push(source);\n    visited[source] = true;\n    while (!q.empty()) {\n        int node = q.front();\n        q.pop();\n        if (node == sink)\n            break;\n        for (int i = 0; i &lt; g[node].size(); i++) {\n            int child = g[node][i];\n            if (c[node][child] &lt;= 0 or visited[child])\n                continue;\n            visited[child] = true;\n            parent[child] = node;\n            q.push(child);\n        }\n    }\n    return visited[sink];\n}\nint max_flow() {\n    while (bfs()) {\n        int curFlow = -1, node = sink;\n        while (node != source) {\n            // curFlow is the minimum capacity in the current path, i.e. the flow we found.\n            int len = c[parent[node]][node];\n            if (curFlow == -1)\n                curFlow = len;\n            else\n                curFlow = min(curFlow, len);\n            node = parent[node];\n        }\n        flow += curFlow;\n        node = sink;\n        while (node != source) {\n            c[parent[node]][node] -= curFlow;\n            // We are subtracting the flow we found from the path we found.\n            c[node][parent[node]] += curFlow;  // We are adding the reverses of the edges\n            node = parent[node];\n        }\n    }\n    return flow;\n}\n</code></pre>","tags":["Graph","Max Flow","Maximum Flow","Ford Fulkerson"]},{"location":"graph/minimum-spanning-tree/","title":"Minimum Spanning Tree","text":"","tags":["Graph","Minimum Spanning Tree","Prim","Kruskal"]},{"location":"graph/minimum-spanning-tree/#definition","title":"Definition","text":"<p>Given an undirected weighted connected graph \\(G = (V,E)\\) Spanning tree of G is a connected acyclic sub graph that covers all nodes and some edges. In a disconnected graph -where there is more than one connected component- the spanning tree of that graph is defined as the forest of the spanning trees of each connected component of the graph.</p> <p>Minimum spanning tree (MST) is a spanning tree in which the sum of edge weights is minimum. The MST of a graph is not unique in general, there might be more than one spanning tree with the same minimum cost. For example, take a graph where all edges have the same weight, then any spanning tree would be a minimum spanning tree. In problems involving minimum spanning trees where you have to output the tree itself (and not just the minimum cost), it either puts more constraint so the answer is unique, or simply asks for any minimum spanning tree.</p> MST of the graph. It spans all nodes of the graph and it is connected. <p>To find the minimum spanning tree of a graph, we will introduce two algorithms. The first one called Prim's algorithm, which is similar to Dijkstra's algorithm. Another algorithm is  Kruskal agorithm, which makes use of the disjoint set data structure. Let's discover each one of them in detail!</p>","tags":["Graph","Minimum Spanning Tree","Prim","Kruskal"]},{"location":"graph/minimum-spanning-tree/#prim-algorithm","title":"Prim Algorithm","text":"<p>Prim algorithm is very similar to Dijkstra's shortest path algorithm. In this algorithm we have a set \\(S\\) which represents the explored nodes and again we can maintain a priority queue data structure the closest node in \\(V-S\\). It is a greedy algorithm just like Dijkstra's shortest path algorithm.</p> <pre><code>G = (V, E)   V set of all nodes, E set of all edges\nT = {}       result, edges of MST\nS = {1}      explored nodes\nwhile S /= V do\n    let (u, v) be the lowest cost edge such that u in S and v in V - S;\n    T = T U {(u, v)}\n    S = S U {v}\nend\n</code></pre> Prim Algorithm in Pseudo code, what is the problem here? <p>There is a problem with this implementation, it assumes that the graph is connected. If the graph is not connected this algorithm will be stuck on loop. There is a good visualization for Prim algorithm at [10]. If we use priority queue complexity would be \\(O(ElogV)\\).</p> Example of how Prim Algorithm constructs the MST","tags":["Graph","Minimum Spanning Tree","Prim","Kruskal"]},{"location":"graph/minimum-spanning-tree/#kruskal-algorithm","title":"Kruskal Algorithm","text":"<p>In Prim algorithm we started with a specific node and then proceeded with choosing the closest neighbor node to our current graph. In Kruskal algorithm, we follow a different strategy; we start building our MST by choosing one edge at a time, and link our (intially separated) nodes together until we connect all of the graph.</p> <p>To achieve this task, we will start with having all the nodes separated each in a group. In addition, we will have the list of edges from the original graph sorted based on their cost. At each step, we will:</p> <ol> <li>Pick the smallest available edge (that is not taken yet)</li> <li>Link the nodes it connects together, by merging their group into one unified group</li> <li>Add the cost of the edge to our answer</li> </ol> <p>However, you may realize in some cases the link we add will connect two nodes from the same group (because they were grouped before by other taken edges), hence violating the spanning tree condition (Acyclic) and more importantly introducing unnecessary edges that adds more cost to the answer. So to solve this problem, we will only add the edges as long as they connect two currently (at the time of processing this edge) separated nodes that belong to different groups, hence completing the algorithm.</p> <p>The optimality of Kruskal algorithm comes from the fact that we are taking from a sorted list of edges. For more rigorous proof please refer to [11].</p> <p>So how can we effectively merge the group of nodes and check that which group each node belong? We can utilize disjoint set data structure which will help us to make union and find operations in an amortized constant \\(\\mathcal{O}(1)\\) time.</p> <pre><code>typedef pair&lt;int,pair&lt;int,int&gt;&gt; edge;\n// represent edge as triplet (w,u,v)\n// w is weigth, u and v verticies.\n// edge.first is weigth edge.second.first -&gt; u, edge.second.second -&gt; v\ntypedef vector&lt;edge&gt; weigthed_graph;\n\n/*union - find data structure utilities */\nconst int maxN = 3005;\nint parent[maxN];\nint ssize[maxN];\nvoid make_set(int v);\nint find_set(int v);\nvoid union_sets(int a, int b);\nvoid init_union_find();\n\n/*Code that finds edges in MST */\nvoid kruskal(vector&lt;edge&gt; &amp;edgeList ){\n    vector&lt;edge&gt; mst;\n    init_union_find();\n    sort(edgeList.begin(),edgeList.end(), \\\n        [](const auto &amp;a, const auto  &amp;b) { return a.first&lt; b.first;}); \n    //well this weird syntax is lambda function \n    // for sorting pairs to respect their first element.\n    for( auto e: edgeList){\n        if( find_set(e.second.first )!= find_set(e.second.second)){\n            mst.push_back(e);\n            union_sets(e.second.first, e.second.second);\n        }\n    }\n}\n</code></pre> <p>To calculate the time complexity, observe how we first sorted the edges, this takes \\(\\mathcal{O}(E log E)\\). In addition we pass through the edges one by one, and each time we check which group the two nodes of the edge belongs to, and in some cases merge the two groups. So in the worst case we will assume that both operations (finding and merging) happens, but since the disjoint data structure guarantee \\(\\mathcal{O}(1)\\) amortized time for both operations, we end up with \\(\\mathcal{O}(E)\\) amortized time of processing the nodes.</p> <p>So in total we have \\(\\mathcal{O}(E log E)\\) from sorting edges and \\(\\mathcal{O}(E)\\) from processing them, those results in a total of \\(\\mathcal{O}(E log E)\\) (if you don't understand why please refer to the first bundle where we discuss time complexity).</p> Example of how Kruskal Algorithm constructs the MST","tags":["Graph","Minimum Spanning Tree","Prim","Kruskal"]},{"location":"graph/representing-graphs/","title":"Representing Graphs","text":"","tags":["Graph"]},{"location":"graph/representing-graphs/#edge-lists","title":"Edge Lists","text":"<p>A simple way to define edge list is that it has a list of pairs. We just have a list of objects consisting of the vertex numbers of 2 nodes and other attributes like weight or the direction of edges. [16]</p> <ul> <li>+ For some specific algorithms you need to iterate over all the edges, (i.e. kruskal's algorithm)</li> <li>+ All edges are stored exactly once.</li> <li>- It is hard to determine whether two nodes are connected or not.</li> <li>- It is hard to get information about the edges of a specific vertex.</li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\nusing namespace std;\n\nint main(){\n    int edge_number;\n    vector&lt;pair &lt;int,int&gt; &gt; edges;\n    cin &gt;&gt; edge_number;\n    for( int i=0 ; i&lt;edge_number ; i++ ){\n        int a,b;\n        cin &gt;&gt; a &gt;&gt; b;\n        edges.push_back(make_pair(a,b)); // a struct can be used if edges are weighted or have other properties.\n    }\n}\n</code></pre>","tags":["Graph"]},{"location":"graph/representing-graphs/#adjacency-matrices","title":"Adjacency Matrices","text":"<p>Stores edges, in a 2-D matrix. matrix[a][b] keeps an information about road from a to b. [16] - + We can easily check if there is a road between two vertices. - - Looping through all edges of a specific node is expensive because you have to check all of the empty cells too. Also these empty cells takes huge memory in a graph which has many vertices. (For example representing a tree)</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\nusing namespace std;\nint main(){\n    int node_number;\n    vector&lt;vector&lt;int&gt; &gt; Matrix;\n    cin &gt;&gt; node_number;\n    for( int i=0 ; i&lt;node_number ; i++ )\n        for( int j=0 ; j&lt;node_number ; j++ ){\n            Matrix.push_back(vector &lt;int&gt; ());\n            int weight;\n            cin &gt;&gt;weight ;\n            Matrix[i].push_back(weight);\n        }\n}\n</code></pre>","tags":["Graph"]},{"location":"graph/representing-graphs/#adjacency-list","title":"Adjacency List","text":"<p>Each node has a list consisting of nodes each is adjacent to. So, there will be no empty cells. Memory will be equal to number of edges. The most used one is in algorithms. [16]</p> <ul> <li>+ You do not have to use space for empty cells.</li> <li>+ Easily iterate over all the neighbors of a specific node.</li> <li>- If you want to check if two nodes are connected, in this form you still need to iterate over all the neighbors of one of them. But, there are some structures that you can do this operation in O(log N). For example if you won't add any edge, you can sort every vector with nodes' names, so you can find it by binary search.  </li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\nusing namespace std;\n\nint main(){\n    int node_number,path_number;\n\n    vector&lt;vector&lt;int&gt; &gt; paths; \n    // use object instead of int, \n    //if you need to store other features\n\n    cin &gt;&gt; node_number &gt;&gt; path_number;\n    for( int i=0 ; i&lt;node_number ; i++ )\n        Matrix.push_back(vector &lt;int&gt; ());\n    for( int j=0 ; j&lt; path_number ; j++ ){\n        int beginning_node, end_node;\n        cin &gt;&gt; beginning_node &gt;&gt; end_node;\n\n        Matrix[ beginning_node ].push_back( end_node ); // push st\n        // Matrix[ end_node ].push_back(  beginning_node ); \n        // ^^^ If edges are Undirected, you should push in reverse direction too\n    }\n}\n</code></pre>","tags":["Graph"]},{"location":"graph/shortest-path/","title":"Shortest Path Problem","text":"","tags":["Graph","Shortest Path Problem","Dijkstra"]},{"location":"graph/shortest-path/#definition","title":"Definition","text":"<p>Let \\(G(V,E)\\) be a graph, \\(v_i\\) and \\(v_j\\) be two nodes of \\(G\\). We say a path between \\(v_i\\) and \\(v_j\\) is the shortest path if sum of the edge weights (cost) in the path is minimum. In other words, the shortest path problem is the problem of finding a path between two vertices (or nodes) in a graph such that the sum of the weights of its constituent edges is minimized. [5]</p> Example shortest path in graph. Source is A and target is F. Image taken from [5]. <p>We will cover several shortest path algorithms in this bundle. One of them is Dijkstra\u2019s Shortest Path Algorithm but it has some drawbacks: Edge weights should be non-negative for the optimally of the algorithm. We will discover other algorithms in which these condition isn\u2019t necessary, like Floyd-Warshall and Bellman-Ford algorithms.</p>","tags":["Graph","Shortest Path Problem","Dijkstra"]},{"location":"graph/shortest-path/#dijkstras-shortest-path-algorithm","title":"Dijkstra's Shortest Path Algorithm","text":"<p>Dijkstra\u2019s Shortest Path algorithm is straight forward. In brief we have a set \\(S\\) that contains explored nodes and \\(d\\) which contains the shortest path cost from source to another node. In other words, \\(d(u)\\) represents the shortest path cost from source to node \\(u\\). The procedure follows as that. First, add source node to set \\(S\\) which represents the explored nodes and assigns the minimum cost of the source to zero. Then each iteration we add node to \\(S\\) that has lowest cost \\((d(u))\\) from unexplored nodes. Let\u2019s say \\(S\u2032 = V \u2212 S\\) which means unexplored nodes. For all nodes in \\(S\u2032\\) we calculate \\(d(x)\\) for each node \\(x\\) is \\(S\u2032\\) then we pick minimum cost node and add it to \\(S\\). So how we calculate \\(d(x)\\)? For any \\(x\\) node from \\(S\u2032\\), \\(d(x)\\) calculated as that, let\u2019s say \\(e\\) cost of any edge from \\(S\\) to \\(x\\) then \\(d(x) = min(d(u) + e)\\). It is a greedy algorithm.</p> <p>Here is the explanation of the algorithm step by step.</p> <ol> <li> <p>Initialize an empty set, distance array, insert source to set.</p> </li> <li> <p>Initialize a min-heap, put source to heap with key is zero.</p> </li> <li> <p>While heap is not empty, take the top element from heap and add its neighbours to min-heap.</p> </li> <li> <p>Once we pick an element from the heap, it is guaranteed that the same node will never be added to heap with lower key value.</p> </li> </ol> <p>In implementation we can use priority queue data structure in order to increase efficiency. If we put unexplored nodes to min - priority queue where the distance is key, we can take the lowest cost unexplored node in \\(O(log(n))\\) time which is efficient.</p> <pre><code>typedef pair&lt;int,int&gt; edge;\ntypedef vector&lt;edge&gt; adjList;\ntypedef vector&lt;adjList&gt; graph;\n\nvoid dijkstra(graph &amp;g, int s) {\n    vector&lt;int&gt; dist(g.size(),INT_MAX/2);\n    vector&lt;bool&gt; visited(g.size(),false);\n\n    dist[s] = 0;\n\n    priority_queue&lt;edge, vector&lt;edge&gt;, greater&lt;edge&gt;&gt; q;\n    q.push({0, s});\n\n    while(!q.empty()) {\n        int v = q.top().second;\n        int d = q.top().first;\n        q.pop();\n\n        if(visited[v]) continue;\n        visited[v] = true;\n\n        for(auto it: g[v]) {\n            int u = it.first;\n            int w = it.second;\n            if(dist[v] + w &lt; dist[u]) {\n                dist[u] = dist[v] + w;\n                q.push({dist[u], u});\n            } \n        }       \n    }\n}\n</code></pre>","tags":["Graph","Shortest Path Problem","Dijkstra"]},{"location":"graph/strong-connectivity-and-biconnectivity/","title":"Strong Connectivity and Biconnectivity","text":"","tags":["Strong Connectivity","Biconnectivity","Graph"]},{"location":"graph/strong-connectivity-and-biconnectivity/#strong-connectivity","title":"Strong Connectivity","text":"<p>To reach a target node from a given node, it must be possible to arrive at the target by passing through a finite number of nodes.</p> <p>In an undirected graph, if every node is reachable from every other node, the graph is called connected. When the same concept is applied to directed graphs, it is called strongly connected.</p> <p>In other words, for a directed graph to be strongly connected, it must be possible to reach every other node from any given node.</p>","tags":["Strong Connectivity","Biconnectivity","Graph"]},{"location":"graph/strong-connectivity-and-biconnectivity/#biconnectivity","title":"Biconnectivity","text":"<p>In an undirected graph, if the remaining graph remains connected when any node is removed, the graph is called biconnected. In other words, if the graph has no articulation points, it is considered a biconnected graph.</p> An example of biconnected graph","tags":["Strong Connectivity","Biconnectivity","Graph"]},{"location":"graph/strongly-connected-components/","title":"Strongly Connected Components","text":"<p>All directed graphs can be divided into disjoint subgraphs that are strongly connected. For two subgraphs to be disjoint, they must not share any common edges or nodes. If we consider each of these resulting subgraphs as a single node and create a new graph, the resulting graph will be a directed acyclic graph (DAG), meaning it will have no cycles.</p> The subgraphs marked in red are the strongly connected components. The newly formed graph, created by treating each strongly connected component as a single node, results in a directed acyclic graph (DAG), meaning it contains no cycles. <p>Tarjan's Algorithm for finding strongly connected components (SCCs) in a directed graph (An alternative approach is Kosaraju's Algorithm, but Tarjan's algorithm is often preferred in practice due to its speed and simpler understanding):</p> <ul> <li>Start traversing the graph using DFS order from any node and push the visited nodes onto a stack. Calculate the discovery time for each node. (Discovery time is the time unit when the node is first reached during DFS traversal, and we will call this the index.)</li> <li>If a node is in the stack, it is not yet part of any strongly connected component. This is because, when a strongly connected component is found, all the nodes belonging to that component are removed from the stack.</li> <li>For each node, calculate the index of the node that has the minimum index among the nodes reachable from the current node and its subtree through edges that do not belong to any strongly connected component. This value is called the \"minimum reachable depth\" from the subtree of the node (also known as the \"low\" value).</li> <li>If a node's low value is equal to its own index, then this node and all nodes below it in the stack form a strongly connected component. This is because if we call this node \"u,\" there must be an edge from u's subtree back to u itself. Otherwise, u's low value would be clearly smaller than its own index.</li> <li>When a strongly connected component is found (as explained in the previous step), remove all nodes belonging to this component from the stack.</li> </ul> <p>Using Tarjan's Algorithm, we can find all strongly connected components in a graph with a time complexity of \\(\\mathcal{O}(V + E)\\), where \\(V\\) is the number of vertices and \\(E\\) is the number of edges.</p> <pre><code>void dfs(int node) {\n    low[node] = index[node] = ++curTime;\n    // curTime holds the discovery time of each node.\n    used[node] = true;\n\n    st.push(node);\n    inStack[node] = true;\n    // inStack holds whether a node is in the stack or not.\n    for (auto it : g[node]) {\n        if (!used[it]) {\n            dfs(it);\n            low[node] = min(low[node], low[it]);\n        } else if (inStack[it])\n            low[node] = min(low[node], index[it]);\n        // If the adjacent node is in the stack, then this edge can be a back edge.\n    }\n    if (low[node] == index[node]) {\n        while (1) {\n            int x = st.top();\n            st.pop();\n            cout &lt;&lt; x &lt;&lt; \" \";\n            inStack[x] = false;\n            if (x == node)\n                break;\n        }\n        cout &lt;&lt; endl;\n    }\n}\n\nvoid scc() {\n    for (int i = 0; i &lt; n; i++)\n        if (!used[i])\n            dfs(i);\n}\n</code></pre>","tags":["Strongly Connected Components","Graph"]},{"location":"graph/topological-sort/","title":"Topological Sort","text":"","tags":["Graph","Topological Sort"]},{"location":"graph/topological-sort/#definition","title":"Definition","text":"<p>Topological sorting for Directed Acyclic Graph (DAG) is a linear ordering of vertices such that for every directed edge u-&gt;v, vertex u comes before v in the ordering. Topological Sorting for a graph is not possible if the graph is not a DAG [6].</p> <p>There are many important usages of topological sorting in computer science; applications of this type arise in instruction scheduling, ordering of formula cell evaluation when recomputing formula values in spreadsheets, logic synthesis, determining the order of compilation tasks to perform in makefiles, data serialization, and resolving symbol dependencies in linkers. It is also used to decide in which order to load tables with foreign keys in databases [7].</p> <p>There are known algorithms (e.g Kahn\u2019s algorithm) to find topological order in linear time. Below, you can find one of the implementations:</p> For example, a topological sorting of this graph is \u201c5 4 2 3 1 0\u201d. There can be more than one topological sorting for a graph. For example, another topological sorting of the following graph is \u201c4 5 2 3 1 0\u201d. The first vertex in topological sorting is always a vertex with in-degree as 0 (a vertex with no incoming edges)[6].","tags":["Graph","Topological Sort"]},{"location":"graph/topological-sort/#algorithm","title":"Algorithm","text":"<pre><code>typedef vector&lt;int&gt; adjList;\ntypedef vector&lt;adjList&gt; graph;\ntypedef pair&lt;int,int&gt; ii;\n\nvoid kahn(graph &amp;g) {\n    vector&lt;int&gt; result;\n    queue&lt;int&gt; q;\n    vector&lt;int&gt; degree(g.size(),0); // number of incoming egdes.\n    for(auto &amp;list: g){\n        for(auto &amp;node:list) {\n            degree[node]++;\n        } \n    }\n\n    for(int i=0; i &lt; g.size(); ++i) {\n        if (degree[i] == 0)\n        q.push(i);\n    }\n\n    while( !q.empty()) {\n        int node = q.front();\n        result.push_back(node);\n        q.pop();\n\n        for (auto &amp;ng: g[node]) {\n            degree[ng]--;\n            if (degree[ng] == 0)\n                q.push(ng); \n        }\n    }\n\n    for(auto &amp;i:result)\n        cout &lt;&lt; i &lt;&lt; \" \";\n    cout &lt;&lt; endl;\n}\nint main(){\n    graph g(6);\n    g[1].push_back(0);\n    g[1].push_back(2);\n    g[2].push_back(3);\n    g[3].push_back(4);\n    g[4].push_back(5);\n    kahn(g);\n    return 0; \n}\n</code></pre> <p>As for time complexity: we traverse all edges in the beginning (calculating degrees) and in the while segment we remove edges (once for an edge) and traverse all nodes. Hence, the time complexity of this algorithm is \\(O(V +E)\\). Note that this implementation assumes the graph is DAG. Try improving this code to support checking if the graph is DAG!</p>","tags":["Graph","Topological Sort"]},{"location":"graph/tree-traversals/","title":"Tree Traversals","text":"<p>The tree traversal is the process of visiting every node exactly once in a tree structure  for some purposes(like getting information or updating information). In a binary tree there are some described order to travel, these are specific for binary trees but they may be generalized to other trees and even graphs as well.</p> a binary tree","tags":["Tree","Preorder","Postorder","Inorder"]},{"location":"graph/tree-traversals/#preorder-traversal","title":"Preorder Traversal","text":"<p>Preorder means that a root will be evaluated before its children. In other words the order of evaluation is: Root-Left-Right</p> <pre><code>Preorder Traversal\n    Look Data\n    Traverse the left node\n    Traverse the right node\n</code></pre> <p>Example: 50 \u2013 7 \u2013 3 \u2013 2 \u2013 8 \u2013 16 \u2013 5 \u2013 12 \u2013 17 \u2013 54 \u2013 9 \u2013 13</p>","tags":["Tree","Preorder","Postorder","Inorder"]},{"location":"graph/tree-traversals/#inorder-traversal","title":"Inorder Traversal","text":"<p>Inorder means that the left child (and all of the left child\u2019s children) will be evaluated before the root and before the right child and its children. Left-Root-Right (by the way, in binary search tree inorder retrieves data in sorted order)</p> <pre><code>Inorder Traversal\n    Traverse the left node\n    Look Data\n    Traverse the right node \n</code></pre> <p>Example: 2 \u2013 3 \u2013 7 \u2013 16 \u2013 8 \u2013 50 \u2013 12 \u2013 54 \u2013 17 \u2013 5 \u2013 9 \u2013 13</p>","tags":["Tree","Preorder","Postorder","Inorder"]},{"location":"graph/tree-traversals/#postorder-traversal","title":"Postorder Traversal","text":"<p>Postorder is the opposite of preorder, all children are evaluated before their root: Left-Right-Root</p> <pre><code>Postorder Traversal\n    Traverse the left node\n    Traverse the right node \n    Look Data\n</code></pre> <p>Example: 2 \u2013 3 \u2013 16 \u2013 8 \u2013 7 \u2013 54 \u2013 17 \u2013 12 \u2013 13 \u2013 9 \u2013 5 \u2013 50</p>","tags":["Tree","Preorder","Postorder","Inorder"]},{"location":"graph/tree-traversals/#implementation","title":"Implementation","text":"<pre><code>class Node:\n    def __init__(self,key):\n        self.left = None\n        self.right = None\n        self.val = key\n\ndef printInorder(root):\n    if root:\n        printInorder(root.left)\n        print(root.val)\n        printInorder(root.right)\n\ndef printPostorder(root):\n    if root:\n        printPostorder(root.left)\n        printPostorder(root.right)\n        print(root.val)\n\ndef printPreorder(root):\n    if root:\n        print(root.val)\n        printPreorder(root.left)\n        printPreorder(root.right)\n</code></pre>","tags":["Tree","Preorder","Postorder","Inorder"]},{"location":"graph/union-find/","title":"Union Find","text":"<p>A disjoint-set data structure is a data structure that keeps track of a set of elements partitioned into a number of disjoint (non-overlapping) subsets. A union-find algorithm is an algorithm that performs two useful operations on such a data structure: [11, 12]</p> <ul> <li>Find: Determine which subset a particular element is in. This can be used for determining if two elements are in the same subset.</li> <li>Union: Join two subsets into a single subset</li> <li>Union-Find Algorithm can be used to check whether an undirected graph contains cycle or not. This is another method based on Union-Find. This method assumes that graph doesn\u2019t contain any self-loops. </li> <li>Most commonly used in kruskal's minumum spanning tree algorithm, it is used to check whether two nodes are in same connected component or not. [10]</li> </ul>","tags":["Graph","Union Find","Disjoint Set Union","DSU"]},{"location":"graph/union-find/#implementation","title":"Implementation","text":"<pre><code>#include &lt;bits/stdc++.h&gt;\nusing namespace std;\n\ncont int MaxN=100005; // Max number of nodes\n\nint ancestor[MaxN];\n\nint parent(int k) // return the ancestor\n{\n    if(ancestor[k]==k) return k;\n    return ancestor[k] = parent(ancestor[k]); \n    // do not forget to equlize ancestor[k], it is going to decrease time complexity for the next operations\n}\n\nint MakeUnion(int a,int b) // setting parent of root(a) as root(b).\n{\n    a = parent(a);\n    b= parent(b);\n    ancestor[a] = b;\n}\nint find(int a,int b)\n{\n    return parent(a)==parent(b);\n}\n</code></pre>","tags":["Graph","Union Find","Disjoint Set Union","DSU"]},{"location":"graph/union-find/#complexity","title":"Complexity","text":"<p>Using both path compression, splitting, or halving and union by rank or size ensures that the amortized time per operation is only \\(\\mathcal{O}(\\alpha (n))\\), which is optimal, where \\(\\alpha (n)\\) is the inverse Ackermann function. This function has a value \\(\\alpha (n)&lt;5\\) for any value of n that can be written in this physical universe, so the disjoint-set operations take place in essentially constant time.</p>","tags":["Graph","Union Find","Disjoint Set Union","DSU"]},{"location":"introduction/","title":"Introduction","text":"<p>Editor: Muhammed Burak Bu\u011frul</p> <p>Reviewers: Kadir Emre Oto &amp; Yusuf Hakan Kalayc\u0131</p>"},{"location":"introduction/#introduction","title":"Introduction","text":"<p>First of all, this is an intensive algorithm programme prepared by inzva, which includes lectures, contests, problem-solvings and a variety of practises. \"Competitive Programming\" term will be mentioned frequently in this programme, especially for it's community, help in progress in algorithms and data structures etc.</p> <p>Just for a quick cover up we will have a look at what happens when you compile and run a program, basic data types and functions. After that, we will examine C++ Standard Template Library(STL), time complexity and memory space.</p>"},{"location":"introduction/#command-line","title":"Command Line","text":"<p>A lot of people don't use command line if they can use an alternative. There are powerful IDEs (Integrated Development Environments) and they really make programming easier in some aspects. However, knowing how to use command line is important, especially for competitive programming. Firstly, it gives a low level knowledge and full control; secondly, every computer and environment has command line interface.</p> <p>In this document, you will find only a basic introduction to the command line, which is no more than the basic usage of a file system, compiler, and programs.</p> <p>There are a lot of differences between command line of Windows and Linux. But the differences between those of Mac and Linux are less.</p>"},{"location":"introduction/#linux-and-mac","title":"Linux and Mac","text":"<p>Mac users can use the built-in Terminal. You can find it by searching from Spotlight. Linux users can use gnome-terminal or any other installed one. Again, you can find them by using the built-in search tab.</p> <p>Some basic commands:</p> <ul> <li><code>ls</code> list files in current directory. Usage: <code>ls</code></li> <li><code>cd</code> change directory. Usage: <code>cd ~/Desktop</code></li> <li><code>mkdir</code> make a new directory. Usage: <code>mkdir directory_name</code></li> <li><code>mv</code> move command (cut). Usage: <code>mv source_path destination_path</code></li> <li><code>cp</code> copy command. Usage: <code>cp source_path destination_path</code></li> <li><code>rm</code> remove command. Usage: <code>rm file_path</code></li> </ul> <p>You can read more about the unix command line at: http://linuxcommand.org</p>"},{"location":"introduction/#compiling-and-executing-programs","title":"Compiling and Executing Programs","text":""},{"location":"introduction/#g","title":"G++","text":"<p>G++ is installed in Linux environments but in Mac, you should install Xcode first.</p> <p>You can compile your cpp souce file by typing <code>g++ source.cpp</code>. Default output of this command is <code>a.out</code>.</p>"},{"location":"introduction/#running-executable-files","title":"Running Executable Files","text":"<p>For Linux and Mac, the command to run a program is <code>./program_name</code>. If you use default <code>g++</code> command, the name of your program will be <code>a.out</code>, so you should type <code>./a.out</code> in order to run it.</p>"},{"location":"introduction/#closing-a-program","title":"Closing a Program","text":"<p>When you want to kill a program in running step, you can simply hit Control + C.</p> <p>When you want to suspend a program in running step, you can simply hit Control + Z.</p> <p>When you want to register EOF on standart input, you can simply hit Control + D.</p>"},{"location":"introduction/#inputoutput-redirection","title":"Input/Output Redirection","text":"<p>You can redirect input and output streams of a program by using command line and it is surprisingly easy.</p>"},{"location":"introduction/#saving-output-to-a-file","title":"Saving Output to a File","text":"<p>The only thing you need is <code>&gt;</code> symbol. Just add it at the end of your run command with the output file name: <code>./a.out &gt; output.txt</code></p> <p>Note: This redirection process creates <code>output.txt</code> if it doesn't exist; otherwise deletes all content in it, then writes into it. If you want to use <code>&gt;</code> in appending mode you should use <code>&gt;&gt;</code> instead.</p>"},{"location":"introduction/#reading-input-from-a-file","title":"Reading Input from a File","text":"<p>It is almost the same as output file redirection. The symbol is <code>&lt;</code> now. Usage: <code>./a.out &lt; input.txt</code></p> <p>This will make your job easier than copying and pasting input to test your program, especially in the contests.</p>"},{"location":"introduction/#using-both-at-the-same-time","title":"Using Both at the Same Time","text":"<p>One of the wonderful things about these redirections is that they can be used at the same time. You can simply add both to the end of your run command: <code>./a.out &lt; input.txt &gt; output.txt</code></p>"},{"location":"introduction/#pipe","title":"pipe","text":"<p>Sometimes, you may want to redirect the output of a program to another program as input. You can use the <code>|</code> symbol for this. Usage: <code>./program1 | ./program2</code></p>"},{"location":"introduction/#diff","title":"diff","text":"<p>As the name denotes, it can check two files line by line if they are the same or not. If not, it outputs different lines. Usage: <code>diff file1.txt file2.txt</code></p> <p>It is very useful for comparing output of brute force solution and real solution.</p>"},{"location":"introduction/#structs-and-classes","title":"Structs and Classes","text":"<p>In almost every programming language, you can define your own data type. C++ has structs, classes; Python has dictionaries, classes etc. You can think of them as packets that store more than one different data and implement functions at the simplest level. They have a lot more abilities than these two (You can check OOP out).</p> <p>Let us examine a fraction struct written in C++.</p> <p>We need to store two values for a fraction, numerator and denominator.</p> <pre><code>struct Fraction {\n  int numerator, denominator;\n};\n</code></pre> <p>This is the simplest definition of a struct. Fraction struct contains two <code>int</code> variables. We call them members. So, Fraction struct has two members called numerator and denominator.</p> <pre><code>#include &lt;cstdio&gt;\n\nstruct Fraction {\n    int numerator, denominator;\n};\n\nFraction bigFraction(Fraction a, Fraction b) {\n\n    if( a.numerator * b.denominator &gt; a.denominator * b.numerator )\n        return a;\n\n    return b;\n}\n\nint main() {\n    // Create two Fractions in order to compare them\n    Fraction a, b;\n\n    a.numerator = 15;\n    a.denominator = 20;\n\n    b.numerator = 12;\n    b.denominator = 18;\n\n    // Create a new Fraction in order to store biggest of Fraction a and Fraction b.\n    fraction biggest = bigFraction(a, b);\n\n    printf(\"The biggest fraction is %d / %d\\n\", biggest.numerator, biggest.denominator);\n    return 0;\n}\n</code></pre> <p>Let us do the same in Python3:</p> <pre><code>class Fraction:\n\n    def __init__(self, numerator, denominator):\n        self.numerator, self.denominator = numerator, denominator\n\ndef bigFraction(a, b):\n\n    if a.numerator * b.denominator &gt; a.denominator * b.numerator:\n        return a\n\n    return b\n\na, b = Fraction(15, 20), Fraction(12, 18)  # Create two Fractions in order to compare them\nbiggest = bigFraction(a, b)\n\nprint(biggest.numerator, biggest.denominator)\n</code></pre> <p>In the sample codes above, <code>a</code>, <code>b</code> and <code>biggest</code> are called objects of <code>Fraction</code>. Also, the word instance can be used instead of object.</p>"},{"location":"introduction/#the-arrow-operator-c","title":"The Arrow Operator (C++)","text":"<p>Sometimes, usage of struct can change in C++. When you have a pointer to a struct, you should use <code>-&gt;</code> to access its members instead of <code>.</code> operator. If you still want to use <code>.</code> operator, you should do in this way: <code>(*ptr).member</code>. But arrow operator is simpler: <code>ptr-&gt;member</code>.</p>"},{"location":"introduction/#big-o-notation","title":"Big O Notation","text":"<p>When dealing with algorithms or coming up with a solution, we need to calculate how fast our algorithm or solution is. We can calculate this in terms of number of operations. Big \\(\\mathcal{O}\\) notation moves in exactly at this point. Big \\(\\mathcal{O}\\) notation gives an upper limit to these number of operations. The formal definition of Big \\(\\mathcal{O}\\) is [1].</p> <p>Let \\(f\\) be a real or complex valued function and \\(g\\) a real valued function, both defined on some unbounded subset of the real positive numbers, such that \\(g(x)\\) is strictly positive for all large enough values of \\(x\\). One writes:</p> <p>\\[f(x) = \\mathcal{O}{(g(x))} \\ as\\ x \\rightarrow \\infty\\]</p> <p>If and only if for all sufficiently large values of x, the absolute value of \\(f(x)\\) is at most a positive constant multiple of \\(g(x)\\). That is, \\(f(x)\\) = \\(\\mathcal{O}{(g(x))}\\) if and only if there exists a positive real number \\(M\\) and a real number \\(x_0\\) such that:</p> <p>\\[|f(x)| \\leq Mg(x)\\ for \\ all\\ x\\ such\\ that\\ x_0 \\leq x\\]</p> <p>In many contexts, the assumption that we are interested in the growth rate as the variable \\(x\\) goes to infinity is left unstated, and one writes more simply that:</p> <p>\\[f(x) = \\mathcal{O}(g(x))\\]</p> <p>Almost every case for competitive programming, basic understanding of Big \\(\\mathcal{O}\\) notation is enough to decide whether to implement a solution or not.</p> <p>Note: Big \\(\\mathcal{O}\\) notation can be used for calculating both the run time complexity and the memory space used.</p>"},{"location":"introduction/#recursion","title":"Recursion","text":"<p>Recursion occurs when functions repeat themselves in order to create repeated applications or solve a problem by handling smaller situations first. There are thousands of examples in mathematics. One of the simple ones is factorial of \\(n\\). It can be shown by \\(n!\\) in mathematics and it gives the product of all positive integers from \\(1\\) to \\(n\\), for example, \\(4! = 1\\cdot 2\\cdot 3\\cdot 4 = 24\\). If we write factorial in a mathematical way, it will be:</p> <p>\\[ \\begin{align*}     f(n) &amp;= \\begin{cases}     1 &amp; \\text{if $n = 0$\\,\\, } \\\\     n \\cdot f(n - 1) &amp; \\text{if $n &gt; 0$\\,\\,}     \\end{cases} \\end{align*} \\]</p> <p>The reason why we didn't simply write it as \\(f(n) = n \\cdot f(n-1)\\) is that it doesn't give sufficient information about function. We should know where to end the function calls, otherwise it can call itself infinitely. Ending condition is \\(n = 0\\) here. We call it base case. Every recursive function needs at least one base case.</p> <p>So if we write every step of \\(f(4)\\), it will be:</p> <p>\\[ \\begin{align*}     4!     &amp;= 4\\cdot f(3) &amp;&amp; \\text{recursive step} \\\\     &amp;= 4\\cdot 3\\cdot f(2) &amp;&amp; \\text{recursive step} \\\\     &amp;= 4\\cdot 3\\cdot 2\\cdot f(1) &amp;&amp; \\text{recursive step} \\\\     &amp;= 4\\cdot 3\\cdot 2\\cdot 1\\cdot f(0) &amp;&amp; \\text{recursive step} \\\\     &amp;= 4\\cdot 3\\cdot 2\\cdot 1\\cdot 1 &amp;&amp; \\text{base case} \\\\     &amp;= 24 &amp;&amp; \\text{arithmetic} \\end{align*} \\]</p> <p>Basically, we can apply this recursive logic into programming:</p> <pre><code>int factorial(int n) {\n    int result = 1;\n    for(int i = 1; i &lt;= n; i++)\n        res *= i;\n    return result;\n}\n</code></pre> <p>We can say a function is a recursive if it calls itself. Let us change this iterative factorial function into a recursive one. When you imagine how the recursive code will look like, you will notice it will look like the mathematical one:</p> <pre><code>int factorial(int n) {\n    if(n == 0)\n        return 1;\n    return n * factorial(n - 1);\n}\n</code></pre> <p>Note that we didn't forget to put our base case into the recursive function implementation.</p>"},{"location":"introduction/#time-complexity","title":"Time Complexity","text":"<p>In case above, it can be seen that both recursive and iterative implementations of factorial function runs in \\(\\mathcal{O}{(n)}\\) time. But this equality doesn't occur always. Let us examine fibonacci function, it is mathematically defines as:</p> <p>\\[ \\begin{align*}     f(n) &amp;= \\begin{cases}     1 &amp; \\text{if $n = 0$ or $n = 1$\\,\\, } \\\\     f(n - 1) + f(n - 2) &amp; \\text{if $n &gt; 1$\\,\\,}     \\end{cases} \\end{align*} \\]</p> <p>We can implement this function with just one for loop:</p> <pre><code>int fibonacci(int n) {\n    int result = 1, previous = 1;\n    for (int i = 2; i &lt;= n; i++) {\n        int tmp = result;\n        result += previous;\n        previous = tmp;\n    }\n    return result;\n}\n</code></pre> <p>Again, we can implement recursive one according to the mathematical formula:</p> <pre><code>int fibonacci(int n) {\n    if( n == 0 || n == 1 )\n        return 1;\n    return fibonacci(n - 1) + fibonacci(n - 2);\n}\n</code></pre> <p>Let us calculate time complexity of iterative one. There are three basic operations inside a for loop that repeats \\(n-2\\) times. So time complexity is \\(\\mathcal{O}(n)\\). But what about the recursive one? Let us examine its recursion tree(diagram of function calls) for \\(n = 5\\) on visualgo.</p> <p>\\(f\\) function called more than one for some values of n. Actually in every level, number of function calls doubles. So time complexity of the recursive implementation is \\(\\mathcal{O}{(2^n)}\\). It is far away worse than the iterative one. Recursive one can be optimized by techniques like memoization, but it is another topic to learn in further weeks.</p>"},{"location":"introduction/#mutual-recursion","title":"Mutual Recursion","text":"<p>Mutual recursion occurs when functions call each other. For example function <code>f</code> calls another function <code>g</code>, which also somehow calls <code>f</code> again.</p> <p>Note: When using mutual recursions in C++, don't forget to declare one of the functions so that the other function can know first one from its' prototype.</p> <p>Note 2: You can chain more than two functions and it will be still a mutual recursion.</p>"},{"location":"introduction/#enumeration-and-brute-force","title":"Enumeration and Brute-Force","text":"<p>Enumeration is numbering method on a set.</p> <p>For example, permutation is one of enumeration techniques. First permutation of numbers in range \\(1\\) and \\(n\\) is:</p> <p>\\[1, 2, 3... n-1, n\\]</p> <p>And second one is:</p> <p>\\[1, 2, 3... n, n-1\\]</p> <p>Finally, the last one is:</p> <p>\\[n, n-1... 3, 2, 1\\]</p> <p>Additionally, we can try to enumerate all possible distributions of \\(n\\) elements into 3 different sets. An example of a distribution of 5 elements can be represented as:</p> <p>\\[1, 1, 2, 1, 3\\]</p> <p>In this distribution the first, the second and the fourth elements goes into the first set; third element goes into second set and the last element goes into the third set.</p> <p>Enumerations can be done with recursive functions easily. We will provide example implementations of 3-set one. But before examining recursive implementation, let us try to implement iterative one:</p> <pre><code>#include &lt;cstdio&gt;\n\nint main(){\n\n    for( int i=1 ; i&lt;=3 ; i++ )\n        for( int j=1 ; j&lt;=3 ; j++ )\n            for( int k=1 ; k&lt;=3 ; k++ )\n                for( int l=1 ; l&lt;=3 ; l++ )\n                    for( int m=1 ; m&lt;=3 ; m++ )\n                        printf(\"%d %d %d %d %d\\n\", i, j, k, l, m);\n\n    return 0;\n}\n</code></pre> <p>It will print all possible distributions of 5 elements into 3 sets. But what if we had 6 elements? Yes, we should have added another for loop. What if we had \\(n\\) elements? We can not add infinite number of for loops. But we can apply same logic with recursive functions easily:</p> <pre><code>#include &lt;cstdio&gt;\n\nint ar[100];\n\nvoid enumerate( int element, int n ){\n\n    if( element &gt; n ){ // Base case\n\n        for( int i=1 ; i&lt;=n ; i++ )\n            printf(\"%d \", ar[i]);\n\n        printf(\"\\n\");\n        return;\n    }\n\n    for( int i=1 ; i&lt;=3 ; i++ ){\n        ar[element] = i;\n        enumerate(element + 1, n);\n    }\n}\n\nint main(){\n    enumerate(1, 5);\n    return 0;\n}\n</code></pre> <p>Brute-Force is trying all cases in order to achieve something(searching best, shortest, cheapest etc.).</p> <p>One of the simplest examples of brute-forces approaches is primality checking. We know that for a prime \\(P\\) there is no positive integer in range \\([2, P-1]\\) that evenly divides \\(P\\). We can simply check all integers in this range to decide if it is prime:</p> <pre><code>bool isPrime(int N) {\n    for( int i=2 ; i&lt;N ; i++ )\n        if( N % i == 0 )\n            return false;\n    return true;\n}\n</code></pre> <p>It is a simple function, but its' time complexity is \\(\\mathcal{O}(N)\\). Instead we can benefit from the fact if there is a positive integer \\(x\\) that evenly divides \\(N\\), there is a positive integer \\(\\frac{N}{x}\\) as well. As we know this fact, we can only check the integer in range \\([2, \\sqrt{N}]\\):</p> <pre><code>bool isPrime(int N) {\n    for( int i=2 ; i*i &lt;= N ; i++ )\n        if( N % i == 0 )\n            return false;\n    return true;\n}\n</code></pre> <p>Now, its' time complexity is \\(\\mathcal{O}{(\\sqrt{N})}\\). It is far away better than \\(\\mathcal{O}{(N)}\\).</p>"},{"location":"introduction/#built-in-data-structures-and-functions","title":"Built-In Data Structures and Functions","text":"<p>There is no need to reinvent the wheel. Every language has its' own built in data structures, functions etc. After this point, the document will be C++ centered. But Python alternatives will be given.</p>"},{"location":"introduction/#the-c-standard-template-library-stl","title":"The C++ Standard Template Library (STL)","text":"<p>The STL is a well known library for C++ that includes variety of data structures and algorithms.</p> <p>Note: Third party libraries generally not allowed in contests.</p>"},{"location":"introduction/#pairs","title":"Pairs","text":"<p>C++: Sometimes you may need to store two elements for an object. We can do this by creating a struct/class or two dimensional array. They will all work well but using pairs will be much more easier. You can think of pair as a class that has two variables named first and second. That's all for the basic. The good part is, you can decide their types(int, double, your own struct/class etc.). An example for pairs in C++:</p> <pre><code>#include &lt;iostream&gt;\n\nusing namespace std;\n\nint main(){\n\n    pair&lt;int, int&gt; p(1, 2);\n    pair&lt;int, string&gt; p2;\n\n    p2.first = 3;\n    p2.second = \"Hey there!\";\n\n    pair&lt;pair&lt;int, string&gt;, string&gt; nested;\n\n    nested.first = p2;\n    nested.second = \"This is a nested one\";\n\n    cout &lt;&lt; \"Info of p -&gt; \" &lt;&lt; p.first &lt;&lt; \" \" &lt;&lt; p.second &lt;&lt; endl;\n    cout &lt;&lt; \"Info of p2 -&gt; \" &lt;&lt; p2.first &lt;&lt; \" \" &lt;&lt; p2.second &lt;&lt; endl;\n    cout &lt;&lt; \"Info of nested -&gt; \" &lt;&lt; nested.first.first &lt;&lt; \" \" &lt;&lt; nested.first.second\n            &lt;&lt; \" \" &lt;&lt; nested.second &lt;&lt; endl;\n\n    return 0;\n}\n</code></pre> <p>Python: You can simply create a tuple or an array:</p> <pre><code>p = (1, 2)\np2 = [1, \"Hey there!\"]\nnested = ((3, \"inner?\"), \"outer\", \"this is a tuple you can add more\")\n\np2[0] = 3\n# nested[0] = \"don't\"  # In python you can't change tuples, but you can change arrays\n\nprint(p, p2, nested)\n</code></pre>"},{"location":"introduction/#vectors","title":"Vectors","text":"<p>C++: When using array, we should decide its size. What if we don't have to do this, what if we could add elements into it without considering the current size? Well, all these ideas take us to vectors.</p> <p>C++ has this structure. Its name is vector. It is a dynamic array but you don't have to think about its size. You can simply add elements into it. Like pairs, you can use it with any type (int, double, another vector, your struct/class etc.). Usage of a vector is very similar to classic array:</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nusing namespace std;\n\nint main(){\n\n    vector&lt;int&gt; ar;\n\n    for( int i=0 ; i&lt;10 ; i++ )\n        ar.push_back(i);\n\n    for( int i=0 ; i&lt;(int)ar.size() ; i++ )\n        cout &lt;&lt; ar[i] &lt;&lt; \" \";\n\n    cout &lt;&lt; endl;\n    return 0;\n}\n</code></pre> <p>Python: Python lists already behave like vectors:</p> <pre><code>ar = []\n\nfor i in range(10):\n    ar.append(i)\n\nprint(ar)\n</code></pre>"},{"location":"introduction/#stacks-queues-and-deques","title":"Stacks, Queues, and Deques","text":"<p>C++: They are no different than stack, queue and deque we already know. It provides the implementation, you can simply include the libraries and use them. See queue, stack, deque.</p>"},{"location":"introduction/#priority-queues","title":"Priority Queues","text":"<p>It is basically a built-in heap structure. You can add an element in \\(\\mathcal{O}(logN)\\) time, get the first item in \\(O(logN)\\) time. The first item will be decided according to your choice of priority. This priority can be magnitude of value, enterence time etc.</p> <p>C++: The different thing for priority queue is you should add <code>#include &lt;queue&gt;</code>, not <code>&lt;priority_queue&gt;</code>. You can find samples here. Again, you can define a priority queue with any type you want.</p> <p>Note: Default <code>priority_queue</code> prioritizes elements by highest value first. Here is three ways of defining priority.</p> <p>Python: You can use heapq in python.</p>"},{"location":"introduction/#sets-and-maps","title":"Sets and Maps","text":"<p>C++: Now that we mentioned binary trees (heap above), we can continue on built-in self balanced binary trees. Sets are key collections, and maps are key-value collections. Sets are useful when you want to add/remove elements in \\(\\mathcal{O}(logN)\\) time and also check existence of an item(key) in \\(O(logN)\\) time. Maps basically do the same but you can change value associated to a key without changing the position of the key in the tree. You can check c++ references for set and map. You can define them with any type you want. If you want to use them with your own struct/class, you must implement a compare function.</p> <p>Python: You can use dictionaries for map and sets for set in python without importing any other libraries.</p>"},{"location":"introduction/#iterators","title":"Iterators","text":"<p>C++: You can use iterators for every built-in data structure in C++ for pointing their objects.</p> <p>Python: You can iterate through any iterable in python by using in. You can check this example.</p>"},{"location":"introduction/#sorting","title":"Sorting","text":"<p>C++: In almost every language, there is a built-in sort function. C++ has one as well. It runs in \\(\\mathcal{O}(N log N)\\) time. You can pass your own compare function into sort function of C++.</p> <p>Python: You can use sort function for any list or list like collection in order to sort them or if you don't want to change the original collection, you can use <code>sorted()</code> function instead. You can pass your own compare function into sort function of python by using key variable.</p>"},{"location":"introduction/#suggested-readings","title":"Suggested Readings","text":""},{"location":"introduction/#c","title":"C++","text":"<ul> <li>next_permutation: Link</li> <li>STL document: Link</li> <li>binary_search: Link</li> <li>upper_bound: Link</li> <li>lower_bound: Link</li> <li>reverse: Link</li> <li>fill: Link</li> <li>count: Link</li> </ul>"},{"location":"introduction/#python","title":"Python","text":"<ul> <li>bisect: Link</li> <li>collections: Link</li> <li>built-in functions: Link</li> <li>lambda: Link</li> </ul>"},{"location":"introduction/#references","title":"References","text":"<ol> <li>Landau, Edmund (1909). Handbuch der Lehre von der Verteilung der Primzahlen [Handbook on the theory of the distribution of the primes] (in German). Leipzig: B. G. Teubner. p. 31.</li> </ol>"}]}